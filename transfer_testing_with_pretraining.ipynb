{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll #pip3 install this if you don't have it\n",
    "import torchtext.data as tt\n",
    "import torchtext\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_vec = torchtext.vocab.FastText(language='af')\n",
    "du_vec = torchtext.vocab.FastText(language='nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRIKAANS_TRAIN = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "AFRIKAANS_DEV = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu'\n",
    "AFRIKAANS_TEST = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu'\n",
    "\n",
    "DUTCH_TRAIN = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\"\n",
    "DUTCH_DEV = \"UD_Dutch-Alpino/nl_alpino-ud-dev.conllu\"\n",
    "DUTCH_TEST = \"UD_Dutch-Alpino/nl_alpino-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb\n",
    "def make_sentences(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    sentences = []\n",
    "    tagged_sentences = []\n",
    "    for each in data:\n",
    "        tagged_sentence=[]\n",
    "        sentence = []\n",
    "        for token in each:\n",
    "            if token.upos and token.form:\n",
    "                tagged_sentence.append(token.upos)\n",
    "                sentence.append(token.form.lower())\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_afr_raw, tagged_train_afr_raw = make_sentences(AFRIKAANS_TRAIN)\n",
    "dev_afr_raw, tagged_dev_afr_raw = make_sentences(AFRIKAANS_DEV)\n",
    "test_afr_raw, tagged_test_afr_raw = make_sentences(AFRIKAANS_TEST)\n",
    "\n",
    "train_du_raw, tagged_train_du_raw = make_sentences(DUTCH_TRAIN)\n",
    "dev_du_raw, tagged_dev_du_raw = make_sentences(DUTCH_DEV)\n",
    "test_du_raw, tagged_test_du_raw = make_sentences(DUTCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFRIKAANS\n",
      "Tagged sentences in train set:  1315\n",
      "Tagged words in train set: 33894\n",
      "========================================\n",
      "Tagged sentences in dev set:  194\n",
      "Tagged words in dev set: 5317\n",
      "========================================\n",
      "Tagged sentences in test set:  425\n",
      "Tagged words in test set: 10065\n",
      "****************************************\n",
      "Total sentences in dataset: 1934\n"
     ]
    }
   ],
   "source": [
    "print(\"AFRIKAANS\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_afr_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_afr_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_afr_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_afr_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_afr_raw)+len(tagged_dev_afr_raw)+len(tagged_test_afr_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH\n",
      "Tagged sentences in train set:  12264\n",
      "Tagged words in train set: 185999\n",
      "========================================\n",
      "Tagged sentences in dev set:  718\n",
      "Tagged words in dev set: 11549\n",
      "========================================\n",
      "Tagged sentences in test set:  596\n",
      "Tagged words in test set: 11053\n",
      "****************************************\n",
      "Total sentences in dataset: 13578\n"
     ]
    }
   ],
   "source": [
    "print(\"DUTCH\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_du_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_du_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_du_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_du_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_du_raw)+len(tagged_dev_du_raw)+len(tagged_test_du_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tringm/POSTagger_Pytorch/blob/master/src/util/nlp.py\n",
    "def build_tag_field(sentences_tokens):\n",
    "    token_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('tokens', token_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_tokens]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return token_field\n",
    "    \n",
    "def build_text_field(sentences_words):\n",
    "    text_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_words]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, AFR\n",
    "train_afr = build_text_field(train_afr_raw)\n",
    "dev_afr = build_text_field(dev_afr_raw)\n",
    "test_afr = build_text_field(test_afr_raw)\n",
    "tagged_train_afr = build_tag_field(tagged_train_afr_raw)\n",
    "tagged_dev_afr = build_tag_field(tagged_dev_afr_raw)\n",
    "tagged_test_afr = build_tag_field(tagged_test_afr_raw)\n",
    "\n",
    "fields_train_afr = ((\"text\", train_afr), (\"udtags\", tagged_train_afr))\n",
    "examples_train_afr = [tt.Example.fromlist(item, fields_train_afr) for item in zip(train_afr_raw, tagged_train_afr_raw)]\n",
    "fields_dev_afr = ((\"text\", dev_afr), (\"udtags\", tagged_dev_afr))\n",
    "examples_dev_afr = [tt.Example.fromlist(item, fields_dev_afr) for item in zip(dev_afr_raw, tagged_dev_afr_raw)]\n",
    "fields_test_afr = ((\"text\", test_afr), (\"udtags\", tagged_test_afr))\n",
    "examples_test_afr = [tt.Example.fromlist(item, fields_test_afr) for item in zip(test_afr_raw, tagged_test_afr_raw)]\n",
    "\n",
    "train_data_afr = tt.Dataset(examples_train_afr, fields_train_afr)\n",
    "valid_data_afr = tt.Dataset(examples_dev_afr, fields_dev_afr)\n",
    "test_data_afr = tt.Dataset(examples_test_afr, fields_test_afr)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "dev_afr.vocab = train_afr.vocab\n",
    "test_afr.vocab = train_afr.vocab\n",
    "tagged_train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "tagged_dev_afr.vocab = tagged_train_afr.vocab\n",
    "tagged_test_afr.vocab = tagged_train_afr.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, DUT\n",
    "train_du = build_text_field(train_du_raw)\n",
    "dev_du = build_text_field(dev_du_raw)\n",
    "test_du = build_text_field(test_du_raw)\n",
    "tagged_train_du = build_tag_field(tagged_train_du_raw)\n",
    "tagged_dev_du = build_tag_field(tagged_dev_du_raw)\n",
    "tagged_test_du = build_tag_field(tagged_test_du_raw)\n",
    "\n",
    "fields_train_du = ((\"text\", train_du), (\"udtags\", tagged_train_du))\n",
    "examples_train_du = [tt.Example.fromlist(item, fields_train_du) for item in zip(train_du_raw, tagged_train_du_raw)]\n",
    "fields_dev_du = ((\"text\", dev_du), (\"udtags\", tagged_dev_du))\n",
    "examples_dev_du = [tt.Example.fromlist(item, fields_dev_du) for item in zip(dev_du_raw, tagged_dev_du_raw)]\n",
    "fields_test_du = ((\"text\", test_du), (\"udtags\", tagged_test_du))\n",
    "examples_test_du = [tt.Example.fromlist(item, fields_test_du) for item in zip(test_du_raw, tagged_test_du_raw)]\n",
    "\n",
    "train_data_du = tt.Dataset(examples_train_du, fields_train_du)\n",
    "valid_data_du = tt.Dataset(examples_dev_du, fields_dev_du)\n",
    "test_data_du = tt.Dataset(examples_test_du, fields_test_du)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "dev_du.vocab = train_du.vocab\n",
    "test_du.vocab = train_du.vocab\n",
    "tagged_train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "tagged_dev_du.vocab = tagged_train_du.vocab\n",
    "tagged_test_du.vocab = tagged_train_du.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afrikaans words missing:  0\n",
      "dutch words missing:  0\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "afr_matrix_len = len(train_afr.vocab.itos)\n",
    "afr_weights_matrix = torch.zeros((afr_matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "words_missing = 0\n",
    "\n",
    "for i, word in enumerate(train_afr.vocab.itos):\n",
    "#     print(i, word)\n",
    "    try: \n",
    "        afr_weights_matrix[i] = af_vec[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "#         print(\"test\")\n",
    "        afr_weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "        words_missing += 1\n",
    "\n",
    "print(\"Afrikaans words missing: \", words_missing)\n",
    "\n",
    "du_matrix_len = len(train_du.vocab.itos)\n",
    "du_weights_matrix = torch.zeros((du_matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "words_missing = 0\n",
    "\n",
    "for i, word in enumerate(train_afr.vocab.itos):\n",
    "#     print(i, word)\n",
    "    try: \n",
    "        du_weights_matrix[i] = du_vec[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "#         print(\"test\")\n",
    "        du_weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "        words_missing += 1\n",
    "\n",
    "print(\"dutch words missing: \", words_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, pad_idx, non_trainable=False):\n",
    "    input_dim, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, input_dim, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "#model\n",
    "batch_size=128\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#needs to be tuple of dataset objects\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_du, valid_data_du, test_data_du), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions\n",
    "\n",
    "class BiLSTMTagger_Pretrained(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, weights_matrix, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding, input_dim, embedding_dim = create_emb_layer(weights_matrix, pad_idx, False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_du.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_du.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_du.vocab.stoi[train_du.pad_token]\n",
    "tag_pad_idx = tagged_train_du.vocab.stoi[tagged_train_du.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMTagger_Pretrained(du_weights_matrix, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(text)        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags) \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 15s\n",
      "\tTrain Loss: 1.523 | Train Acc: 57.30%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 78.78%\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.330 | Train Acc: 90.78%\n",
      "\t Val. Loss: 0.314 |  Val. Acc: 90.40%\n",
      "Epoch: 03 | Epoch Time: 0m 57s\n",
      "\tTrain Loss: 0.137 | Train Acc: 96.31%\n",
      "\t Val. Loss: 0.261 |  Val. Acc: 91.80%\n",
      "Epoch: 04 | Epoch Time: 1m 9s\n",
      "\tTrain Loss: 0.086 | Train Acc: 97.50%\n",
      "\t Val. Loss: 0.243 |  Val. Acc: 92.30%\n",
      "Epoch: 05 | Epoch Time: 1m 22s\n",
      "\tTrain Loss: 0.064 | Train Acc: 98.13%\n",
      "\t Val. Loss: 0.237 |  Val. Acc: 92.49%\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch: 06 | Epoch Time: 1m 50s\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.66%\n",
      "\t Val. Loss: 0.248 |  Val. Acc: 92.12%\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch: 07 | Epoch Time: 1m 55s\n",
      "\tTrain Loss: 0.036 | Train Acc: 99.02%\n",
      "\t Val. Loss: 0.258 |  Val. Acc: 92.01%\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping, reloading checkpoint model\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "patience = 3\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=False,filename='checkpt_tr_un.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping, reloading checkpoint model\")\n",
    "        model.load_state_dict(torch.load('checkpt_tr_un.pt'))\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.270 |  Test Acc: 91.49%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding.weight', Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1330,  0.1126, -0.1482,  ..., -0.1375,  0.1143, -0.1882],\n",
      "        ...,\n",
      "        [-0.0113, -0.0080,  0.0308,  ...,  0.0014, -0.0197,  0.0279],\n",
      "        [ 0.0339, -0.0339, -0.0333,  ...,  0.0249, -0.0289, -0.0306],\n",
      "        [ 0.0329, -0.0310, -0.0327,  ..., -0.0167, -0.0190, -0.0318]],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 8.9730e-02, -6.9807e-02,  6.7183e-05,  ..., -3.3914e-02,\n",
      "         -1.5017e-01,  9.2717e-02],\n",
      "        [ 1.0583e-01, -4.5381e-02, -1.0718e-01,  ..., -5.9115e-02,\n",
      "         -6.5828e-02,  4.2318e-02],\n",
      "        [ 4.3417e-02,  3.8162e-02,  1.0084e-01,  ..., -5.8114e-02,\n",
      "         -7.6137e-02,  1.0897e-01],\n",
      "        ...,\n",
      "        [ 6.2515e-02,  5.4579e-02,  1.6341e-01,  ..., -8.3769e-02,\n",
      "         -1.0880e-01, -1.1832e-02],\n",
      "        [ 1.1698e-01,  7.2385e-02,  5.7016e-02,  ..., -6.5518e-02,\n",
      "         -3.1303e-05,  7.2493e-02],\n",
      "        [ 3.3542e-02, -1.9817e-01, -4.5058e-02,  ..., -1.9415e-01,\n",
      "          5.2853e-02, -1.1202e-01]], requires_grad=True)), ('lstm.weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.1136,  0.0029, -0.0026,  ...,  0.0003, -0.0069, -0.0900],\n",
      "        [ 0.0549,  0.0262,  0.0625,  ...,  0.0678, -0.0892, -0.0638],\n",
      "        [ 0.0141,  0.0830,  0.1089,  ...,  0.1713, -0.1010, -0.1946],\n",
      "        ...,\n",
      "        [-0.2276,  0.0298,  0.1335,  ..., -0.0086,  0.0769, -0.0543],\n",
      "        [ 0.0862,  0.0985,  0.0304,  ...,  0.0368, -0.0903, -0.1562],\n",
      "        [-0.0091,  0.0244,  0.0547,  ..., -0.0329, -0.0076, -0.0812]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0', Parameter containing:\n",
      "tensor([ 0.1175,  0.1145,  0.1352,  0.0053,  0.0597,  0.1389,  0.1757,  0.0118,\n",
      "         0.0516,  0.0762, -0.0020,  0.0441,  0.0803,  0.0193,  0.0986,  0.0063,\n",
      "        -0.0148,  0.0464,  0.0287,  0.1045,  0.0516,  0.0442,  0.0875,  0.0003,\n",
      "         0.0221,  0.0541,  0.1750,  0.0616, -0.0289,  0.0406,  0.0791,  0.0107,\n",
      "         0.1340,  0.1560,  0.1088,  0.1419,  0.1131,  0.1100,  0.1521,  0.0607,\n",
      "         0.1043,  0.0381,  0.1845,  0.2009,  0.1782,  0.1755,  0.0057,  0.0433,\n",
      "         0.1806,  0.1154,  0.1248,  0.1234,  0.1467,  0.0780,  0.1033,  0.0613,\n",
      "         0.1149,  0.1334,  0.0885,  0.0873,  0.0752,  0.0624,  0.1043,  0.1423,\n",
      "         0.1414,  0.1809,  0.1710,  0.0913,  0.0140,  0.1352,  0.0567, -0.0032,\n",
      "         0.1425,  0.0187,  0.1004,  0.0417,  0.0608,  0.0660,  0.0307,  0.0997,\n",
      "         0.1580,  0.1226,  0.1779,  0.1303,  0.1989,  0.0618, -0.0051,  0.0333,\n",
      "         0.0053,  0.1437,  0.2033,  0.1334,  0.0020,  0.1625,  0.1609,  0.1577,\n",
      "         0.0709,  0.0734,  0.0397, -0.0095,  0.0471,  0.1091,  0.1127,  0.1185,\n",
      "         0.1540,  0.1407,  0.0388,  0.0729,  0.0045,  0.1245,  0.1426,  0.0946,\n",
      "         0.0781,  0.0501,  0.0433,  0.1330,  0.1595, -0.0300,  0.0142,  0.0466,\n",
      "        -0.0135,  0.0568,  0.1706,  0.0294,  0.0505,  0.0038,  0.1796,  0.0936,\n",
      "        -0.0950,  0.0008, -0.0399, -0.0786,  0.1011, -0.0452,  0.0391,  0.0677,\n",
      "        -0.0211,  0.0646, -0.0353,  0.1081,  0.0224,  0.0935, -0.0252,  0.0457,\n",
      "         0.0304, -0.0119,  0.0535, -0.0271, -0.0988,  0.0174,  0.0778, -0.0381,\n",
      "        -0.0203, -0.0261,  0.0386,  0.0814,  0.1125, -0.0158, -0.0319,  0.0393,\n",
      "        -0.0917, -0.1129, -0.0648, -0.1005,  0.0455, -0.1248,  0.0298, -0.0100,\n",
      "         0.1140,  0.0808, -0.0322, -0.0219,  0.0604, -0.0576,  0.0082,  0.1078,\n",
      "        -0.0846, -0.0923, -0.0487,  0.0424, -0.0525,  0.0568,  0.0685, -0.0943,\n",
      "         0.0039, -0.0191,  0.0924,  0.0110, -0.0319, -0.0807,  0.0730, -0.0493,\n",
      "        -0.0302, -0.0113, -0.0029, -0.0189, -0.0137,  0.0597, -0.0522,  0.0811,\n",
      "        -0.0373,  0.0657, -0.0210,  0.0812, -0.0664,  0.0747, -0.0457, -0.0051,\n",
      "         0.0240,  0.0707,  0.0422, -0.0598, -0.0629, -0.0596,  0.0612, -0.0540,\n",
      "         0.0360,  0.0435,  0.0193, -0.0024,  0.0852,  0.0472, -0.0504,  0.0214,\n",
      "        -0.0082, -0.0163,  0.0136,  0.0836, -0.0281,  0.0559, -0.0338,  0.1107,\n",
      "        -0.0168, -0.0026,  0.1004,  0.0952,  0.0562,  0.0107, -0.0689,  0.0656,\n",
      "        -0.0276,  0.0111, -0.1067,  0.0776,  0.0137, -0.0036, -0.0677,  0.0378,\n",
      "         0.1169, -0.0366, -0.0704, -0.0376,  0.0359,  0.1094, -0.1010,  0.0301,\n",
      "        -0.0959,  0.0866,  0.0291,  0.0410,  0.1183, -0.0421,  0.0110, -0.0857,\n",
      "        -0.1065,  0.1139,  0.0121, -0.0399,  0.0656, -0.1297, -0.0299,  0.1194,\n",
      "        -0.0688,  0.0741,  0.0889,  0.0875,  0.0485, -0.0886, -0.0886, -0.0938,\n",
      "        -0.1022,  0.0665,  0.0339,  0.1406, -0.0448,  0.0910, -0.1343, -0.0703,\n",
      "        -0.0307,  0.0524,  0.0691, -0.0454, -0.0048, -0.0150,  0.0740, -0.0327,\n",
      "        -0.0488,  0.0725,  0.0122,  0.0916, -0.0961,  0.0087, -0.0190, -0.0093,\n",
      "         0.1001,  0.0392, -0.0671, -0.1110, -0.0248, -0.1200,  0.0373, -0.0110,\n",
      "         0.0657,  0.0939, -0.0311,  0.0655, -0.0820,  0.0151, -0.0665,  0.0836,\n",
      "        -0.0447,  0.1344,  0.0996,  0.0392, -0.0253, -0.1302, -0.0318,  0.0093,\n",
      "         0.1228,  0.0448,  0.0007,  0.1113, -0.0050, -0.0732,  0.0203, -0.0518,\n",
      "         0.1078, -0.0479, -0.0256, -0.0417, -0.0143,  0.1358,  0.0586,  0.1080,\n",
      "        -0.0149, -0.1324, -0.0060, -0.0198, -0.0815, -0.0748, -0.0014, -0.0129,\n",
      "        -0.0839, -0.0412, -0.0629, -0.0736, -0.1219, -0.0194, -0.0006, -0.1234,\n",
      "        -0.0958, -0.1119,  0.1174, -0.1359,  0.0978,  0.0072, -0.0205,  0.0972,\n",
      "        -0.1055, -0.0667,  0.0343,  0.0149,  0.0735,  0.0487, -0.1172,  0.0008,\n",
      "        -0.0354,  0.0062,  0.0663,  0.1233, -0.0617,  0.0390, -0.0882, -0.0991,\n",
      "         0.0251,  0.0966,  0.0583,  0.0378, -0.0166,  0.0994,  0.0616,  0.0241,\n",
      "         0.0660,  0.1479,  0.0052,  0.0428,  0.1176,  0.1337,  0.0952,  0.0066,\n",
      "         0.1109,  0.0529,  0.0881, -0.0247,  0.1396,  0.0508,  0.0421, -0.0139,\n",
      "         0.0697,  0.0618,  0.1027,  0.0379,  0.1087,  0.1221, -0.0186,  0.0380,\n",
      "         0.0903,  0.1241,  0.0409,  0.1322,  0.0436,  0.1386,  0.1646,  0.1444,\n",
      "         0.1176,  0.0620,  0.1940,  0.1073,  0.1214,  0.1837,  0.0405,  0.0845,\n",
      "         0.1509,  0.1564,  0.1177,  0.0645,  0.0712,  0.1300,  0.1684,  0.0822,\n",
      "        -0.0513,  0.1420,  0.0434, -0.0328, -0.0138,  0.1730,  0.0358,  0.0762,\n",
      "         0.0651,  0.1028,  0.0238,  0.0953,  0.1438,  0.0670,  0.0800,  0.1804,\n",
      "         0.1072,  0.1827,  0.0088, -0.0221,  0.1895,  0.1041,  0.1542,  0.1408,\n",
      "         0.1873, -0.0099,  0.0769,  0.1857,  0.0288,  0.0432,  0.0286,  0.1446,\n",
      "         0.0487,  0.0063,  0.1284,  0.1031,  0.1328,  0.1830,  0.0744,  0.0328,\n",
      "         0.0745, -0.0331,  0.0495,  0.0152,  0.0125,  0.0121,  0.0745,  0.1179,\n",
      "         0.0558, -0.0038,  0.0182, -0.0236,  0.1180,  0.1194,  0.0792,  0.0954,\n",
      "         0.0840,  0.0792,  0.0131, -0.0194,  0.0879, -0.0199,  0.0372,  0.1012,\n",
      "         0.0842,  0.0460,  0.1075,  0.1703, -0.0284,  0.0692,  0.0800,  0.0273],\n",
      "       requires_grad=True)), ('lstm.bias_hh_l0', Parameter containing:\n",
      "tensor([ 3.6725e-02,  5.5539e-02,  2.7176e-02,  1.8909e-02,  3.1001e-02,\n",
      "         1.6917e-01,  1.1046e-01,  1.6576e-01,  1.0006e-01,  6.1289e-02,\n",
      "         4.5961e-02,  1.0217e-01,  6.4636e-02,  1.3101e-01,  1.3362e-01,\n",
      "         8.7906e-02,  7.1465e-02,  7.2223e-02,  2.7595e-02,  1.4196e-02,\n",
      "         1.7394e-01,  1.1151e-01,  4.9477e-02,  1.1127e-01,  4.1797e-02,\n",
      "         6.3945e-02,  1.1330e-01,  5.6673e-02,  6.6429e-02,  1.4743e-01,\n",
      "         1.0456e-02,  1.5639e-02,  1.8495e-01,  2.1077e-02,  2.9119e-02,\n",
      "         1.3808e-01,  8.3111e-02,  1.5451e-01,  1.6860e-01,  9.5689e-02,\n",
      "         1.0245e-01,  1.6110e-02,  5.4036e-02,  1.6476e-01,  1.6734e-01,\n",
      "         6.7468e-02,  7.1629e-02,  1.1395e-02,  1.3559e-01,  8.6178e-02,\n",
      "         9.3394e-02,  4.4965e-02,  7.8388e-03,  3.3719e-02,  1.7317e-01,\n",
      "         4.9905e-02,  8.3367e-02,  1.0284e-01,  1.5213e-01,  1.0630e-01,\n",
      "         6.3438e-02,  7.8936e-02,  5.9650e-02,  9.6452e-02,  8.6963e-02,\n",
      "         1.8827e-01,  6.7641e-02,  1.1682e-01,  9.7836e-03,  8.5896e-03,\n",
      "         1.3466e-02,  4.3707e-02,  1.4639e-01,  4.5796e-02,  6.9981e-03,\n",
      "         2.9537e-02,  1.2072e-01,  8.3721e-03, -4.3302e-06,  2.7785e-02,\n",
      "         6.7932e-02,  8.1499e-02,  1.9692e-01,  1.0776e-01,  1.9065e-01,\n",
      "         1.5364e-01,  9.7011e-02,  8.8169e-02,  2.7349e-02,  1.4937e-01,\n",
      "         2.9002e-02,  1.4569e-01, -1.1366e-03,  1.1705e-01,  1.2913e-01,\n",
      "         1.2425e-01,  8.9517e-02,  8.2972e-02,  1.1078e-01,  1.0328e-01,\n",
      "         2.2823e-02,  1.2375e-01, -9.4934e-04,  7.1893e-02,  4.4503e-02,\n",
      "         1.4219e-01,  7.5751e-02,  1.4683e-01,  1.2045e-01,  1.2085e-01,\n",
      "         1.7526e-01,  6.4102e-02,  3.5229e-02,  2.0214e-02,  1.3565e-01,\n",
      "         1.2787e-01,  1.2574e-01,  6.8186e-02,  3.9197e-02,  5.1018e-02,\n",
      "        -1.3040e-02,  1.1209e-03,  1.3540e-01,  1.8203e-01,  1.3156e-01,\n",
      "         1.0015e-01,  6.8901e-02, -1.6454e-02, -1.2417e-01, -8.4230e-02,\n",
      "        -8.2532e-02, -7.0712e-02,  5.2200e-02, -1.1546e-01,  2.6473e-02,\n",
      "         1.6092e-02,  6.9745e-02,  1.0170e-01, -2.8592e-02,  4.9641e-02,\n",
      "        -1.4874e-02,  6.4059e-02,  7.1378e-02, -2.5038e-02,  1.1020e-01,\n",
      "        -6.8390e-02,  8.7728e-02,  5.7735e-02, -4.1583e-02,  7.1352e-02,\n",
      "        -6.6344e-02,  5.9321e-02, -9.2205e-02, -5.3467e-02, -8.8400e-03,\n",
      "         1.9382e-03,  7.0028e-02, -3.9702e-02,  1.3488e-01,  6.8714e-02,\n",
      "        -8.3534e-02, -2.7174e-02, -6.6890e-02, -1.2473e-01,  3.5339e-02,\n",
      "        -6.3346e-02, -3.2161e-02, -8.3742e-02, -1.5088e-02, -3.2728e-02,\n",
      "         8.4341e-03,  9.7866e-03, -7.0094e-02,  3.3519e-02,  6.0780e-02,\n",
      "         9.5866e-02, -4.8852e-02, -4.3917e-03,  1.1012e-01,  3.9345e-02,\n",
      "         3.9057e-02,  3.7115e-02, -5.1038e-03, -5.0518e-02,  5.1662e-02,\n",
      "        -1.2294e-01, -5.0029e-02, -4.5737e-02, -5.5358e-02, -8.6421e-02,\n",
      "         1.0124e-03,  9.2347e-02,  8.0990e-02, -5.1654e-02, -1.9578e-02,\n",
      "         6.9461e-02,  4.0715e-02, -4.0770e-02, -5.3558e-02, -1.7799e-02,\n",
      "         6.0081e-02, -4.7522e-02,  6.5484e-02,  1.0037e-01,  5.5778e-02,\n",
      "         1.0956e-01, -5.3259e-02, -1.1700e-02, -7.9966e-02, -5.1503e-02,\n",
      "        -1.5460e-02, -1.6558e-02,  3.8665e-02, -7.7898e-02,  1.0814e-01,\n",
      "        -4.4615e-02,  1.8297e-02, -1.0976e-02, -1.1080e-03, -5.6257e-02,\n",
      "        -3.7321e-02, -1.5062e-02, -3.9982e-02, -1.4933e-02,  1.0198e-01,\n",
      "         9.6920e-02, -5.1167e-02,  6.8814e-02,  6.2085e-02,  6.3614e-02,\n",
      "        -3.1711e-02,  1.0552e-01,  5.1510e-02,  6.0143e-02, -5.3493e-02,\n",
      "        -1.8821e-02,  6.3027e-02, -5.3838e-02, -5.7050e-02, -7.2933e-02,\n",
      "        -4.2087e-02, -1.0183e-01, -1.3374e-01,  3.2247e-02,  4.0777e-02,\n",
      "         6.0669e-02,  4.1595e-02,  2.9024e-02,  4.3177e-02,  2.7045e-02,\n",
      "        -4.2031e-02,  5.2075e-02,  3.5560e-02,  2.6373e-02, -2.3756e-02,\n",
      "         5.4294e-02, -5.3777e-02,  1.1872e-01, -9.8080e-02,  1.2410e-01,\n",
      "        -2.7690e-02,  4.8882e-02,  3.6021e-02,  2.1627e-02, -5.2922e-02,\n",
      "        -1.2176e-02, -4.6725e-02, -7.5658e-02, -1.3961e-02, -1.1369e-01,\n",
      "        -6.4272e-02,  1.0677e-01,  2.3517e-02, -5.6035e-02,  4.1843e-02,\n",
      "         9.6363e-02, -8.5066e-03, -7.9683e-03,  5.6345e-02, -3.2822e-02,\n",
      "        -6.0280e-02,  5.9383e-02, -4.0996e-02,  5.8286e-02, -1.6009e-02,\n",
      "         5.2308e-02, -8.7092e-02, -1.4235e-01,  4.4041e-02,  5.4473e-02,\n",
      "         6.0720e-02, -2.8436e-02,  2.4433e-02,  7.1460e-02,  1.1647e-02,\n",
      "         2.9569e-02, -1.2562e-01,  1.4112e-02, -1.1425e-01, -5.1204e-02,\n",
      "        -1.2379e-03, -1.3378e-01, -9.6620e-02, -4.6627e-02,  4.4373e-02,\n",
      "         6.7801e-02, -3.2341e-02,  4.0618e-02,  1.1030e-01, -8.4188e-02,\n",
      "        -9.7463e-03,  4.2583e-02,  3.9645e-02, -6.4665e-02, -1.2699e-01,\n",
      "         5.7403e-02,  4.4400e-03,  6.8627e-02, -7.6389e-02,  3.0282e-02,\n",
      "        -9.7837e-02,  8.0449e-02, -1.1142e-02, -1.1399e-02,  7.1728e-02,\n",
      "        -5.5143e-02, -9.6584e-02,  8.6525e-02, -9.1200e-03,  4.0313e-02,\n",
      "        -1.3602e-02,  8.6434e-02, -4.8899e-02, -1.3809e-01,  4.3553e-02,\n",
      "        -3.5825e-02,  7.0466e-02, -1.2491e-01, -1.5667e-02, -8.5561e-03,\n",
      "         8.0583e-02,  8.6131e-02,  1.4121e-01,  1.3684e-01,  6.3853e-02,\n",
      "         1.5537e-02,  4.2265e-02,  1.1640e-01, -1.1710e-01, -9.0961e-03,\n",
      "        -2.0196e-03, -9.1809e-02, -2.4166e-02, -6.8069e-02, -2.1418e-02,\n",
      "        -1.0005e-01, -1.9489e-03,  2.9993e-02, -3.8873e-03, -3.4181e-02,\n",
      "        -5.0000e-02,  5.4296e-03,  7.7307e-02, -9.5577e-02,  3.4087e-02,\n",
      "        -1.2371e-01, -1.0326e-01, -2.5325e-02, -1.0864e-01,  5.3431e-02,\n",
      "         3.8650e-02, -1.0897e-01,  3.1273e-02, -8.0521e-02, -2.5031e-03,\n",
      "         1.2186e-02, -6.2257e-03,  9.1343e-02,  8.9150e-02, -5.7998e-03,\n",
      "        -4.2292e-02,  1.9440e-02, -3.9784e-02,  1.9484e-02,  9.9257e-02,\n",
      "         9.7187e-02,  1.0560e-01,  2.6811e-02,  2.2051e-02,  5.6171e-02,\n",
      "         1.1977e-01,  4.9153e-02,  5.4777e-02,  3.2766e-02,  7.4145e-02,\n",
      "        -3.2993e-02,  1.5379e-01, -6.4033e-03, -7.0296e-03,  7.9780e-02,\n",
      "        -2.2368e-02,  1.4436e-01,  1.4515e-02,  2.8690e-02,  1.4033e-01,\n",
      "         9.8791e-02,  4.5271e-02, -3.5075e-02,  3.4563e-02,  1.4871e-01,\n",
      "         1.3761e-01,  1.0432e-01, -4.9851e-03,  1.1978e-01,  7.4339e-02,\n",
      "         8.4807e-02,  1.2985e-01,  1.2173e-01,  7.7144e-02,  1.7184e-01,\n",
      "         1.5068e-01,  1.4522e-01,  2.0202e-01,  1.4793e-01,  1.0891e-01,\n",
      "         5.8942e-02,  9.4152e-02,  2.0061e-01,  1.8346e-01,  5.8168e-02,\n",
      "         4.9260e-02,  5.9266e-02,  8.1408e-02,  3.6540e-02,  1.1814e-01,\n",
      "         1.2391e-01,  1.3414e-01,  9.6813e-02,  1.3973e-01,  8.1889e-02,\n",
      "         5.7235e-02,  1.1361e-01,  5.0703e-02,  5.3171e-02,  3.5477e-02,\n",
      "         1.3157e-01,  1.2019e-01,  1.3713e-01,  1.2421e-01,  1.3108e-01,\n",
      "         4.7091e-02,  4.0150e-02,  1.5226e-02, -1.0648e-02,  1.8148e-01,\n",
      "         6.4428e-02,  1.2759e-01,  3.8946e-02, -2.6315e-02, -3.6737e-02,\n",
      "         6.2824e-02,  1.1179e-01,  8.2631e-02,  1.5215e-01,  1.9353e-01,\n",
      "         3.6292e-03,  1.4788e-01,  1.0704e-01,  8.6605e-02,  2.4385e-02,\n",
      "         9.5556e-02,  4.0301e-02,  3.6470e-02,  9.9731e-02,  1.5991e-01,\n",
      "         6.3822e-02,  1.1247e-01,  5.4235e-02,  1.5636e-01,  7.2455e-02,\n",
      "         1.2815e-01,  3.6354e-02,  8.8532e-02,  1.2975e-01,  5.5013e-02,\n",
      "         2.7207e-02, -5.5767e-02,  8.6174e-02,  1.2014e-01,  1.1112e-01,\n",
      "         1.2116e-01,  1.1655e-01,  4.1798e-02,  7.4652e-02,  1.4730e-01,\n",
      "         8.2546e-02,  1.3471e-01,  7.4853e-02,  1.5059e-01,  1.2590e-01,\n",
      "         6.8046e-02,  3.9479e-03,  1.2751e-01,  6.0791e-02,  6.0878e-02,\n",
      "         6.9607e-02,  1.5149e-01,  1.6921e-01,  7.1157e-02,  1.1153e-01,\n",
      "         8.4156e-02,  7.5296e-02], requires_grad=True)), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[-0.0905,  0.0254,  0.1259,  ...,  0.0214, -0.0679,  0.1082],\n",
      "        [ 0.0385, -0.0905,  0.0473,  ..., -0.0962, -0.1277,  0.0603],\n",
      "        [ 0.1165,  0.1462, -0.0030,  ..., -0.1135, -0.0749,  0.0840],\n",
      "        ...,\n",
      "        [-0.1130, -0.0748, -0.1037,  ..., -0.1187, -0.1056, -0.1927],\n",
      "        [ 0.0592,  0.0413, -0.0105,  ...,  0.0073, -0.0710, -0.0620],\n",
      "        [ 0.0418, -0.0055,  0.0359,  ..., -0.0296,  0.0080,  0.1111]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[-0.0651,  0.0220, -0.0178,  ..., -0.0474, -0.0450, -0.0268],\n",
      "        [ 0.0148, -0.0176,  0.0035,  ..., -0.1233, -0.0172, -0.0283],\n",
      "        [-0.0182,  0.0344, -0.0824,  ..., -0.0639, -0.1251, -0.1553],\n",
      "        ...,\n",
      "        [-0.0443,  0.0615, -0.0108,  ..., -0.0632,  0.0699, -0.1105],\n",
      "        [-0.0842, -0.0041, -0.0460,  ..., -0.0772, -0.1130, -0.1108],\n",
      "        [-0.1010,  0.0977,  0.0135,  ..., -0.0716, -0.0862, -0.0924]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([-3.3459e-02,  1.1341e-01,  5.7301e-02,  3.4534e-03,  5.3172e-02,\n",
      "         1.4759e-01,  3.7298e-02,  2.6210e-02,  4.7107e-02,  1.0862e-01,\n",
      "         1.1264e-01,  9.3631e-03,  4.4443e-02,  8.9610e-02,  1.5032e-01,\n",
      "         5.5054e-02,  6.6498e-05,  9.1557e-02,  1.5309e-01,  1.2212e-01,\n",
      "         7.3833e-02,  6.8224e-03,  8.4983e-02,  7.7155e-02,  1.1390e-01,\n",
      "         7.0183e-02,  8.7041e-02,  1.2954e-01,  7.5973e-02,  1.4048e-01,\n",
      "         2.5395e-02,  3.0803e-02, -1.7049e-02,  2.5016e-04,  1.0031e-01,\n",
      "         1.1298e-01,  5.3153e-02,  1.5505e-01,  4.2986e-02,  1.8066e-01,\n",
      "         1.4548e-01,  1.3070e-01,  1.0378e-01, -1.0419e-02,  9.4079e-03,\n",
      "        -3.9178e-04, -1.7665e-03,  1.1408e-01,  1.7070e-01,  1.5547e-01,\n",
      "         1.4065e-01,  2.6486e-02, -3.4807e-02,  1.8372e-01,  6.6275e-02,\n",
      "         2.8946e-02,  6.9475e-02,  6.6566e-02, -8.8732e-03,  1.3619e-01,\n",
      "         1.1038e-01,  5.1819e-02,  9.2836e-02,  1.9616e-03, -8.3040e-03,\n",
      "         1.1864e-01, -4.9232e-02,  1.3182e-02, -1.5489e-02, -5.3805e-03,\n",
      "         6.8164e-02,  1.5082e-01,  4.3685e-02,  6.2212e-02,  1.2200e-01,\n",
      "         1.3939e-01,  5.4716e-02, -1.7439e-02,  1.5514e-01,  4.0297e-02,\n",
      "         8.5237e-02,  1.0250e-01,  1.6521e-03, -5.9123e-02,  3.0990e-02,\n",
      "         1.0156e-01,  2.9026e-02,  8.6738e-02,  8.5125e-02,  6.9242e-02,\n",
      "         1.3970e-01,  9.9159e-02,  1.0714e-02,  1.0910e-01, -3.4892e-02,\n",
      "         1.3931e-01,  8.6107e-02,  6.1320e-02,  6.8925e-02,  8.8818e-02,\n",
      "         7.0886e-02,  4.5362e-02,  1.4625e-01, -1.8885e-03, -5.2877e-02,\n",
      "         6.9853e-02,  7.3217e-03,  8.7427e-02,  5.9206e-02,  9.3149e-02,\n",
      "         1.3343e-01,  2.1794e-02,  1.4419e-01,  5.0604e-02,  2.1060e-02,\n",
      "         8.5127e-02,  7.9651e-02,  1.4261e-01,  6.3677e-02, -7.7056e-04,\n",
      "         1.2652e-01,  1.6682e-01,  2.6384e-03,  7.7517e-02,  8.4852e-02,\n",
      "         4.1361e-02, -7.8919e-03,  1.1611e-01,  2.1219e-02,  3.6128e-02,\n",
      "        -1.1918e-02,  1.8161e-02, -1.4655e-03, -1.0165e-01,  1.2661e-02,\n",
      "        -3.7687e-02,  8.6860e-02, -5.3991e-02, -5.7249e-02, -2.4547e-02,\n",
      "         6.8155e-02,  1.4481e-02, -6.9588e-03, -3.8840e-02, -2.7904e-02,\n",
      "         5.8059e-02, -4.6868e-02,  7.5392e-02,  1.0200e-01,  2.7551e-03,\n",
      "        -1.7473e-02,  5.0192e-02, -9.5854e-02,  4.9969e-02,  2.8394e-02,\n",
      "        -3.8415e-02,  2.3796e-02, -3.8902e-02, -4.0112e-02,  8.8351e-02,\n",
      "        -6.9195e-02,  4.8754e-04,  5.3092e-02,  4.1092e-03,  5.3720e-02,\n",
      "        -1.5441e-01,  6.9394e-03,  7.0285e-02, -5.5194e-02, -1.1145e-02,\n",
      "         1.2088e-01,  2.2760e-02, -1.2925e-02, -7.6065e-02,  1.3012e-02,\n",
      "        -5.6666e-02,  1.8191e-02, -1.2347e-02,  2.4835e-02, -1.9903e-02,\n",
      "         8.6200e-02, -7.2706e-02, -6.8056e-03, -8.4851e-03, -7.5037e-02,\n",
      "        -2.8015e-02, -5.2511e-03,  5.1551e-02, -3.2649e-04,  1.0200e-01,\n",
      "        -3.0902e-02,  8.1560e-02,  3.3845e-02, -4.9001e-02,  8.5093e-02,\n",
      "         6.9644e-02,  6.8513e-02, -4.0022e-02,  3.1758e-03, -4.3877e-02,\n",
      "        -4.3929e-02,  7.3416e-03, -5.8284e-02,  7.5518e-02, -6.3351e-02,\n",
      "         5.9246e-02,  3.1452e-02,  3.3032e-02, -5.7605e-02,  5.3034e-02,\n",
      "        -4.8682e-02,  3.6615e-02,  2.4090e-02, -7.6804e-02,  9.0237e-02,\n",
      "        -2.1803e-02,  6.2732e-02,  5.7130e-02, -4.9622e-02, -2.9907e-02,\n",
      "        -6.9900e-02,  4.0718e-02, -3.8547e-03, -3.3447e-02, -3.6332e-02,\n",
      "        -6.0475e-02,  3.6041e-02, -2.3151e-02,  1.6392e-02, -2.8622e-02,\n",
      "        -7.3090e-02,  2.3817e-02, -1.7290e-02, -5.7872e-02,  7.1014e-03,\n",
      "         6.6733e-02, -3.7544e-02, -3.8249e-02,  6.6836e-02, -4.0528e-02,\n",
      "        -4.0846e-02, -9.2781e-03,  2.4150e-02,  6.6111e-02, -4.2855e-02,\n",
      "         3.2694e-02,  1.5604e-03, -6.5332e-02,  5.4151e-02, -7.2747e-03,\n",
      "        -3.2247e-02,  3.1597e-02, -2.2099e-04,  3.0688e-02,  8.2718e-02,\n",
      "         3.0899e-02, -1.0705e-01,  7.3755e-02, -6.8037e-02,  5.3099e-02,\n",
      "         7.0296e-02, -9.0066e-02, -5.6491e-03,  1.1391e-03,  1.1157e-01,\n",
      "         5.5055e-02, -4.5630e-03, -1.0248e-02,  9.0848e-02, -3.1904e-02,\n",
      "         7.2404e-02,  7.0004e-03,  4.4826e-02, -8.1536e-02,  5.7162e-02,\n",
      "         6.2066e-02, -6.4807e-03, -4.0631e-02,  8.4256e-02,  1.2960e-02,\n",
      "        -1.4494e-02, -2.3113e-03,  7.5853e-02,  4.0328e-02, -1.7696e-02,\n",
      "         2.4519e-02,  1.0661e-01,  9.9783e-02,  4.7944e-02,  7.1478e-03,\n",
      "         7.4288e-04,  6.6270e-02,  5.8005e-02, -5.3970e-02, -4.8177e-02,\n",
      "         1.2109e-01,  2.5129e-02, -7.1145e-02,  4.6469e-02, -6.9108e-02,\n",
      "        -3.8492e-02,  1.9498e-02,  8.5684e-02, -7.8157e-02,  8.2342e-02,\n",
      "         1.6263e-02, -5.4962e-02, -4.6445e-02,  1.2162e-01,  3.7643e-02,\n",
      "        -5.5101e-02, -3.8699e-02, -5.0070e-02, -7.6139e-02,  3.8327e-02,\n",
      "         8.7316e-02, -7.6874e-02,  9.6997e-02, -6.0041e-02,  8.1706e-03,\n",
      "         1.2383e-02, -8.0332e-02,  2.3232e-02,  8.6118e-02, -8.1052e-02,\n",
      "         1.0634e-01,  2.5488e-03, -4.1248e-02,  7.7160e-03, -2.0214e-02,\n",
      "         9.3446e-02, -3.0999e-02, -1.4624e-02, -2.5456e-02, -1.0310e-01,\n",
      "         2.8161e-02,  5.7140e-02,  1.6983e-03, -5.7693e-02,  8.7859e-03,\n",
      "        -9.7985e-02,  1.4257e-02, -4.9143e-02,  7.4605e-02, -5.0054e-02,\n",
      "        -4.9873e-02,  3.8156e-03, -8.4611e-02, -9.1335e-02,  7.0803e-02,\n",
      "         5.9533e-02, -4.1590e-02,  7.4683e-02, -1.0725e-01,  6.7769e-02,\n",
      "         1.4719e-02, -9.9396e-02, -2.5253e-02,  3.8447e-02, -1.3403e-03,\n",
      "        -7.2188e-02, -4.3997e-02, -7.5022e-02, -4.0713e-02, -1.4288e-02,\n",
      "         7.2592e-02, -4.0697e-03,  8.0916e-03,  9.2831e-02, -6.7003e-02,\n",
      "         9.6277e-02,  3.2171e-02, -6.4582e-02,  5.5225e-02, -7.4016e-03,\n",
      "        -9.7690e-02, -4.4825e-02, -4.8126e-02,  3.2611e-03,  5.2541e-02,\n",
      "        -8.0609e-02, -9.3621e-02, -4.5122e-02,  1.2103e-02, -1.7900e-02,\n",
      "         8.4759e-02,  5.2437e-02, -5.3002e-03, -4.1612e-02,  6.2814e-02,\n",
      "         1.5726e-01, -3.6201e-02,  6.3053e-02,  1.3381e-01,  6.1809e-02,\n",
      "         1.3730e-01,  8.9237e-02,  1.5466e-02,  4.0339e-02,  3.2014e-02,\n",
      "         1.5303e-01,  3.3214e-02,  2.5437e-02,  8.8441e-02,  5.6385e-02,\n",
      "         1.3097e-02, -1.5469e-03,  6.4109e-02,  1.4175e-01,  1.1534e-01,\n",
      "         1.0325e-01,  4.1434e-02,  2.9219e-02,  1.4269e-01,  1.0268e-01,\n",
      "        -1.2370e-02,  7.4868e-02,  6.8783e-03,  1.9314e-02,  6.3753e-02,\n",
      "         1.0283e-01,  9.1947e-02, -1.4569e-02,  1.7781e-01,  2.3872e-02,\n",
      "         5.1514e-02,  9.3863e-02, -3.0778e-03,  3.6790e-02,  1.2357e-01,\n",
      "         9.1943e-03,  8.4392e-02,  1.0075e-02,  2.9383e-02,  1.1767e-01,\n",
      "         1.2343e-01,  8.1406e-02,  1.5107e-01,  1.4467e-01,  1.4876e-01,\n",
      "         1.3986e-01,  7.8555e-02,  7.3622e-02,  4.5319e-02,  1.0908e-01,\n",
      "         3.2809e-02,  1.4097e-01, -1.6474e-02,  3.1006e-02,  1.2393e-01,\n",
      "        -2.2858e-02,  1.3483e-01,  5.4600e-03,  1.1679e-01,  7.2286e-02,\n",
      "         1.0773e-01,  7.2434e-02, -4.5203e-03,  8.7291e-02,  7.4861e-02,\n",
      "         1.3377e-01,  5.8164e-02,  2.5506e-02,  4.8963e-02, -1.7398e-02,\n",
      "        -2.7270e-02,  1.3605e-01,  7.0423e-03,  1.0359e-01,  2.1408e-02,\n",
      "         6.7820e-03,  1.0938e-01,  1.2677e-01,  3.9659e-02,  7.5996e-02,\n",
      "         9.2135e-02,  2.2789e-02,  7.6758e-02,  9.9983e-02,  6.0362e-02,\n",
      "         5.5648e-02,  3.0948e-02,  2.9404e-02,  1.3354e-01, -3.4640e-02,\n",
      "         2.0684e-02,  4.5182e-02,  2.7940e-02,  2.4559e-02,  3.0016e-02,\n",
      "         3.5516e-02,  8.9236e-02, -7.1268e-03, -1.3486e-02,  3.7750e-02,\n",
      "         1.2027e-01,  6.4398e-02, -4.3989e-02,  3.5284e-02, -2.8879e-02,\n",
      "         4.4535e-04,  1.3911e-01,  9.6301e-02,  8.1029e-02,  5.1455e-02,\n",
      "         2.7084e-02,  1.9000e-02,  2.6306e-02,  7.2884e-03, -2.0821e-02,\n",
      "        -1.7852e-02,  1.2496e-01], requires_grad=True)), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([ 2.8157e-02,  7.3041e-04,  9.9931e-02, -3.4610e-02, -5.6018e-03,\n",
      "         1.5592e-01,  1.0052e-01,  2.4067e-02,  1.2913e-01,  2.4668e-02,\n",
      "         7.8363e-02,  1.1321e-02,  6.9204e-03,  1.3877e-01, -1.7775e-02,\n",
      "         6.7508e-02,  1.7320e-01,  5.0002e-02,  6.4115e-02, -2.9367e-02,\n",
      "         6.3457e-02,  2.2800e-02,  8.8148e-02,  1.1405e-01,  1.5569e-01,\n",
      "        -1.3828e-02,  1.3248e-01,  5.5213e-02, -4.4024e-02,  2.9055e-02,\n",
      "         3.9826e-03, -1.0819e-02, -6.9760e-03,  1.0585e-01,  1.0029e-01,\n",
      "         1.3778e-01,  1.5209e-01,  1.7403e-01,  2.5088e-02,  8.6398e-02,\n",
      "         1.2675e-01,  1.3164e-01,  1.6390e-02, -4.2130e-02, -5.8544e-02,\n",
      "         5.9396e-02,  1.4161e-01,  1.2304e-01,  1.1875e-01,  6.8292e-02,\n",
      "         9.4620e-02, -4.4313e-02, -2.6486e-02,  7.8331e-02,  1.4833e-01,\n",
      "         9.6254e-02,  1.2256e-01,  2.2518e-02,  2.6060e-03,  3.7903e-02,\n",
      "         1.3026e-01,  9.6942e-02,  1.5105e-01,  1.0362e-01,  3.8678e-02,\n",
      "        -1.7062e-02,  7.7130e-02, -8.0989e-03,  8.9679e-02,  4.5310e-02,\n",
      "         3.1955e-02,  1.4515e-01,  1.3946e-01,  1.0467e-01,  8.5651e-02,\n",
      "         3.8002e-02,  7.0901e-02, -2.7706e-03,  1.6676e-01,  6.6058e-02,\n",
      "         7.0763e-02,  1.0309e-01,  1.2571e-01,  1.1185e-01, -2.6406e-02,\n",
      "         1.1562e-01,  1.0764e-01,  8.2625e-02,  8.2187e-02,  1.2136e-01,\n",
      "         1.0227e-01,  5.1705e-04,  6.5530e-02, -2.6399e-02,  9.7519e-02,\n",
      "         3.5099e-02,  4.2378e-02,  8.3971e-02,  3.0395e-02,  1.0657e-01,\n",
      "         4.7296e-03,  1.5730e-01,  1.5764e-01,  6.0789e-02,  7.3775e-02,\n",
      "         9.2940e-02, -5.4407e-03,  1.0559e-02,  1.0872e-01,  5.7331e-02,\n",
      "         1.5182e-02,  1.1041e-01,  3.9427e-02, -2.1494e-02, -6.2542e-02,\n",
      "         9.0232e-02, -2.5823e-02,  1.6227e-01,  9.9176e-02,  1.3892e-01,\n",
      "         1.0221e-01,  2.2986e-02,  3.3725e-02,  6.9079e-02,  1.5791e-01,\n",
      "         8.1906e-04,  1.2906e-02,  8.7616e-02,  8.7890e-02, -2.0606e-02,\n",
      "        -8.9219e-02,  9.6956e-02,  5.4598e-02, -1.0833e-04,  7.6171e-02,\n",
      "         7.8474e-02, -1.4731e-02,  1.6759e-02, -1.1769e-01,  3.6687e-03,\n",
      "         6.8798e-03, -9.2934e-02,  5.9416e-02, -2.6938e-03,  1.1237e-01,\n",
      "        -4.1928e-03,  2.6443e-02,  5.4423e-02,  8.6308e-02, -6.4684e-02,\n",
      "        -3.2258e-03,  9.4965e-02, -1.6945e-02,  2.7782e-02,  4.9112e-02,\n",
      "         9.4023e-02,  7.4568e-03, -5.0851e-02,  8.4004e-02,  4.5339e-02,\n",
      "         3.1883e-02,  6.2674e-02, -7.2924e-02,  5.9315e-02, -3.0586e-02,\n",
      "        -9.6930e-02,  6.0302e-02,  8.6783e-02, -5.7598e-02, -3.9114e-02,\n",
      "         8.4861e-02,  3.6821e-02,  6.3199e-04,  8.6498e-02, -4.0917e-03,\n",
      "        -6.3934e-02, -7.4754e-02, -6.9053e-02, -4.9628e-02,  1.2700e-02,\n",
      "         1.7934e-02, -1.5705e-02, -5.7048e-03, -1.1144e-01,  7.8114e-02,\n",
      "         1.0876e-01,  7.2816e-03,  5.2817e-04, -1.9561e-04, -2.7981e-02,\n",
      "        -2.9881e-02,  1.7368e-02,  1.6431e-02, -5.1753e-02,  3.8970e-02,\n",
      "        -7.8452e-02,  7.2677e-02, -1.7480e-02, -4.8434e-02, -7.0083e-02,\n",
      "        -4.0022e-02,  6.5427e-02, -4.5558e-02, -6.4923e-02,  5.7429e-02,\n",
      "        -2.2141e-02, -2.6884e-02,  9.9072e-02,  1.0098e-01, -7.8262e-02,\n",
      "        -1.0366e-01,  6.0583e-02, -8.0670e-03, -1.0194e-01,  2.5780e-03,\n",
      "        -1.0711e-02, -6.2633e-02,  6.4049e-02,  1.5831e-03,  1.5034e-02,\n",
      "        -3.3716e-02,  6.7018e-02, -4.3195e-02, -7.6984e-03, -2.5923e-02,\n",
      "         7.9728e-02,  2.0388e-02,  5.7074e-02,  1.7662e-02,  1.1449e-01,\n",
      "        -4.9984e-02, -8.1535e-02,  9.0741e-02,  9.0317e-02, -1.4362e-02,\n",
      "         4.1282e-02,  3.6458e-02, -7.0000e-02,  7.6943e-02,  7.7444e-03,\n",
      "         5.9346e-02,  1.0398e-01,  6.6033e-02,  4.7078e-02,  5.9427e-02,\n",
      "        -5.3957e-02, -2.0938e-02,  2.3806e-02, -5.0095e-04, -7.6882e-02,\n",
      "        -1.2208e-02,  4.5999e-02,  1.6503e-02, -6.5409e-02,  4.7554e-02,\n",
      "         6.8996e-02,  5.1247e-03, -1.5975e-03,  4.4651e-02,  9.5692e-02,\n",
      "         7.7673e-02,  6.2478e-02,  3.1968e-02,  8.5220e-02,  3.0005e-02,\n",
      "         5.7819e-02,  6.6961e-02,  4.2907e-02,  2.0971e-02, -4.7288e-02,\n",
      "        -8.5822e-02,  1.1576e-01, -1.2018e-01, -7.0769e-02,  2.7544e-02,\n",
      "         3.6180e-02,  7.7437e-02, -1.1093e-03,  5.7132e-02,  3.6893e-02,\n",
      "        -1.2080e-02, -1.3377e-02,  4.9594e-02,  4.2630e-02, -5.9363e-02,\n",
      "        -7.2477e-02,  9.4496e-02,  8.3673e-02,  6.4700e-02, -1.3064e-02,\n",
      "        -6.2773e-02,  3.8032e-02,  3.9377e-02, -2.1150e-02, -5.7319e-02,\n",
      "        -1.0991e-02, -7.5666e-02, -4.6884e-02, -8.9037e-02, -5.2019e-02,\n",
      "        -6.2883e-02, -8.7028e-02, -2.9699e-02,  7.1238e-02,  5.4168e-02,\n",
      "         4.7804e-02, -2.3873e-02, -1.0878e-01, -4.4024e-02, -3.5444e-02,\n",
      "         4.4148e-02, -5.7770e-02,  7.8175e-02, -8.8968e-02,  9.0811e-02,\n",
      "         7.1464e-02, -5.9445e-02,  6.5841e-02, -7.1876e-02, -3.4416e-02,\n",
      "         2.4659e-03, -4.5997e-02,  2.1065e-02,  4.9364e-02, -9.3943e-03,\n",
      "        -6.2831e-02, -4.1432e-02,  9.5426e-02,  5.7481e-02, -2.3364e-03,\n",
      "         1.8455e-02,  1.7654e-02, -5.9870e-02,  6.0759e-02, -1.2857e-02,\n",
      "         8.7850e-02, -5.6152e-02, -8.7277e-02,  7.3820e-02,  3.7645e-02,\n",
      "        -6.5240e-02,  4.8305e-02, -2.6232e-02,  8.5555e-02, -6.7466e-02,\n",
      "        -1.1129e-01,  7.1383e-04,  2.3476e-03, -1.1458e-01, -6.3055e-03,\n",
      "        -1.7458e-02, -4.2147e-02,  8.6067e-02, -2.6261e-02, -8.5615e-02,\n",
      "        -6.6670e-03,  2.1572e-02, -1.1263e-01, -5.8536e-03, -1.1679e-03,\n",
      "        -2.1433e-02,  5.6511e-02, -1.7394e-02,  9.5516e-02, -7.2399e-02,\n",
      "         1.8870e-02,  5.6779e-02, -3.4416e-02,  6.8059e-02,  1.3500e-02,\n",
      "         5.7504e-02, -6.5127e-02,  8.4152e-02, -6.3100e-02,  1.0943e-01,\n",
      "         6.2488e-02,  5.9202e-02,  4.0599e-02, -1.0375e-01, -4.3689e-02,\n",
      "        -1.0420e-01, -1.1783e-02, -9.3236e-02, -3.0275e-02,  3.3854e-02,\n",
      "         1.1871e-01,  1.4075e-01,  1.3843e-02, -5.7063e-02,  6.1023e-02,\n",
      "         1.3130e-01, -5.3612e-02,  8.7329e-02,  8.8579e-02,  1.0974e-01,\n",
      "         5.4152e-02,  1.3068e-01,  1.8399e-02,  1.2495e-01,  7.4932e-02,\n",
      "         4.7377e-02,  1.0293e-01,  1.6987e-01,  6.3527e-02,  4.3210e-02,\n",
      "         1.2198e-01,  1.9572e-02, -7.0021e-03,  1.6134e-01,  9.6469e-02,\n",
      "         9.9013e-02,  7.7055e-02,  9.3030e-02,  4.3391e-03,  9.6402e-02,\n",
      "         4.0148e-02,  1.3191e-01,  7.3801e-03,  1.0195e-01,  1.3336e-01,\n",
      "         6.9248e-03,  9.1293e-02,  6.9756e-02,  1.2415e-01,  6.9869e-02,\n",
      "         5.3265e-02,  1.1749e-01,  9.6075e-02, -5.5962e-03,  9.0378e-02,\n",
      "         7.7254e-02,  8.7998e-03,  1.6373e-01,  6.7230e-02, -1.1024e-02,\n",
      "         9.7051e-03,  1.8618e-02,  1.7145e-01,  1.3408e-01,  5.0085e-02,\n",
      "         8.9936e-03,  9.4975e-02, -2.2499e-02,  1.8582e-02, -8.2714e-03,\n",
      "         5.9860e-02,  1.2340e-01, -3.2972e-02,  7.5601e-02,  5.1283e-02,\n",
      "         1.2006e-01,  1.1807e-01,  7.3607e-02,  1.1860e-01,  6.6309e-02,\n",
      "         3.7029e-02,  1.5207e-01, -6.1758e-02,  1.2186e-01,  9.7112e-02,\n",
      "         7.1917e-03,  1.0705e-01,  6.9051e-02,  1.2163e-01, -4.6435e-02,\n",
      "         9.2776e-03,  7.6483e-02,  4.9147e-02,  5.0780e-02, -1.4836e-02,\n",
      "         6.4247e-02,  3.4777e-02,  2.6151e-02,  7.3007e-02,  1.4192e-01,\n",
      "         1.4891e-01,  8.9496e-02, -4.8447e-02,  1.3357e-02,  7.6510e-03,\n",
      "         1.3865e-01,  6.0260e-02,  9.9896e-02,  2.5393e-02, -3.0665e-02,\n",
      "         1.6181e-01,  1.2331e-02,  1.3352e-01,  8.2540e-02,  4.2698e-02,\n",
      "         1.4804e-01,  1.0790e-01,  6.3939e-02, -9.0746e-03,  8.1915e-02,\n",
      "         4.1552e-02,  9.6250e-02, -5.5296e-02,  4.7451e-02, -2.0849e-02,\n",
      "         9.6840e-02,  1.5105e-01,  1.4533e-02,  1.1338e-02,  1.0125e-01,\n",
      "         9.3865e-02,  3.6011e-02, -9.8008e-03,  5.2578e-02, -1.1144e-02,\n",
      "         1.1596e-01,  4.5024e-02], requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[ 0.0373,  0.0167,  0.0521,  ...,  0.0458,  0.0392,  0.0632],\n",
      "        [ 0.0154,  0.0153, -0.0164,  ...,  0.0782,  0.0636,  0.0283],\n",
      "        [ 0.0436,  0.0826, -0.0221,  ..., -0.0634, -0.1178, -0.1323],\n",
      "        ...,\n",
      "        [-0.0067, -0.1863, -0.1037,  ...,  0.0080, -0.0392, -0.1092],\n",
      "        [ 0.0490, -0.1784, -0.0247,  ...,  0.0330, -0.0595, -0.0767],\n",
      "        [ 0.1255, -0.1091, -0.1351,  ..., -0.0077, -0.0245,  0.0261]],\n",
      "       requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([-0.0120, -0.0197,  0.0301,  0.0046,  0.0357,  0.0293,  0.0325,  0.0361,\n",
      "         0.0813,  0.0053,  0.0448, -0.0396, -0.0513, -0.0263,  0.0058, -0.0389,\n",
      "        -0.0176,  0.0076, -0.0220,  0.0119], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_afr, valid_data_afr, test_data_afr), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_afr.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_afr.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_afr.vocab.stoi[train_afr.pad_token]\n",
    "tag_pad_idx = tagged_train_afr.vocab.stoi[tagged_train_afr.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTMTagger_Pretrained(afr_weights_matrix, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate dutch params in dict\n",
    "transfer_param_dict = {}\n",
    "params = model.named_parameters()\n",
    "for name, param in params:\n",
    "    transfer_param_dict[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(transfer_param_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "params2 = model2.named_parameters()\n",
    "for name, param in params2:\n",
    "    if(name == \"embedding.weight\" or name == \"fc.weight\" or name == \"fc.bias\"):\n",
    "        continue\n",
    "    else:\n",
    "        param.data = transfer_param_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 2.510 | Train Acc: 21.95%\n",
      "\t Val. Loss: 2.294 |  Val. Acc: 34.30%\n",
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 2.002 | Train Acc: 49.61%\n",
      "\t Val. Loss: 1.871 |  Val. Acc: 57.52%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "patience = 3\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=False,filename='checkpt_tr_un2.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model2, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model2, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    early_stopping(valid_loss, model2)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping, reloading checkpoint model\")\n",
    "        model2.load_state_dict(torch.load('checkpt_tr_un2.pt'))\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(90.96,93.60) (91.04, 93.04) (90.89, 92.75) (90.98, 93.48),(90.85, 93.17) 2layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
