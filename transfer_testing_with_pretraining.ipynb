{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll #pip3 install this if you don't have it\n",
    "import torchtext.data as tt\n",
    "import torchtext\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_vec = torchtext.vocab.FastText(language='af')\n",
    "du_vec = torchtext.vocab.FastText(language='nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRIKAANS_TRAIN = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "AFRIKAANS_DEV = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu'\n",
    "AFRIKAANS_TEST = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu'\n",
    "\n",
    "DUTCH_TRAIN = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\"\n",
    "DUTCH_DEV = \"UD_Dutch-Alpino/nl_alpino-ud-dev.conllu\"\n",
    "DUTCH_TEST = \"UD_Dutch-Alpino/nl_alpino-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb\n",
    "def make_sentences(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    sentences = []\n",
    "    tagged_sentences = []\n",
    "    for each in data:\n",
    "        tagged_sentence=[]\n",
    "        sentence = []\n",
    "        for token in each:\n",
    "            if token.upos and token.form:\n",
    "                tagged_sentence.append(token.upos)\n",
    "                sentence.append(token.form.lower())\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_afr_raw, tagged_train_afr_raw = make_sentences(AFRIKAANS_TRAIN)\n",
    "dev_afr_raw, tagged_dev_afr_raw = make_sentences(AFRIKAANS_DEV)\n",
    "test_afr_raw, tagged_test_afr_raw = make_sentences(AFRIKAANS_TEST)\n",
    "\n",
    "train_du_raw, tagged_train_du_raw = make_sentences(DUTCH_TRAIN)\n",
    "dev_du_raw, tagged_dev_du_raw = make_sentences(DUTCH_DEV)\n",
    "test_du_raw, tagged_test_du_raw = make_sentences(DUTCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFRIKAANS\n",
      "Tagged sentences in train set:  1315\n",
      "Tagged words in train set: 33894\n",
      "========================================\n",
      "Tagged sentences in dev set:  194\n",
      "Tagged words in dev set: 5317\n",
      "========================================\n",
      "Tagged sentences in test set:  425\n",
      "Tagged words in test set: 10065\n",
      "****************************************\n",
      "Total sentences in dataset: 1703\n"
     ]
    }
   ],
   "source": [
    "print(\"AFRIKAANS\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_afr_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_afr_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_afr_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_afr_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_afr_raw)+len(tagged_dev_afr_raw)+len(tagged_dev_afr_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH\n",
      "Tagged sentences in train set:  12264\n",
      "Tagged words in train set: 185999\n",
      "========================================\n",
      "Tagged sentences in dev set:  718\n",
      "Tagged words in dev set: 11549\n",
      "========================================\n",
      "Tagged sentences in test set:  596\n",
      "Tagged words in test set: 11053\n",
      "****************************************\n",
      "Total sentences in dataset: 13700\n"
     ]
    }
   ],
   "source": [
    "print(\"DUTCH\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_du_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_du_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_du_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_du_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_du_raw)+len(tagged_dev_du_raw)+len(tagged_dev_du_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tringm/POSTagger_Pytorch/blob/master/src/util/nlp.py\n",
    "def build_tag_field(sentences_tokens):\n",
    "    token_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('tokens', token_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_tokens]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return token_field\n",
    "    \n",
    "def build_text_field(sentences_words):\n",
    "    text_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_words]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, AFR\n",
    "train_afr = build_text_field(train_afr_raw)\n",
    "dev_afr = build_text_field(dev_afr_raw)\n",
    "test_afr = build_text_field(test_afr_raw)\n",
    "tagged_train_afr = build_tag_field(tagged_train_afr_raw)\n",
    "tagged_dev_afr = build_tag_field(tagged_dev_afr_raw)\n",
    "tagged_test_afr = build_tag_field(tagged_test_afr_raw)\n",
    "\n",
    "fields_train_afr = ((\"text\", train_afr), (\"udtags\", tagged_train_afr))\n",
    "examples_train_afr = [tt.Example.fromlist(item, fields_train_afr) for item in zip(train_afr_raw, tagged_train_afr_raw)]\n",
    "fields_dev_afr = ((\"text\", dev_afr), (\"udtags\", tagged_dev_afr))\n",
    "examples_dev_afr = [tt.Example.fromlist(item, fields_dev_afr) for item in zip(dev_afr_raw, tagged_dev_afr_raw)]\n",
    "fields_test_afr = ((\"text\", test_afr), (\"udtags\", tagged_test_afr))\n",
    "examples_test_afr = [tt.Example.fromlist(item, fields_test_afr) for item in zip(test_afr_raw, tagged_test_afr_raw)]\n",
    "\n",
    "train_data_afr = tt.Dataset(examples_train_afr, fields_train_afr)\n",
    "valid_data_afr = tt.Dataset(examples_dev_afr, fields_dev_afr)\n",
    "test_data_afr = tt.Dataset(examples_test_afr, fields_test_afr)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "dev_afr.vocab = train_afr.vocab\n",
    "test_afr.vocab = train_afr.vocab\n",
    "tagged_train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "tagged_dev_afr.vocab = tagged_train_afr.vocab\n",
    "tagged_test_afr.vocab = tagged_train_afr.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, DUT\n",
    "train_du = build_text_field(train_du_raw)\n",
    "dev_du = build_text_field(dev_du_raw)\n",
    "test_du = build_text_field(test_du_raw)\n",
    "tagged_train_du = build_tag_field(tagged_train_du_raw)\n",
    "tagged_dev_du = build_tag_field(tagged_dev_du_raw)\n",
    "tagged_test_du = build_tag_field(tagged_test_du_raw)\n",
    "\n",
    "fields_train_du = ((\"text\", train_du), (\"udtags\", tagged_train_du))\n",
    "examples_train_du = [tt.Example.fromlist(item, fields_train_du) for item in zip(train_du_raw, tagged_train_du_raw)]\n",
    "fields_dev_du = ((\"text\", dev_du), (\"udtags\", tagged_dev_du))\n",
    "examples_dev_du = [tt.Example.fromlist(item, fields_dev_du) for item in zip(dev_du_raw, tagged_dev_du_raw)]\n",
    "fields_test_du = ((\"text\", test_du), (\"udtags\", tagged_test_du))\n",
    "examples_test_du = [tt.Example.fromlist(item, fields_test_du) for item in zip(test_du_raw, tagged_test_du_raw)]\n",
    "\n",
    "train_data_du = tt.Dataset(examples_train_du, fields_train_du)\n",
    "valid_data_du = tt.Dataset(examples_dev_du, fields_dev_du)\n",
    "test_data_du = tt.Dataset(examples_test_du, fields_test_du)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "dev_du.vocab = train_du.vocab\n",
    "test_du.vocab = train_du.vocab\n",
    "tagged_train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "tagged_dev_du.vocab = tagged_train_du.vocab\n",
    "tagged_test_du.vocab = tagged_train_du.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afrikaans words missing:  0\n",
      "dutch words missing:  0\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "afr_matrix_len = len(train_afr.vocab.itos)\n",
    "afr_weights_matrix = torch.zeros((afr_matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "words_missing = 0\n",
    "\n",
    "for i, word in enumerate(train_afr.vocab.itos):\n",
    "#     print(i, word)\n",
    "    try: \n",
    "        afr_weights_matrix[i] = af_vec[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "#         print(\"test\")\n",
    "        afr_weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "        words_missing += 1\n",
    "\n",
    "print(\"Afrikaans words missing: \", words_missing)\n",
    "\n",
    "du_matrix_len = len(train_du.vocab.itos)\n",
    "du_weights_matrix = torch.zeros((du_matrix_len, embedding_dim))\n",
    "words_found = 0\n",
    "words_missing = 0\n",
    "\n",
    "for i, word in enumerate(train_afr.vocab.itos):\n",
    "#     print(i, word)\n",
    "    try: \n",
    "        du_weights_matrix[i] = du_vec[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "#         print(\"test\")\n",
    "        du_weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "        words_missing += 1\n",
    "\n",
    "print(\"dutch words missing: \", words_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, pad_idx, non_trainable=False):\n",
    "    input_dim, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, input_dim, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "#model\n",
    "batch_size=128\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#needs to be tuple of dataset objects\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_du, valid_data_du, test_data_du), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions\n",
    "\n",
    "class BiLSTMTagger_Pretrained(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, weights_matrix, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding, input_dim, embedding_dim = create_emb_layer(weights_matrix, pad_idx, False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_du.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_du.vocab)\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "pad_index = train_du.vocab.stoi[train_du.pad_token]\n",
    "tag_pad_idx = tagged_train_du.vocab.stoi[tagged_train_du.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMTagger_Pretrained(du_weights_matrix, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(text)        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags) \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 11s\n",
      "\tTrain Loss: 1.612 | Train Acc: 50.95%\n",
      "\t Val. Loss: 0.706 |  Val. Acc: 79.59%\n",
      "Epoch: 02 | Epoch Time: 1m 53s\n",
      "\tTrain Loss: 0.329 | Train Acc: 91.16%\n",
      "\t Val. Loss: 0.321 |  Val. Acc: 90.34%\n",
      "Epoch: 03 | Epoch Time: 1m 41s\n",
      "\tTrain Loss: 0.130 | Train Acc: 96.46%\n",
      "\t Val. Loss: 0.269 |  Val. Acc: 91.37%\n",
      "Epoch: 04 | Epoch Time: 1m 25s\n",
      "\tTrain Loss: 0.087 | Train Acc: 97.46%\n",
      "\t Val. Loss: 0.284 |  Val. Acc: 91.07%\n",
      "Epoch: 05 | Epoch Time: 1m 16s\n",
      "\tTrain Loss: 0.067 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 90.88%\n",
      "Epoch: 06 | Epoch Time: 1m 22s\n",
      "\tTrain Loss: 0.053 | Train Acc: 98.43%\n",
      "\t Val. Loss: 0.312 |  Val. Acc: 90.92%\n",
      "Epoch: 07 | Epoch Time: 1m 13s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.81%\n",
      "\t Val. Loss: 0.339 |  Val. Acc: 90.89%\n",
      "Epoch: 08 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.031 | Train Acc: 99.16%\n",
      "\t Val. Loss: 0.334 |  Val. Acc: 91.09%\n",
      "Epoch: 09 | Epoch Time: 1m 21s\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.39%\n",
      "\t Val. Loss: 0.406 |  Val. Acc: 91.02%\n",
      "Epoch: 10 | Epoch Time: 1m 18s\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.55%\n",
      "\t Val. Loss: 0.377 |  Val. Acc: 91.33%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.415 |  Test Acc: 91.23%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding.weight', Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1028, -0.0077,  0.1081,  ..., -0.0456, -0.0399,  0.0344],\n",
      "        ...,\n",
      "        [ 0.0195,  0.0151, -0.0267,  ..., -0.0334, -0.0371,  0.0006],\n",
      "        [-0.0266, -0.0268,  0.0068,  ..., -0.0028,  0.0209,  0.0052],\n",
      "        [ 0.0408, -0.0402,  0.0251,  ..., -0.0093,  0.0122, -0.0263]],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.1289,  0.0157, -0.0014,  ..., -0.0678, -0.0357,  0.0552],\n",
      "        [ 0.0866, -0.1017, -0.0422,  ..., -0.0014, -0.0085, -0.0364],\n",
      "        [ 0.1485, -0.0080, -0.1188,  ...,  0.0073,  0.0518,  0.0063],\n",
      "        ...,\n",
      "        [-0.1028,  0.1237,  0.0354,  ..., -0.0605, -0.0718,  0.1619],\n",
      "        [ 0.0479, -0.0212, -0.0422,  ...,  0.0163,  0.0509, -0.0606],\n",
      "        [-0.2298, -0.1074,  0.0397,  ..., -0.0796,  0.1078,  0.1057]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.0937, -0.1037, -0.1328,  ...,  0.1095, -0.1098, -0.1628],\n",
      "        [ 0.0011,  0.0108, -0.0614,  ...,  0.0918, -0.0736,  0.0459],\n",
      "        [ 0.0179, -0.0579, -0.1703,  ...,  0.0826, -0.0095, -0.1009],\n",
      "        ...,\n",
      "        [ 0.0029,  0.0020, -0.0166,  ...,  0.0058, -0.1332,  0.0169],\n",
      "        [-0.0053,  0.0512, -0.0632,  ...,  0.0338, -0.1143, -0.1305],\n",
      "        [ 0.0624,  0.0081, -0.0746,  ...,  0.0287, -0.0517, -0.0516]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0', Parameter containing:\n",
      "tensor([ 0.0819,  0.0417,  0.1220,  0.0364,  0.0826,  0.1599,  0.1417,  0.1081,\n",
      "         0.0149,  0.0153,  0.0929, -0.0116,  0.1368,  0.1747,  0.0263,  0.0920,\n",
      "         0.0902,  0.0436, -0.0507,  0.0401,  0.0147,  0.1565,  0.0434,  0.0776,\n",
      "         0.0220,  0.1248,  0.1661,  0.0928,  0.1831,  0.0906,  0.0898,  0.1577,\n",
      "         0.2217,  0.0985,  0.0629,  0.1138,  0.0174,  0.1631,  0.0614,  0.1335,\n",
      "         0.1872,  0.0410,  0.0546,  0.1065,  0.1420,  0.1040,  0.1608,  0.1251,\n",
      "         0.0446, -0.0164,  0.0738,  0.0915,  0.0435,  0.1204,  0.1446,  0.0620,\n",
      "         0.0268,  0.1439,  0.0617,  0.1842,  0.1105,  0.1572,  0.1741,  0.1191,\n",
      "         0.1365,  0.1544,  0.0646,  0.0548,  0.0856,  0.1314,  0.0639,  0.1157,\n",
      "         0.1789,  0.1039,  0.0826,  0.0460,  0.0136,  0.0767, -0.0149,  0.0306,\n",
      "         0.0218,  0.1158, -0.0035,  0.0828,  0.1269,  0.0880,  0.0930, -0.0170,\n",
      "         0.0725, -0.0015, -0.0035,  0.0311,  0.1999,  0.0797,  0.0552, -0.0079,\n",
      "         0.1322,  0.0197,  0.1641,  0.1493,  0.0236,  0.0006,  0.1527, -0.0022,\n",
      "         0.1272,  0.0568, -0.0516,  0.0758,  0.0808,  0.0636,  0.0866,  0.0152,\n",
      "         0.1521,  0.0721,  0.0262,  0.0389,  0.1379,  0.1247,  0.0064,  0.0927,\n",
      "         0.0552,  0.0586, -0.0129,  0.1110, -0.0322,  0.0711,  0.1304,  0.0468,\n",
      "         0.0608, -0.0265, -0.0625,  0.0400,  0.0262, -0.0596,  0.0290, -0.0610,\n",
      "        -0.0117, -0.0833, -0.0699,  0.0516,  0.0067,  0.0885, -0.1340,  0.0463,\n",
      "         0.0501,  0.0648,  0.0953,  0.0402, -0.0070, -0.0937,  0.0699,  0.0513,\n",
      "         0.0535,  0.0358, -0.0366,  0.0366, -0.0413,  0.0593, -0.0247, -0.1052,\n",
      "        -0.0143, -0.0569,  0.0277, -0.0275, -0.0604,  0.1234,  0.0012,  0.0570,\n",
      "        -0.1373, -0.0114, -0.0462,  0.0397,  0.0672, -0.1036, -0.0451,  0.0367,\n",
      "         0.0668,  0.0671,  0.0198, -0.0110, -0.0113,  0.0716, -0.0136,  0.0559,\n",
      "         0.0053,  0.0845, -0.0020, -0.0672,  0.0012, -0.0880,  0.0661, -0.0205,\n",
      "        -0.0837,  0.0616,  0.0359,  0.1163, -0.0549,  0.0928, -0.1020,  0.0568,\n",
      "        -0.0944,  0.0959,  0.0125,  0.0998, -0.0698,  0.0869,  0.0817,  0.0476,\n",
      "        -0.0106, -0.0591, -0.0286,  0.0545,  0.0385, -0.0692,  0.0624,  0.0553,\n",
      "         0.0847, -0.0052,  0.0109, -0.0460,  0.0038,  0.0066, -0.0058,  0.0291,\n",
      "        -0.0518, -0.0037,  0.0049, -0.0241,  0.0774,  0.1024, -0.0527,  0.0048,\n",
      "         0.0736,  0.0517, -0.0023,  0.0836, -0.0204,  0.0027,  0.0829, -0.0727,\n",
      "        -0.0744,  0.0222,  0.0056, -0.0030, -0.0592,  0.0026,  0.1143, -0.0560,\n",
      "         0.0198, -0.0490,  0.0170,  0.0573,  0.0925, -0.0511,  0.1029, -0.0432,\n",
      "        -0.0567, -0.1124, -0.0556,  0.0972, -0.0623,  0.0847,  0.1033, -0.0846,\n",
      "         0.0201,  0.0211, -0.0453,  0.1261,  0.0832,  0.0029,  0.0822,  0.0805,\n",
      "         0.0576,  0.0121,  0.0009,  0.0424,  0.0445,  0.0124, -0.0075, -0.0015,\n",
      "        -0.0774,  0.0248,  0.0020, -0.0293, -0.0380,  0.0576, -0.0128, -0.0465,\n",
      "        -0.0713, -0.0084,  0.1003,  0.0304,  0.0084, -0.0219, -0.0254, -0.0344,\n",
      "         0.0583, -0.1004, -0.0905,  0.0639, -0.0437,  0.0830, -0.0666, -0.0512,\n",
      "        -0.1054,  0.1022,  0.1020, -0.0401,  0.0914, -0.0862, -0.0163, -0.0112,\n",
      "        -0.0971,  0.1171, -0.0434,  0.0103, -0.0121,  0.0073,  0.0860,  0.0634,\n",
      "         0.0911,  0.0760,  0.1083, -0.0772, -0.0216, -0.0962, -0.0065, -0.0124,\n",
      "         0.1024,  0.0837,  0.0454, -0.0506, -0.1076, -0.0356, -0.0269, -0.0502,\n",
      "        -0.0660, -0.0413, -0.0446,  0.0313,  0.0034, -0.0869,  0.0643, -0.0973,\n",
      "        -0.0274,  0.0366, -0.0455,  0.1061,  0.0144, -0.0220, -0.0972, -0.0934,\n",
      "         0.1069, -0.0441, -0.0941,  0.1036, -0.0988,  0.0316, -0.1019,  0.1124,\n",
      "         0.0053,  0.0487,  0.0816,  0.1107, -0.0015, -0.0094,  0.0837,  0.0205,\n",
      "         0.0041, -0.0864,  0.1035,  0.0219,  0.0672,  0.0227,  0.0624,  0.0320,\n",
      "        -0.0187,  0.0900, -0.0558,  0.0986, -0.0560,  0.0268,  0.0327, -0.1266,\n",
      "         0.0474, -0.0108, -0.0121,  0.1467,  0.1154,  0.0917,  0.1497, -0.0446,\n",
      "         0.1062,  0.0729,  0.1660, -0.0288,  0.0761,  0.0897,  0.0552,  0.1051,\n",
      "         0.1437,  0.0943, -0.0044,  0.1689, -0.0596,  0.1591,  0.1288,  0.0852,\n",
      "        -0.0041,  0.0544,  0.0918,  0.0608,  0.0817,  0.0669, -0.0447,  0.0444,\n",
      "         0.1208,  0.0929,  0.1068,  0.0120,  0.1111,  0.1638,  0.0055,  0.1514,\n",
      "         0.0374,  0.0902,  0.0954, -0.0056,  0.0329,  0.0082,  0.0202,  0.1209,\n",
      "        -0.0175, -0.0576,  0.1179,  0.1352,  0.0439,  0.0797,  0.0735,  0.1096,\n",
      "         0.1473, -0.0322,  0.0672,  0.0967,  0.1179,  0.0906,  0.1081,  0.1367,\n",
      "         0.1688,  0.1317,  0.0426, -0.0015,  0.0972,  0.0636,  0.1548,  0.0522,\n",
      "         0.0142,  0.0596,  0.1746, -0.0276,  0.0613,  0.0888,  0.0954,  0.1323,\n",
      "         0.0403,  0.0660, -0.0158, -0.0642,  0.0427,  0.1744,  0.1084,  0.0238,\n",
      "         0.0473,  0.0668, -0.0009,  0.1060,  0.0442,  0.0547, -0.0447,  0.0626,\n",
      "         0.0132,  0.0659,  0.0390,  0.1051,  0.1162,  0.0302,  0.0605,  0.1265,\n",
      "         0.0485, -0.0557, -0.0095,  0.0713,  0.1725,  0.0693,  0.0556,  0.1034,\n",
      "         0.1241,  0.0455,  0.1203,  0.0697,  0.1398,  0.1325,  0.1077,  0.0334,\n",
      "         0.1141,  0.1130,  0.0092, -0.0142,  0.0790, -0.0186,  0.0826,  0.0404],\n",
      "       requires_grad=True)), ('lstm.bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.1422,  0.0893,  0.0635,  0.1724,  0.0811,  0.1112,  0.0406, -0.0416,\n",
      "         0.0968,  0.1502,  0.0772, -0.0165,  0.0634,  0.1588,  0.1381,  0.1060,\n",
      "         0.0422,  0.1101, -0.0330,  0.0857, -0.0220,  0.1251,  0.1312,  0.1010,\n",
      "         0.0588,  0.1291,  0.1156,  0.0678,  0.0866,  0.0993,  0.1439,  0.1877,\n",
      "         0.1596,  0.1965,  0.0885,  0.1353, -0.0196,  0.1435,  0.1394,  0.1378,\n",
      "         0.1621,  0.1547,  0.0408,  0.0896,  0.1482,  0.0947,  0.1452,  0.0749,\n",
      "        -0.0129, -0.0446, -0.0313,  0.0742,  0.0086,  0.1056,  0.0795, -0.0289,\n",
      "         0.0327,  0.1167,  0.1022,  0.1271,  0.1101,  0.0172,  0.0686,  0.1071,\n",
      "         0.1622,  0.1179, -0.0207,  0.0311,  0.0163,  0.0663,  0.0814,  0.0820,\n",
      "         0.0421,  0.0185,  0.0610,  0.1250, -0.0497,  0.0366,  0.0702,  0.0799,\n",
      "         0.1521,  0.1678, -0.0127, -0.0321,  0.1511,  0.1902, -0.0250,  0.0666,\n",
      "        -0.0104,  0.1004,  0.0526,  0.0257,  0.1339, -0.0565,  0.0771,  0.1011,\n",
      "         0.0504,  0.1072,  0.0767,  0.0527,  0.0100,  0.0721,  0.1508,  0.1182,\n",
      "         0.0809, -0.0204,  0.0828,  0.0802,  0.1149,  0.0268,  0.0580,  0.0428,\n",
      "         0.0996,  0.1267,  0.1225,  0.0669,  0.1096,  0.0991,  0.0159, -0.0046,\n",
      "         0.1000, -0.0073, -0.0424,  0.0428,  0.0530,  0.1180,  0.0300,  0.1125,\n",
      "        -0.0512,  0.0497,  0.0755, -0.0501,  0.0718,  0.1016,  0.0313, -0.0564,\n",
      "         0.0811, -0.0818, -0.0570, -0.0011, -0.0957, -0.0027, -0.1155,  0.0830,\n",
      "         0.0383, -0.0599,  0.0584, -0.0443,  0.0686, -0.1179, -0.0511,  0.0342,\n",
      "         0.0695, -0.0675, -0.0104,  0.0041, -0.0729, -0.0532, -0.0068, -0.0679,\n",
      "        -0.0067, -0.0913, -0.0333,  0.0181, -0.0564,  0.1341, -0.0197,  0.0313,\n",
      "        -0.1417,  0.0743, -0.0498,  0.0614,  0.0250, -0.0282, -0.0237,  0.0630,\n",
      "         0.0019, -0.0595, -0.0728,  0.0341,  0.0526,  0.0357, -0.0691,  0.0529,\n",
      "        -0.0350,  0.0140,  0.0188,  0.0338,  0.0331, -0.0766, -0.0068, -0.0933,\n",
      "        -0.0063, -0.0234,  0.0329, -0.0345, -0.0230,  0.0224, -0.1065, -0.0065,\n",
      "        -0.1381,  0.0964,  0.0999, -0.0003, -0.0403,  0.0041, -0.0017,  0.0646,\n",
      "         0.0006, -0.0478, -0.0694, -0.0255, -0.0494,  0.1003, -0.0013,  0.0296,\n",
      "         0.0872,  0.0304, -0.0256,  0.0960, -0.0761,  0.0706, -0.0528,  0.0745,\n",
      "        -0.0701,  0.1267,  0.0639,  0.0182,  0.0174,  0.0004,  0.0268, -0.0222,\n",
      "        -0.0413, -0.0075, -0.0579,  0.0075,  0.0982,  0.0254,  0.0227, -0.0063,\n",
      "        -0.0364,  0.0132,  0.0149, -0.0414, -0.0711, -0.0315,  0.0643, -0.0024,\n",
      "        -0.0408,  0.0312,  0.0379,  0.0142,  0.0434, -0.0014, -0.0393,  0.0218,\n",
      "         0.0845, -0.1130, -0.0911,  0.0938, -0.0594,  0.0516,  0.0423, -0.0284,\n",
      "        -0.0338, -0.0285, -0.1356, -0.0160,  0.0338, -0.0538,  0.0645,  0.0182,\n",
      "         0.0691, -0.0499,  0.0328, -0.0006,  0.1071, -0.0699,  0.0647, -0.0221,\n",
      "        -0.0982,  0.0471, -0.0882,  0.0702,  0.0463,  0.0562, -0.0305,  0.0497,\n",
      "        -0.0232, -0.0017,  0.0399,  0.0444, -0.1071,  0.0949,  0.0126, -0.0862,\n",
      "        -0.0550,  0.0298,  0.0026,  0.0369,  0.0105,  0.0856, -0.0159,  0.1033,\n",
      "        -0.1266,  0.0454,  0.0323,  0.0871,  0.0751,  0.0441, -0.1021, -0.0296,\n",
      "        -0.0718,  0.0566, -0.0771, -0.0763,  0.0952,  0.0117, -0.0072, -0.0098,\n",
      "         0.0802,  0.0857,  0.0212, -0.1008, -0.0440,  0.0435, -0.0533, -0.0217,\n",
      "        -0.0300,  0.0656, -0.0286, -0.0774, -0.0668, -0.1037, -0.0384,  0.1035,\n",
      "         0.0099,  0.0796, -0.1016, -0.0777, -0.0760, -0.0352,  0.0718,  0.0041,\n",
      "        -0.0645,  0.0259,  0.0368,  0.0558,  0.0151,  0.0580, -0.0027, -0.0303,\n",
      "         0.1024,  0.1195, -0.0124,  0.0654, -0.0470,  0.1282, -0.0495,  0.1041,\n",
      "         0.0902, -0.0060, -0.0133,  0.0691, -0.0916,  0.0911, -0.0382, -0.0272,\n",
      "        -0.0698, -0.0572,  0.0690, -0.1116, -0.0063, -0.0121, -0.0155,  0.0919,\n",
      "        -0.0412, -0.0280, -0.1119,  0.0394, -0.0635,  0.0787, -0.1277, -0.1297,\n",
      "         0.1037,  0.1190,  0.1201,  0.0943,  0.0286,  0.1279,  0.1967,  0.0647,\n",
      "         0.0336,  0.0122,  0.1335, -0.0412,  0.0239,  0.1533,  0.1969,  0.0349,\n",
      "         0.1085,  0.0082,  0.0097,  0.0210, -0.0221,  0.0610,  0.0436,  0.0885,\n",
      "         0.0102,  0.0203,  0.1268,  0.1178,  0.0854,  0.0722,  0.0372,  0.1371,\n",
      "         0.1731,  0.1283,  0.1293,  0.0953, -0.0291,  0.1181, -0.0146,  0.0854,\n",
      "         0.1089, -0.0305,  0.0762,  0.1069,  0.0412,  0.0510,  0.1053,  0.1004,\n",
      "        -0.0158,  0.0303,  0.0972,  0.0317, -0.0237,  0.1463,  0.0009,  0.0753,\n",
      "         0.0829,  0.0727,  0.1502,  0.1501, -0.0338,  0.0644,  0.1508,  0.0719,\n",
      "         0.1388,  0.0375,  0.1088,  0.0409,  0.1150,  0.0183,  0.0198,  0.0494,\n",
      "         0.0011,  0.0160,  0.0968,  0.0739,  0.0676,  0.0098,  0.1000, -0.0231,\n",
      "         0.0142,  0.0093, -0.0205,  0.0510,  0.1334,  0.0885,  0.0704, -0.0576,\n",
      "         0.0651,  0.1205,  0.0422,  0.0696,  0.0839, -0.0487, -0.0075, -0.0108,\n",
      "         0.0251,  0.0723,  0.0287,  0.0669,  0.0810,  0.0083,  0.0816,  0.1283,\n",
      "         0.0685,  0.0675,  0.0745,  0.0695,  0.1155,  0.0923,  0.0396,  0.0132,\n",
      "         0.1506,  0.0069,  0.0750,  0.0379,  0.1540, -0.0401,  0.1175,  0.0820,\n",
      "         0.1056,  0.1572,  0.0362, -0.0035,  0.0102,  0.0601,  0.0406,  0.0294],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[-0.0227,  0.0992, -0.0481,  ..., -0.0159, -0.0208, -0.0341],\n",
      "        [ 0.0127,  0.0652,  0.0798,  ..., -0.0078, -0.0699,  0.0939],\n",
      "        [-0.0227, -0.0858,  0.0699,  ..., -0.0692, -0.1007,  0.0167],\n",
      "        ...,\n",
      "        [ 0.0063, -0.1824, -0.1856,  ..., -0.1113, -0.0619, -0.0820],\n",
      "        [ 0.1151, -0.1210,  0.1515,  ..., -0.0425, -0.1175,  0.0241],\n",
      "        [-0.0060, -0.0913, -0.0013,  ..., -0.0772,  0.0101, -0.0622]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.0517, -0.0208, -0.0692,  ...,  0.0148, -0.0182, -0.0259],\n",
      "        [-0.0281,  0.0351, -0.0080,  ...,  0.0184, -0.0278, -0.0303],\n",
      "        [ 0.0240,  0.1127, -0.0290,  ..., -0.0314,  0.0855, -0.0951],\n",
      "        ...,\n",
      "        [-0.0185,  0.0515,  0.0115,  ..., -0.0851,  0.1124,  0.0380],\n",
      "        [-0.0301,  0.0712, -0.0684,  ..., -0.0107,  0.0819, -0.0571],\n",
      "        [-0.1202,  0.1205, -0.1613,  ..., -0.0697,  0.0618, -0.0352]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([-3.4676e-02,  4.9264e-02, -4.7673e-02,  6.8915e-02,  2.3737e-02,\n",
      "         8.7408e-02,  3.3959e-02,  5.4892e-02,  1.0899e-01,  1.2926e-02,\n",
      "         6.3004e-02,  1.1647e-01,  3.0983e-02,  1.1200e-01,  5.1599e-02,\n",
      "         7.5917e-02,  2.3336e-02,  1.3592e-01,  1.8653e-02,  1.2122e-01,\n",
      "         1.1813e-03,  1.0805e-01,  1.3568e-01,  1.1456e-01,  7.4852e-03,\n",
      "         3.6084e-02,  1.4639e-01,  6.7231e-02, -1.7795e-02, -2.3519e-02,\n",
      "        -3.9305e-02,  1.5028e-01,  7.7646e-02,  6.5455e-02,  4.1681e-02,\n",
      "         5.1045e-02,  3.0830e-02,  8.5038e-02,  7.4651e-02,  9.3431e-02,\n",
      "         7.4956e-02, -2.9438e-02,  4.0764e-02,  1.5211e-01,  7.7702e-02,\n",
      "         1.0669e-01, -1.9065e-02,  1.2572e-01,  9.6936e-02,  1.2684e-01,\n",
      "        -3.2234e-02, -8.3225e-03,  1.3711e-01, -3.8739e-02,  7.4458e-02,\n",
      "         1.5808e-01, -1.5659e-02,  7.0021e-02,  5.1057e-02,  5.7606e-03,\n",
      "        -5.0661e-02,  1.0125e-01,  1.8712e-03, -1.0418e-02, -1.7603e-02,\n",
      "         4.8478e-03,  9.5259e-02,  5.1164e-02,  1.0269e-01,  1.3086e-01,\n",
      "        -9.2538e-03, -1.1967e-02,  9.4389e-02,  1.0655e-01, -5.6004e-02,\n",
      "        -1.9420e-02,  1.2817e-01,  1.1297e-02,  4.0282e-02,  1.2267e-01,\n",
      "         5.4090e-02,  9.7878e-02,  9.5938e-02, -2.9172e-02,  1.8223e-02,\n",
      "         1.4649e-01, -6.5647e-02,  7.6159e-02, -6.6502e-02, -5.3355e-02,\n",
      "         3.8916e-02,  6.4804e-02, -5.7308e-02, -7.9816e-03, -4.6182e-02,\n",
      "         1.0345e-01,  9.0826e-02, -3.7284e-02,  1.0631e-01,  5.7650e-02,\n",
      "         8.8307e-02,  1.5017e-03,  3.7562e-02,  1.2875e-01,  3.8680e-02,\n",
      "         6.2799e-02,  4.9628e-02,  9.4180e-03,  3.2414e-02,  9.5780e-02,\n",
      "         1.3074e-01,  9.7485e-02,  8.0132e-03,  9.3869e-02,  2.8121e-02,\n",
      "         1.1662e-01,  3.7150e-02,  1.1222e-01,  3.9379e-03,  2.0462e-02,\n",
      "         8.8329e-02,  7.8646e-02,  3.1939e-03, -7.4646e-04, -3.8195e-03,\n",
      "        -9.4897e-03,  1.4233e-01,  5.9135e-02,  6.3164e-03,  3.5014e-03,\n",
      "         7.4374e-02,  8.3815e-02,  7.6462e-02,  1.5202e-02, -5.5459e-02,\n",
      "         4.8876e-02,  1.0652e-01,  4.8747e-02,  3.7782e-02, -5.3777e-02,\n",
      "        -6.9528e-02,  9.1075e-02,  2.8354e-03, -8.3721e-02, -8.8125e-03,\n",
      "        -1.7813e-02, -1.3188e-02,  5.5235e-02,  7.3940e-02, -3.0629e-02,\n",
      "        -5.6413e-02,  3.0399e-02,  2.1812e-02, -4.4293e-02,  1.5050e-02,\n",
      "         3.7371e-02,  2.0864e-02, -5.5480e-02, -4.0255e-02, -1.0363e-02,\n",
      "         7.1583e-03,  1.8849e-02,  4.5277e-02,  3.5510e-02, -2.8780e-02,\n",
      "         9.5474e-02,  9.6660e-02,  3.5049e-02, -1.8799e-02,  7.3462e-02,\n",
      "        -4.0026e-02, -3.9760e-02,  6.0090e-02,  1.1110e-02, -5.4182e-02,\n",
      "        -4.9158e-03,  3.1252e-02, -3.3120e-02, -5.5647e-02,  5.4000e-02,\n",
      "         1.8634e-02, -7.1453e-02,  5.9580e-02, -6.7513e-02, -1.3736e-02,\n",
      "        -5.4569e-03, -1.4722e-02, -3.5491e-03,  7.4643e-02,  8.4319e-02,\n",
      "         7.5225e-02, -2.1173e-02, -4.9459e-02,  6.3066e-03,  2.3023e-02,\n",
      "        -6.7904e-02,  2.8951e-02,  2.2299e-02, -1.1863e-02, -6.2464e-02,\n",
      "         8.3688e-02,  4.0182e-02,  1.0014e-01,  5.6860e-02, -4.6892e-02,\n",
      "         1.7341e-02,  9.7093e-03, -3.0320e-03,  5.9444e-02,  7.5903e-02,\n",
      "         1.0616e-02,  9.2066e-02, -1.2393e-02,  6.1702e-02, -5.1294e-02,\n",
      "        -3.4415e-02,  2.9626e-04,  4.9812e-02, -6.8233e-03, -3.9192e-02,\n",
      "        -6.9369e-02,  5.8093e-03,  3.9153e-02, -4.1933e-02, -6.5504e-02,\n",
      "         5.2738e-02, -3.5561e-02, -5.2728e-02,  1.9990e-02,  3.4995e-02,\n",
      "        -5.9809e-02, -1.4810e-02, -5.4744e-02,  1.0453e-01, -1.6786e-02,\n",
      "         4.7447e-02,  3.1894e-02, -3.4108e-02, -1.0247e-02, -3.5281e-02,\n",
      "        -4.1980e-02, -5.0792e-02, -4.0600e-02, -5.2748e-02,  2.2768e-02,\n",
      "         1.5350e-02, -1.7531e-02,  3.5221e-02, -3.2281e-02, -3.4856e-02,\n",
      "         4.5093e-02,  1.1931e-02,  3.1679e-02, -1.2253e-02, -5.7292e-02,\n",
      "         6.1274e-02,  2.9140e-02, -2.1071e-03,  1.9795e-02,  8.2614e-02,\n",
      "        -5.6755e-02, -6.1229e-02,  6.9982e-02, -7.9570e-02, -3.3775e-02,\n",
      "         5.4244e-02, -7.5894e-02, -6.9832e-02, -4.8463e-02, -5.3435e-02,\n",
      "         1.8310e-02,  1.4476e-02,  6.7426e-02,  5.1087e-02, -6.5082e-05,\n",
      "        -7.8037e-02, -9.6690e-02,  9.5877e-02,  5.1909e-02, -5.3313e-02,\n",
      "        -8.5332e-02, -6.3325e-02, -3.2182e-02,  7.8549e-03,  5.1356e-02,\n",
      "         5.8490e-02,  2.5261e-02, -9.2179e-02,  6.9390e-02,  4.7787e-02,\n",
      "        -1.7412e-02,  8.5747e-02, -5.3308e-02, -4.5241e-02, -3.9688e-02,\n",
      "        -8.4436e-02,  1.6659e-02,  6.7132e-02, -2.5088e-02, -6.8314e-02,\n",
      "        -4.8573e-03, -8.2402e-02,  2.9358e-02, -8.0770e-02,  2.8730e-02,\n",
      "         7.8656e-02, -6.4790e-02,  8.8215e-02, -3.7254e-02, -8.9534e-02,\n",
      "         8.8056e-02,  5.3142e-02, -1.6581e-02,  2.5895e-02,  4.0850e-02,\n",
      "        -6.6724e-02,  2.8494e-02,  1.1969e-02,  7.9107e-02,  3.5027e-02,\n",
      "        -3.3416e-02,  1.0436e-02, -2.8875e-02, -2.4879e-02,  7.0590e-02,\n",
      "        -2.5959e-02, -3.6003e-02,  4.1276e-02,  5.4118e-02,  8.5981e-03,\n",
      "         7.9072e-02,  5.6018e-02,  5.2271e-02,  9.4220e-02,  1.3809e-02,\n",
      "        -7.3683e-03,  3.1863e-02,  8.4913e-02,  6.2307e-02, -7.4242e-02,\n",
      "         6.2664e-02, -9.8090e-02, -7.5792e-02, -5.9959e-02,  1.0556e-02,\n",
      "         7.5202e-02,  6.9579e-02,  7.4210e-02, -6.9771e-02, -6.8653e-02,\n",
      "        -4.2231e-02, -2.8938e-02,  5.6908e-02, -2.8067e-02, -1.3205e-02,\n",
      "        -1.8596e-02,  7.8373e-02, -5.0223e-03, -3.2093e-02,  3.9430e-02,\n",
      "         6.8783e-02, -2.6382e-02,  4.1080e-02, -9.5571e-02, -1.2956e-03,\n",
      "        -6.8536e-02, -7.9906e-02,  9.6705e-02, -8.9174e-02, -6.8115e-02,\n",
      "        -5.1104e-02,  6.4571e-05,  9.9544e-02, -2.6446e-02, -4.3999e-02,\n",
      "        -2.6701e-02,  4.2166e-02,  8.3308e-02,  7.4349e-03, -5.7757e-02,\n",
      "         3.4809e-02,  2.5725e-02,  3.3006e-02, -7.1169e-02, -1.8275e-02,\n",
      "         9.5418e-02, -3.0108e-02,  7.3831e-02,  1.0988e-01, -4.7185e-02,\n",
      "        -4.6216e-02,  1.0750e-01,  9.0821e-02,  6.5099e-02, -1.3276e-02,\n",
      "        -9.2162e-04,  8.8788e-02,  3.4908e-02, -3.5814e-02,  1.4282e-01,\n",
      "        -6.1020e-02,  2.3656e-02, -1.1815e-02,  1.3161e-01, -6.7156e-02,\n",
      "         9.2439e-02,  8.8329e-02,  1.3700e-01, -9.3966e-03,  5.7720e-02,\n",
      "         1.3818e-01, -1.7805e-02,  6.6445e-02,  7.2965e-02,  8.7643e-03,\n",
      "         1.0696e-02,  4.2929e-02,  3.7362e-02, -2.9668e-02, -5.8686e-02,\n",
      "         1.3848e-01,  5.8057e-02,  8.8945e-02,  2.2589e-02,  1.9029e-02,\n",
      "         8.7333e-02,  1.0827e-02, -1.0146e-02, -6.6487e-03,  1.0436e-01,\n",
      "         6.2259e-02,  5.1672e-02,  8.1195e-02,  3.1790e-02, -2.9419e-02,\n",
      "         1.2824e-01, -2.0527e-02,  1.9366e-02,  6.2304e-02,  1.1369e-01,\n",
      "        -5.6854e-03,  4.1459e-02, -4.9226e-03,  6.6321e-02, -4.8133e-02,\n",
      "        -5.1913e-02,  7.8177e-02,  5.2572e-02,  6.0757e-02,  8.5504e-03,\n",
      "        -6.6823e-02,  1.2769e-01,  1.4785e-02,  1.4693e-01,  7.4109e-02,\n",
      "         1.4524e-02, -3.8759e-02,  3.3561e-02,  2.1532e-02,  3.5209e-03,\n",
      "         1.3865e-01,  9.8617e-02, -5.7117e-02,  8.6180e-02, -8.3217e-03,\n",
      "         1.5923e-03,  6.9853e-02, -6.5005e-03, -8.6766e-03,  9.2039e-02,\n",
      "        -2.6059e-02,  6.6053e-02,  8.5291e-02,  5.4392e-02,  9.2006e-02,\n",
      "         1.6771e-02,  7.0567e-02, -7.6296e-03,  1.1307e-02,  7.4812e-02,\n",
      "         7.9816e-02, -5.7579e-02,  7.1489e-02, -3.4541e-04,  4.9826e-02,\n",
      "         4.8317e-02,  1.1791e-01,  5.1934e-02,  8.9342e-02,  7.6264e-02,\n",
      "        -4.4247e-03, -3.4679e-02,  6.1000e-02,  3.0412e-02,  4.6567e-02,\n",
      "        -1.2132e-02,  1.5801e-01,  1.4549e-02,  1.8635e-02,  1.2937e-02,\n",
      "         1.4563e-01,  2.4782e-02,  2.9257e-02,  1.4972e-01,  1.5752e-02,\n",
      "         6.4083e-02,  9.6381e-02,  5.6627e-02, -4.4588e-02, -6.5087e-02,\n",
      "         3.2257e-03, -3.9222e-02], requires_grad=True)), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([-0.0416, -0.0497, -0.0086,  0.0995,  0.0715,  0.0140,  0.0995,  0.1241,\n",
      "         0.0880, -0.0479,  0.0257, -0.0430,  0.1046, -0.0050, -0.0575,  0.0028,\n",
      "         0.0251,  0.1489, -0.0228,  0.0508, -0.0406,  0.1034,  0.0225,  0.0894,\n",
      "         0.1393,  0.0687,  0.1533, -0.0583,  0.0592,  0.0857,  0.0479, -0.0048,\n",
      "         0.0479, -0.0406,  0.1139, -0.0067, -0.0158,  0.0613, -0.0238,  0.1122,\n",
      "        -0.0152,  0.0972, -0.0643,  0.0728,  0.0078,  0.1529,  0.0383,  0.0836,\n",
      "         0.1096,  0.1192,  0.0384,  0.1185,  0.0139,  0.0302, -0.0129,  0.0596,\n",
      "         0.0783,  0.0738, -0.0069, -0.0321, -0.0542,  0.0638, -0.0318,  0.0790,\n",
      "         0.0155, -0.0523, -0.0138,  0.1285,  0.0516,  0.0187, -0.0031,  0.0890,\n",
      "        -0.0087,  0.0678,  0.0466, -0.0279,  0.0379,  0.0478,  0.0891,  0.0653,\n",
      "         0.0076,  0.0935,  0.1182,  0.1292,  0.0026,  0.1315,  0.0198,  0.0035,\n",
      "         0.0253, -0.0442, -0.0388,  0.0559,  0.0884,  0.0499,  0.1089, -0.0594,\n",
      "         0.0343,  0.0229,  0.0152,  0.0448,  0.0292, -0.0287,  0.1328,  0.0624,\n",
      "        -0.0101, -0.0323,  0.0125,  0.0087,  0.0207,  0.0549,  0.0048,  0.1215,\n",
      "         0.1427,  0.1036,  0.0888,  0.1245,  0.0696,  0.0364,  0.0643,  0.0240,\n",
      "         0.0630,  0.0006, -0.0037,  0.0321, -0.0261, -0.0305,  0.1137,  0.1035,\n",
      "        -0.0362,  0.0091,  0.0623,  0.0443, -0.0657,  0.0426, -0.0696,  0.0033,\n",
      "        -0.0117, -0.0305, -0.0004,  0.0440,  0.0275,  0.0724,  0.0017,  0.0144,\n",
      "         0.0291, -0.0560, -0.0007,  0.0585, -0.0160, -0.0746,  0.0346, -0.0084,\n",
      "        -0.0624, -0.0172, -0.0089,  0.0684,  0.0486,  0.0532,  0.0199, -0.0531,\n",
      "         0.0342,  0.0425,  0.0107, -0.0044, -0.0781,  0.0930, -0.0220,  0.0333,\n",
      "         0.0208,  0.0648, -0.0422, -0.0046, -0.0308,  0.0099,  0.0720, -0.0115,\n",
      "        -0.0495, -0.0286, -0.0313,  0.0488,  0.0060,  0.0971, -0.0536, -0.0464,\n",
      "         0.0293,  0.0866,  0.0993,  0.0329,  0.0718,  0.0572, -0.0782,  0.0878,\n",
      "         0.0633,  0.0089,  0.0324, -0.0175, -0.0226, -0.0158, -0.0387,  0.0759,\n",
      "        -0.0515,  0.0034, -0.0050, -0.0151, -0.0025,  0.0187,  0.0230,  0.0930,\n",
      "         0.0279, -0.0541, -0.0820,  0.0967, -0.0382, -0.0644,  0.0605,  0.0675,\n",
      "        -0.0415,  0.0948, -0.0395,  0.0367,  0.0444,  0.0958, -0.0275,  0.0662,\n",
      "         0.0638,  0.0838, -0.0457,  0.0303, -0.0318, -0.0383,  0.0384, -0.0078,\n",
      "         0.0907,  0.0632,  0.0625,  0.0054,  0.0262, -0.0209, -0.0439,  0.0824,\n",
      "         0.0514, -0.0372, -0.0476,  0.0657,  0.0075,  0.0188, -0.0673, -0.0427,\n",
      "         0.0979,  0.0695,  0.0123, -0.0065, -0.0314, -0.0208, -0.0222,  0.0414,\n",
      "         0.0248,  0.0689, -0.0219,  0.0079, -0.0288, -0.0938, -0.0059, -0.0314,\n",
      "        -0.0029, -0.0384, -0.0657,  0.0706, -0.0315, -0.0609,  0.0689,  0.0196,\n",
      "         0.0956, -0.0395, -0.0233,  0.0243, -0.0141, -0.0175,  0.0648, -0.0538,\n",
      "         0.0642,  0.0483,  0.0893,  0.0723,  0.0862,  0.0620,  0.0122, -0.0178,\n",
      "        -0.0411,  0.0222, -0.0046, -0.0280,  0.0208, -0.0510, -0.0016,  0.0239,\n",
      "         0.0122, -0.0732, -0.0730,  0.0235,  0.0250,  0.0014,  0.0081, -0.0092,\n",
      "         0.0564, -0.0421, -0.0293,  0.0431, -0.0482, -0.0853,  0.0490, -0.0434,\n",
      "        -0.0994,  0.0882, -0.0375,  0.0004, -0.0487,  0.0682, -0.0036, -0.0838,\n",
      "        -0.0007,  0.0905, -0.0730, -0.0506,  0.0741, -0.1014, -0.0460, -0.0373,\n",
      "        -0.0512, -0.0112,  0.0198,  0.0730,  0.0702,  0.0210, -0.0674,  0.0383,\n",
      "         0.0559,  0.0252,  0.0270, -0.0199,  0.0852, -0.0479, -0.0876, -0.0475,\n",
      "         0.0082, -0.0532, -0.0601,  0.0413,  0.0018, -0.0718,  0.0456,  0.0554,\n",
      "        -0.0870,  0.0499, -0.0481, -0.0483,  0.0590, -0.0631,  0.0568,  0.0766,\n",
      "        -0.0052, -0.0263, -0.0704, -0.0326,  0.0608,  0.0712, -0.0659, -0.0219,\n",
      "        -0.0055, -0.0380,  0.0397,  0.0277,  0.0496,  0.0299, -0.0045,  0.0154,\n",
      "         0.0324, -0.0274, -0.0042,  0.0861, -0.0298, -0.0700, -0.0288,  0.0704,\n",
      "         0.1226, -0.0076,  0.1128,  0.0299, -0.0349,  0.0749,  0.0229,  0.0830,\n",
      "         0.0143, -0.0314, -0.0221, -0.0490,  0.0024,  0.0298, -0.0368,  0.0253,\n",
      "         0.0053,  0.0353, -0.0031, -0.0337,  0.0376,  0.0771,  0.1463,  0.0690,\n",
      "         0.1104,  0.0601,  0.0562,  0.0511,  0.0692,  0.0272,  0.0672,  0.1326,\n",
      "         0.0681, -0.0202,  0.0711,  0.0383,  0.0375,  0.0541, -0.0445,  0.1405,\n",
      "         0.0763,  0.0871,  0.0395,  0.1316, -0.0534,  0.0264,  0.0758,  0.1421,\n",
      "         0.0984,  0.1242, -0.0667,  0.0089,  0.1060, -0.0345,  0.1134,  0.1569,\n",
      "         0.0366,  0.0209,  0.0379,  0.1027,  0.0570, -0.0050,  0.0616, -0.0703,\n",
      "         0.1229, -0.0205,  0.0366,  0.0660,  0.0556,  0.0132,  0.1004,  0.0439,\n",
      "        -0.0533,  0.0822,  0.0965, -0.0222,  0.0329,  0.0318,  0.0719,  0.0594,\n",
      "         0.0257, -0.0350,  0.0176,  0.0744,  0.0004,  0.1090,  0.0733,  0.0199,\n",
      "        -0.0309, -0.0445, -0.0510,  0.0149,  0.0034,  0.0528, -0.0254,  0.0652,\n",
      "         0.1039, -0.0242,  0.0677,  0.0159, -0.0027, -0.0079,  0.0860,  0.1334,\n",
      "        -0.0613, -0.0090,  0.0602, -0.0588,  0.0286,  0.1420,  0.0089,  0.1395,\n",
      "         0.0118, -0.0131, -0.0608, -0.0062,  0.1032, -0.0480,  0.0651,  0.0829,\n",
      "         0.1312,  0.1296,  0.0822,  0.1032, -0.0096, -0.0273,  0.1503,  0.0629],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l1', Parameter containing:\n",
      "tensor([[ 0.1098,  0.0081, -0.0332,  ..., -0.0418,  0.0948, -0.0645],\n",
      "        [ 0.0510,  0.0231, -0.1477,  ..., -0.0932,  0.0531, -0.0212],\n",
      "        [ 0.0351, -0.0376, -0.0693,  ..., -0.0383,  0.0973,  0.0755],\n",
      "        ...,\n",
      "        [ 0.0012, -0.0075,  0.0365,  ..., -0.0226, -0.1830,  0.0667],\n",
      "        [-0.0258,  0.0144,  0.0844,  ..., -0.0934,  0.0119, -0.0245],\n",
      "        [-0.2001, -0.0196, -0.0274,  ..., -0.0861,  0.1976, -0.0110]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l1', Parameter containing:\n",
      "tensor([[-0.0943,  0.0674, -0.0633,  ...,  0.0094, -0.0396,  0.0228],\n",
      "        [-0.1215,  0.0837, -0.0313,  ...,  0.0877,  0.0735,  0.0164],\n",
      "        [-0.1262,  0.0342, -0.0762,  ...,  0.1608, -0.0308,  0.0301],\n",
      "        ...,\n",
      "        [ 0.0483,  0.0402, -0.0943,  ...,  0.0720, -0.0700, -0.0566],\n",
      "        [ 0.0598,  0.0605, -0.0415,  ...,  0.0412, -0.0611, -0.0231],\n",
      "        [-0.0322, -0.0430,  0.0758,  ...,  0.0068, -0.1174, -0.0456]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l1', Parameter containing:\n",
      "tensor([ 6.8977e-02,  9.6483e-02, -2.6369e-02,  8.3516e-03,  3.0552e-02,\n",
      "        -1.3754e-02,  1.0404e-02,  7.2845e-02,  6.2181e-02,  4.2946e-03,\n",
      "        -3.9357e-02, -3.1582e-02,  1.1750e-01,  1.1978e-01,  4.3726e-03,\n",
      "        -4.7614e-02,  3.1351e-02,  1.0040e-01, -2.4810e-02,  2.2800e-02,\n",
      "        -7.0299e-03,  2.1104e-02, -5.7359e-02,  9.2980e-02, -5.5829e-02,\n",
      "        -1.2378e-02,  5.9238e-02, -3.3729e-02,  2.5977e-02, -6.9044e-03,\n",
      "        -2.2306e-02,  7.1394e-02,  1.0527e-01, -2.0814e-02,  2.9188e-03,\n",
      "        -9.6221e-03,  2.2857e-02, -4.1510e-02, -4.7586e-02,  2.7331e-02,\n",
      "         9.8721e-02,  4.0752e-03,  1.2148e-01,  4.1371e-02,  9.8488e-02,\n",
      "        -4.9961e-02, -2.7975e-02,  9.9660e-02,  1.3162e-02,  3.9855e-02,\n",
      "         1.2576e-01, -4.4664e-02, -5.5854e-02, -1.7274e-03,  9.7078e-02,\n",
      "         7.3346e-02,  6.0009e-03,  7.3399e-02,  5.2954e-02,  9.8699e-02,\n",
      "        -4.1403e-03,  5.5089e-02,  1.0546e-01, -3.8153e-02,  6.5970e-02,\n",
      "        -2.4821e-02,  1.2729e-01,  1.1293e-01,  6.3655e-02,  7.3305e-04,\n",
      "         1.0993e-02,  1.1846e-01, -1.2585e-02,  4.2583e-02,  7.9802e-02,\n",
      "         7.5625e-02, -9.1964e-05,  1.3281e-01,  1.6120e-02,  8.4420e-02,\n",
      "         2.2871e-02,  1.1219e-01,  1.3076e-01,  9.4642e-02, -1.9718e-02,\n",
      "         3.8587e-02, -4.9181e-03,  8.1710e-02,  1.0039e-01,  1.1345e-01,\n",
      "         5.2944e-03,  9.9134e-02, -1.4568e-02, -3.5666e-02,  4.0497e-02,\n",
      "         1.0086e-01,  4.7306e-02, -3.9717e-02,  1.1446e-01, -1.7689e-02,\n",
      "         3.1724e-02,  1.2208e-01, -3.7235e-02,  5.8376e-02,  3.7203e-02,\n",
      "         9.8604e-02, -2.6547e-02, -4.1051e-02,  8.5276e-02,  2.3424e-02,\n",
      "         3.8311e-02,  1.1195e-01,  1.1698e-01,  1.0495e-01,  6.9991e-02,\n",
      "         6.3503e-02,  9.0591e-02,  4.8039e-03, -5.5204e-02, -2.8767e-02,\n",
      "        -3.0648e-02,  6.1589e-02,  1.9720e-03, -4.6902e-02,  3.4833e-02,\n",
      "         4.8086e-02,  3.0645e-02,  1.1149e-01,  3.0558e-02, -4.2432e-02,\n",
      "         7.9935e-02,  5.9576e-02,  8.3679e-02,  3.5984e-02, -1.7932e-02,\n",
      "         6.9678e-03,  2.5123e-03,  5.3715e-02, -2.3608e-02, -8.6195e-02,\n",
      "         4.4694e-02,  2.8333e-05, -1.2055e-01,  6.4627e-02, -1.6928e-02,\n",
      "        -1.8431e-02,  3.2246e-02,  2.6303e-02,  2.0298e-02,  3.6960e-03,\n",
      "         9.7305e-03,  1.8747e-02, -6.6466e-02, -5.2001e-02, -3.3112e-02,\n",
      "         7.0036e-02,  2.0523e-02,  1.9501e-02,  5.0035e-02,  2.3852e-02,\n",
      "        -2.3762e-02,  4.3932e-02, -8.0353e-02,  4.9502e-02, -1.1372e-01,\n",
      "         1.8092e-02,  4.5792e-02,  1.0943e-01,  5.6920e-02, -1.1481e-01,\n",
      "        -9.3623e-02, -1.7396e-02, -5.9243e-02, -5.0517e-02, -3.2821e-02,\n",
      "         1.4639e-02, -3.6697e-02,  1.7317e-02, -9.4094e-02, -3.8521e-02,\n",
      "        -4.4881e-02, -3.4359e-02, -7.4677e-03, -6.2085e-02,  2.0287e-02,\n",
      "        -6.2598e-03,  2.4882e-02, -2.1134e-02, -5.8237e-02,  3.1491e-02,\n",
      "        -8.1348e-02,  9.7847e-03, -1.1096e-02, -2.8111e-02,  5.9341e-02,\n",
      "        -1.6897e-03, -7.1236e-02, -2.7592e-02,  2.9738e-02, -1.0423e-01,\n",
      "         3.9761e-02, -1.3635e-02, -5.7711e-02,  4.1160e-02,  1.7386e-02,\n",
      "         3.6192e-02, -9.4160e-02,  4.8645e-02,  3.9812e-03, -4.5873e-02,\n",
      "         3.4543e-02,  6.5366e-02, -4.0839e-02,  4.3241e-02,  1.9596e-02,\n",
      "        -1.3071e-02,  1.3274e-02,  8.8914e-03, -7.0277e-02, -1.7351e-02,\n",
      "         8.8960e-02,  4.1034e-02, -4.0413e-02,  8.7157e-02, -8.8297e-03,\n",
      "         2.1043e-02, -4.4306e-02, -3.7355e-02, -2.5816e-02, -1.9309e-02,\n",
      "        -9.3019e-02, -9.2575e-02, -4.7469e-03,  1.3736e-02, -3.0540e-02,\n",
      "         9.0665e-02, -2.9144e-02,  2.2568e-02, -6.1019e-03, -3.1074e-02,\n",
      "        -1.3550e-02, -1.5280e-02,  1.6481e-02,  3.2179e-02,  8.5287e-03,\n",
      "        -5.7031e-02,  7.7146e-03, -2.2410e-02, -3.1842e-02,  1.7770e-02,\n",
      "        -9.3564e-02,  4.8534e-03, -4.7460e-02,  9.5033e-02, -4.7074e-02,\n",
      "         6.4698e-02, -2.9565e-02, -4.4255e-02, -9.4376e-02, -2.0890e-02,\n",
      "         8.5812e-02,  1.0110e-01,  3.0577e-02, -5.1805e-02,  4.6966e-02,\n",
      "        -4.7003e-02, -3.9340e-02,  2.8156e-02, -3.3751e-02, -4.3373e-02,\n",
      "         8.0364e-04, -1.5409e-03, -1.2292e-02, -2.3560e-03,  1.9989e-02,\n",
      "         1.0291e-01, -4.0767e-02, -9.8960e-03,  8.1279e-02,  4.2370e-02,\n",
      "         6.2480e-02, -8.1235e-02,  8.0871e-02,  2.7982e-02,  7.2631e-02,\n",
      "         7.1443e-03, -2.3609e-02, -5.2391e-02, -1.0404e-01,  5.4133e-02,\n",
      "         2.9766e-02,  5.9152e-02, -6.0440e-03, -5.0203e-02, -7.3381e-03,\n",
      "        -3.5321e-02, -8.8838e-02, -5.4301e-02,  4.2161e-02, -3.4925e-02,\n",
      "         2.2025e-03,  2.1379e-03, -9.7336e-02,  1.4899e-02, -6.6508e-02,\n",
      "         5.7818e-02,  1.8817e-02,  3.2552e-02, -6.3897e-02, -6.1055e-02,\n",
      "         2.5959e-02, -2.8793e-02,  5.3047e-02,  1.8005e-03,  3.4882e-03,\n",
      "         3.0908e-02,  3.8972e-03, -4.2233e-02,  2.4276e-02,  9.5875e-02,\n",
      "         5.7065e-02, -8.3829e-02,  1.0455e-01, -8.9941e-02,  8.2443e-02,\n",
      "         5.6730e-02, -7.0032e-02,  7.2160e-02, -5.0871e-02,  3.6769e-02,\n",
      "        -7.1194e-03, -5.4424e-02, -6.7526e-02, -6.0750e-02,  5.6989e-03,\n",
      "         2.5469e-02,  8.0291e-02, -2.8681e-02, -5.8833e-02,  7.6788e-02,\n",
      "        -2.7172e-02,  7.6547e-02,  4.1355e-02,  7.5003e-02,  2.6126e-02,\n",
      "         5.9853e-02,  5.8891e-02, -2.0415e-02, -9.7039e-02, -5.1092e-02,\n",
      "         7.6356e-03,  5.8935e-02,  6.3765e-02, -9.9988e-02, -5.2792e-03,\n",
      "        -8.8261e-02,  3.7436e-02,  1.4231e-02,  9.8882e-02,  7.9490e-02,\n",
      "        -3.7793e-02,  7.5187e-02, -3.8866e-02, -8.6885e-02, -3.5062e-02,\n",
      "         5.4040e-02,  2.5852e-02, -7.3808e-02, -8.4150e-02,  4.5603e-02,\n",
      "         7.8895e-02,  3.0712e-02, -6.1313e-02,  3.4720e-02,  8.2758e-03,\n",
      "         2.7368e-02,  5.9506e-02,  7.4014e-02,  5.0467e-02,  1.7458e-02,\n",
      "         7.5528e-02, -7.8157e-03, -2.6441e-02, -3.9729e-02,  3.5147e-02,\n",
      "         1.1482e-01,  1.0862e-02, -2.0663e-02,  9.9814e-02, -1.1063e-03,\n",
      "         1.2303e-01, -2.5777e-02, -3.6073e-02,  9.2325e-02,  9.2016e-02,\n",
      "         7.9707e-02,  9.4929e-02,  1.2665e-01, -5.0013e-02,  7.4907e-02,\n",
      "         1.2420e-01, -4.4075e-02,  8.4940e-02, -7.3198e-02, -2.5710e-02,\n",
      "         7.9495e-02, -4.9174e-02,  1.5955e-02, -2.6652e-02, -3.0667e-02,\n",
      "         2.2409e-02,  1.2871e-01,  9.7564e-02, -3.2339e-03, -5.9431e-02,\n",
      "         1.0282e-01,  4.5041e-02, -4.4277e-02, -2.2501e-02,  5.7320e-02,\n",
      "        -2.8367e-02, -4.4251e-02,  7.2248e-02,  1.0286e-01, -1.8645e-05,\n",
      "         7.9314e-02,  1.5537e-02,  3.9130e-02, -1.7192e-02,  3.2741e-02,\n",
      "         1.6416e-02, -2.1136e-02,  1.0243e-01,  3.9894e-02,  5.2046e-02,\n",
      "        -6.0400e-02, -1.0676e-02,  8.5853e-02,  3.8212e-02,  6.0553e-02,\n",
      "         6.3016e-02, -4.7409e-02,  1.8614e-02,  6.4640e-02, -1.1946e-02,\n",
      "        -1.7211e-02, -3.4728e-02,  1.1268e-01,  8.6486e-02, -1.8545e-02,\n",
      "         1.1485e-01,  3.0318e-02,  1.1130e-01,  4.2871e-02,  7.9305e-02,\n",
      "         7.7267e-02,  1.2683e-01,  1.5940e-02, -1.0066e-04,  3.0618e-03,\n",
      "         2.3406e-02,  9.2971e-02,  2.8612e-02, -4.1018e-02,  5.4838e-02,\n",
      "         1.3161e-01,  1.1522e-01,  4.2202e-02, -2.2176e-02,  9.2964e-02,\n",
      "         9.5618e-02,  3.4118e-02, -1.9986e-02,  8.9487e-02, -5.2530e-02,\n",
      "         7.7204e-02, -4.1824e-02, -1.6518e-02,  5.2707e-02,  1.5439e-02,\n",
      "         1.1426e-02, -3.5289e-02,  1.2451e-01,  5.4779e-03,  1.4391e-02,\n",
      "         1.3110e-01,  1.2498e-01,  1.0726e-01,  4.5267e-02, -1.3020e-02,\n",
      "        -3.7602e-02, -5.3836e-02, -3.6928e-02,  5.8675e-03,  7.0571e-02,\n",
      "         5.4294e-02, -3.5951e-02, -2.1759e-02,  1.3118e-02,  8.0133e-02,\n",
      "        -6.1964e-03,  1.1849e-01, -7.3239e-03, -4.5310e-02, -9.4703e-02,\n",
      "        -1.0160e-02,  5.4168e-02, -3.6822e-02,  4.5591e-02,  8.9850e-02,\n",
      "         2.4207e-02,  1.2005e-01], requires_grad=True)), ('lstm.bias_hh_l1', Parameter containing:\n",
      "tensor([-5.6208e-04,  3.1453e-02, -1.1511e-02,  5.0550e-03, -3.3486e-02,\n",
      "         3.9795e-02,  9.1507e-02, -3.9434e-03,  5.1374e-02, -4.7020e-02,\n",
      "         4.3882e-02, -1.9523e-02,  1.0145e-02,  2.3198e-02,  5.5678e-02,\n",
      "         2.2146e-02,  4.6557e-02,  8.4748e-02,  1.0574e-01,  6.5980e-02,\n",
      "        -2.9725e-03,  7.1708e-02, -1.0077e-02,  1.0219e-02,  7.1510e-02,\n",
      "        -2.7042e-03,  6.6384e-02,  5.2048e-02, -4.7513e-02,  7.4993e-02,\n",
      "        -1.4674e-02, -2.0835e-02,  6.0224e-02,  4.0018e-02, -9.8490e-05,\n",
      "        -3.4379e-02,  1.0545e-01, -3.0977e-02,  7.7223e-03,  7.8304e-02,\n",
      "         4.3522e-02,  1.3484e-01,  1.1872e-01,  1.2671e-01, -6.3871e-02,\n",
      "         4.3233e-02,  9.1856e-02, -3.8376e-02, -1.9707e-03,  1.0112e-01,\n",
      "        -1.6596e-02, -4.4433e-02, -4.7425e-03,  1.1787e-02,  1.0494e-01,\n",
      "         5.5283e-03, -3.2440e-02, -2.8691e-02, -7.2447e-03,  5.4368e-02,\n",
      "        -3.1542e-02, -5.1148e-02,  8.8324e-02,  3.4372e-02,  4.0839e-02,\n",
      "         8.4073e-04,  3.8977e-02,  4.4738e-02,  3.2306e-02, -8.0455e-03,\n",
      "        -3.0319e-02,  9.1671e-03, -3.7857e-02,  2.6423e-02,  6.9786e-03,\n",
      "         8.8028e-03, -4.7275e-02,  2.4148e-02,  1.3041e-01,  2.2438e-02,\n",
      "         2.9897e-03, -3.4261e-02,  1.0655e-01,  8.8099e-02,  2.4434e-02,\n",
      "        -1.5277e-02,  4.9950e-03, -1.0885e-02,  8.5600e-02, -5.4449e-03,\n",
      "        -4.1955e-02,  4.1222e-02, -3.7657e-02, -2.2194e-02,  3.6275e-02,\n",
      "        -5.6303e-02, -1.1791e-02, -1.6991e-02,  1.3036e-01,  5.6972e-02,\n",
      "         1.1790e-01,  5.2901e-02, -3.0744e-03, -3.9014e-02,  7.1163e-02,\n",
      "        -2.4303e-02,  6.3084e-02, -5.4026e-02,  8.7359e-02,  3.8300e-02,\n",
      "        -1.8004e-02,  9.1510e-02,  1.7810e-02, -3.9102e-02, -5.3249e-02,\n",
      "         1.2597e-02,  1.0444e-01,  6.7677e-02, -9.8005e-04,  1.3702e-02,\n",
      "        -4.6935e-02,  4.2726e-02,  1.3729e-01,  2.3010e-02,  6.0118e-02,\n",
      "         1.1418e-02,  6.8129e-02,  7.4021e-02, -3.7325e-02, -2.1446e-02,\n",
      "        -7.9500e-02,  9.8656e-02,  1.0257e-01,  8.0612e-02,  1.3054e-02,\n",
      "        -2.2076e-02, -8.6599e-02,  4.4362e-02, -9.9580e-04,  2.3391e-02,\n",
      "        -1.8262e-02,  7.8304e-03, -6.4259e-02,  4.8765e-02,  1.1226e-02,\n",
      "         2.4492e-02,  6.0322e-02,  8.4741e-02,  1.1378e-02, -3.8023e-02,\n",
      "         6.5442e-02, -1.0901e-01,  8.2323e-02,  5.6854e-02,  8.7387e-02,\n",
      "        -5.5020e-02, -3.4666e-02, -7.4903e-02, -3.4417e-02,  5.1687e-02,\n",
      "         9.4707e-03, -4.6665e-02,  3.1710e-02,  2.0820e-02, -2.4404e-02,\n",
      "         3.6341e-02,  8.3539e-02,  4.0875e-02,  4.7274e-02, -1.0039e-01,\n",
      "         4.5755e-02, -1.1874e-02,  6.6593e-02,  5.2408e-02,  4.2047e-02,\n",
      "        -1.8713e-02, -5.7142e-02,  4.9228e-02, -6.5285e-02, -8.0017e-03,\n",
      "        -2.1022e-02, -9.3172e-02,  1.9813e-02,  2.0099e-02, -2.8573e-03,\n",
      "         7.8986e-02, -7.6297e-02, -5.6433e-03, -3.9526e-02, -2.5809e-02,\n",
      "         2.5705e-02,  7.0700e-02, -9.2771e-02, -5.3979e-02, -1.2490e-02,\n",
      "        -4.8877e-05, -6.0087e-02, -7.4509e-03,  1.0088e-01, -6.2914e-02,\n",
      "        -6.4950e-02,  8.8220e-02,  3.5083e-03, -1.0648e-01,  6.2785e-02,\n",
      "        -8.5816e-02,  4.0463e-02, -1.6709e-02, -2.7477e-02, -1.1087e-01,\n",
      "        -6.4264e-02,  4.5087e-02, -4.6812e-02,  1.0759e-01,  1.4310e-02,\n",
      "         8.1031e-02,  6.0813e-02, -7.1597e-02,  3.3479e-02,  2.6277e-02,\n",
      "        -1.6008e-02,  6.3257e-02, -6.3306e-02, -3.3455e-02,  5.4811e-02,\n",
      "        -2.4728e-02,  7.7775e-03,  5.5859e-02, -3.5957e-02, -9.4486e-02,\n",
      "        -4.3674e-02, -2.0325e-02,  9.2453e-02,  4.2460e-02,  3.2252e-02,\n",
      "         6.1661e-02,  9.8140e-02, -8.6480e-02,  9.6528e-02, -7.5989e-02,\n",
      "        -3.6618e-02,  1.0292e-01, -4.2477e-02, -2.8886e-02,  7.2928e-02,\n",
      "         4.9742e-02,  7.0202e-02,  1.2076e-03, -4.9109e-03, -2.6020e-02,\n",
      "        -1.0123e-01, -1.0485e-02, -8.9593e-02,  1.3859e-02, -3.0790e-02,\n",
      "        -1.8310e-02, -1.0851e-02,  6.7598e-02, -1.6474e-02, -7.9528e-02,\n",
      "        -5.1435e-02,  9.5589e-02,  8.7574e-02, -1.0383e-02, -5.8734e-02,\n",
      "         3.0673e-02, -1.9458e-02, -1.4101e-02,  6.3045e-02,  7.8898e-02,\n",
      "         7.3326e-02,  7.9929e-02,  6.9287e-02,  6.4179e-02, -5.7188e-02,\n",
      "         5.1465e-02, -7.1500e-02, -9.1131e-02,  8.8728e-02,  7.1997e-02,\n",
      "         4.5152e-02, -1.2265e-02, -9.0412e-04,  6.4529e-02,  2.8169e-02,\n",
      "        -3.3351e-02, -7.0804e-02,  3.0973e-02,  4.3392e-02, -8.2335e-02,\n",
      "         9.9571e-03,  4.0302e-02,  6.4873e-02,  8.8295e-02, -5.7570e-02,\n",
      "         3.2230e-02, -3.5854e-02, -2.0872e-02, -8.4267e-02, -7.4823e-02,\n",
      "         6.0410e-02, -8.6512e-03,  3.2180e-02, -1.0320e-02,  4.2348e-02,\n",
      "        -5.2965e-02,  4.8084e-02,  2.3026e-02, -4.7139e-02, -5.0558e-02,\n",
      "        -5.5208e-02,  3.6676e-02,  7.3478e-02, -1.2722e-02,  8.8974e-02,\n",
      "         7.3755e-02,  6.7895e-02,  7.2617e-02,  6.5537e-03, -5.4459e-02,\n",
      "        -8.3569e-02, -2.2524e-03,  4.4722e-02,  1.9375e-02,  7.2876e-02,\n",
      "        -2.0261e-02, -1.7081e-02, -4.6821e-02,  7.9393e-02,  4.1665e-02,\n",
      "        -2.5955e-03,  4.4449e-02, -3.1845e-02,  3.8662e-02,  1.2150e-02,\n",
      "        -2.2169e-02, -3.1347e-02,  4.9520e-02,  6.1743e-02,  3.9689e-02,\n",
      "        -6.5207e-02,  6.1197e-02, -3.3796e-02, -2.0538e-02, -1.7743e-03,\n",
      "        -5.7946e-02,  1.1039e-02, -6.2493e-02,  2.0373e-02, -7.2518e-02,\n",
      "         6.7710e-04,  2.0107e-03, -5.1351e-02, -1.1809e-02, -2.4225e-02,\n",
      "        -1.8202e-02, -1.1447e-02, -3.7391e-02,  1.8693e-02,  7.9503e-02,\n",
      "        -4.1333e-02,  4.9498e-02, -2.7017e-03, -8.8698e-02,  8.9064e-02,\n",
      "         1.0117e-02, -5.5651e-02, -6.5306e-02, -4.6001e-02,  1.0084e-01,\n",
      "         9.8311e-02,  7.0380e-02, -5.5524e-02,  6.3358e-02, -5.4608e-02,\n",
      "         8.6193e-02,  7.1739e-02,  2.7445e-03, -2.2851e-02,  8.8321e-02,\n",
      "        -5.2201e-02, -5.3073e-02, -3.0906e-02, -5.5526e-02,  2.0765e-02,\n",
      "        -2.6642e-03,  1.2623e-01,  8.4825e-02,  7.3681e-02,  1.1983e-01,\n",
      "        -3.4302e-02,  1.2352e-02,  1.2366e-01, -2.1398e-02, -1.7464e-02,\n",
      "         3.3056e-02, -1.6457e-02,  5.6173e-02,  8.6358e-03,  1.1556e-01,\n",
      "         6.1970e-02, -2.9206e-02,  6.7503e-02, -6.5363e-02,  1.9931e-02,\n",
      "         4.8835e-02,  5.8033e-02,  5.9504e-02,  9.2436e-02,  6.0032e-02,\n",
      "        -3.3091e-02, -9.6797e-03,  8.7604e-02, -7.3703e-02,  2.8978e-02,\n",
      "         1.9773e-02,  7.7435e-02, -3.3233e-02,  6.2138e-02,  4.9689e-03,\n",
      "        -1.8438e-02,  6.6750e-02,  6.3816e-02,  4.6771e-02,  2.0684e-03,\n",
      "         1.2790e-01,  1.9426e-02,  1.6033e-02,  6.7626e-02,  4.0721e-02,\n",
      "         6.7431e-02,  4.6476e-02,  6.8650e-02,  6.3549e-02, -2.0299e-02,\n",
      "        -1.0800e-02,  7.4440e-02, -1.7144e-02,  4.9245e-02,  2.2723e-02,\n",
      "         2.4898e-02, -6.0420e-02,  9.2489e-02,  4.7685e-02,  1.6219e-02,\n",
      "         8.4872e-02, -4.7170e-02,  4.5793e-02,  6.2304e-02, -2.2674e-02,\n",
      "         9.8322e-02,  9.5017e-02,  1.4040e-01,  4.5290e-02,  6.4189e-03,\n",
      "         9.9412e-02,  6.0112e-02, -1.1347e-02, -8.8453e-03,  5.6442e-02,\n",
      "         5.8597e-02,  1.3259e-01,  3.8301e-02,  8.8955e-02,  4.8658e-02,\n",
      "        -2.9807e-02, -9.4642e-03,  8.5251e-02, -2.1728e-02,  1.2315e-01,\n",
      "         2.5284e-02,  8.3252e-02, -5.3643e-02, -2.4492e-02,  5.2908e-02,\n",
      "        -3.4328e-02,  8.0989e-02, -2.1710e-02,  9.8556e-02,  2.2087e-02,\n",
      "         1.7716e-03, -2.8999e-02,  7.5177e-02, -9.5086e-03,  7.3823e-02,\n",
      "         7.6718e-02, -3.9303e-02, -1.6892e-02,  1.3435e-01, -4.2354e-02,\n",
      "         6.6534e-03,  1.0543e-02, -3.7086e-02,  4.9735e-02, -8.9715e-03,\n",
      "         8.6949e-02,  5.3216e-02, -7.1999e-03,  2.2951e-03,  5.2301e-02,\n",
      "         4.7001e-02,  5.0848e-02,  3.9957e-02,  2.2532e-02, -4.8859e-02,\n",
      "         1.3664e-01,  9.0548e-02,  2.1847e-02,  4.8947e-02,  1.3666e-01,\n",
      "         1.6915e-02, -8.2235e-03], requires_grad=True)), ('lstm.weight_ih_l1_reverse', Parameter containing:\n",
      "tensor([[-0.0774, -0.0310,  0.0348,  ..., -0.0970,  0.0875, -0.0257],\n",
      "        [-0.0492, -0.0217,  0.0484,  ..., -0.1120, -0.0155,  0.0285],\n",
      "        [ 0.0546, -0.0665, -0.1283,  ..., -0.0047,  0.0238, -0.0456],\n",
      "        ...,\n",
      "        [ 0.0999,  0.0307,  0.0303,  ..., -0.0059, -0.0871, -0.0028],\n",
      "        [ 0.0267,  0.0247, -0.1880,  ..., -0.0136,  0.0348, -0.1488],\n",
      "        [-0.0175, -0.0460,  0.0660,  ..., -0.0462, -0.0144,  0.0067]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l1_reverse', Parameter containing:\n",
      "tensor([[ 0.0067, -0.0283, -0.0516,  ..., -0.0163, -0.0576,  0.0712],\n",
      "        [ 0.0682,  0.0494,  0.0354,  ..., -0.0099, -0.0896, -0.0319],\n",
      "        [-0.0632,  0.0396, -0.0416,  ..., -0.0524, -0.1290, -0.0401],\n",
      "        ...,\n",
      "        [ 0.0286, -0.0973, -0.0542,  ..., -0.0794, -0.0629,  0.0953],\n",
      "        [-0.0152,  0.0046, -0.0051,  ..., -0.0798,  0.0395,  0.0701],\n",
      "        [ 0.0463,  0.0315, -0.1075,  ...,  0.0359, -0.0304,  0.0819]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l1_reverse', Parameter containing:\n",
      "tensor([-1.6499e-02,  7.5695e-02, -1.3435e-02,  6.1193e-02,  5.7099e-02,\n",
      "         6.4567e-02,  5.2386e-02,  4.5842e-03, -7.3894e-03, -5.9613e-02,\n",
      "         7.7278e-02, -2.6641e-02,  6.3791e-02,  8.8170e-02,  1.1122e-01,\n",
      "         1.0029e-01, -2.8133e-02, -3.2789e-02,  9.2469e-02,  1.9457e-02,\n",
      "         8.0617e-02, -7.0006e-02,  9.6863e-02,  7.6529e-02, -5.4607e-02,\n",
      "         1.5145e-02,  5.2574e-02, -5.4153e-02, -3.2949e-02,  9.5840e-02,\n",
      "         5.4191e-02,  1.0450e-01,  1.2153e-01,  2.1425e-02,  3.9396e-02,\n",
      "        -1.4738e-02, -3.1797e-02, -6.2820e-02,  2.2694e-02,  2.0360e-02,\n",
      "         2.7529e-02, -3.3645e-02,  7.7490e-02, -1.4939e-02,  4.8920e-02,\n",
      "        -3.4860e-02, -4.8264e-02,  7.0728e-02, -1.3361e-02,  1.1015e-01,\n",
      "         5.4898e-03,  1.3460e-02, -1.7525e-02,  6.9774e-02, -1.7690e-02,\n",
      "         9.8402e-02,  4.4515e-03,  7.8076e-02,  5.4586e-02,  7.7910e-04,\n",
      "        -2.4595e-02, -1.6244e-02, -2.3810e-02,  1.2135e-01,  2.4298e-03,\n",
      "        -4.8086e-02,  5.8301e-02,  1.1419e-01,  9.9710e-03, -2.5030e-02,\n",
      "         9.1645e-02, -2.4711e-02,  7.4277e-02, -5.9030e-03,  2.8410e-02,\n",
      "        -9.2248e-03, -1.9322e-02,  7.3480e-02, -2.3403e-02,  8.2281e-02,\n",
      "         8.4938e-02, -1.5459e-02, -2.7079e-02,  6.6051e-02, -4.4909e-02,\n",
      "         1.0596e-01,  7.2460e-02,  1.2095e-01,  9.9397e-02,  3.2888e-02,\n",
      "         4.2953e-02,  5.7405e-02,  1.2382e-02,  1.2918e-02,  3.4640e-02,\n",
      "         6.0887e-02, -8.1574e-03,  1.2111e-01, -4.3957e-02, -5.8982e-03,\n",
      "        -1.3781e-02, -3.6399e-02,  3.9495e-02, -3.3817e-02,  5.0102e-03,\n",
      "         1.0898e-01,  2.7911e-02,  6.5073e-02,  6.3345e-02, -5.1316e-02,\n",
      "        -3.1546e-02,  1.1255e-01,  6.4860e-03,  3.9696e-02,  3.2416e-02,\n",
      "        -1.8244e-03,  1.0167e-01,  1.0990e-01, -2.0175e-02, -4.7595e-02,\n",
      "         1.5478e-02,  6.8329e-02, -4.4025e-02, -3.2870e-02,  2.4843e-02,\n",
      "         1.0607e-01,  5.6855e-02,  2.2696e-02, -3.6007e-02,  6.5527e-02,\n",
      "         4.8896e-02,  1.2669e-02, -5.0637e-02,  7.8771e-02,  3.9029e-02,\n",
      "        -6.7872e-02, -1.3287e-02,  6.7034e-02,  3.7248e-03,  1.0697e-01,\n",
      "         4.2361e-02,  7.6766e-02, -1.3218e-02,  9.0294e-02,  6.9826e-02,\n",
      "         8.3223e-02,  8.3245e-02,  2.5221e-02, -3.3404e-02, -6.2616e-02,\n",
      "         5.5120e-02,  5.7339e-02,  9.0804e-02, -1.5806e-03,  3.2999e-02,\n",
      "        -1.5496e-02,  5.0228e-02,  3.3035e-02, -3.4459e-02, -7.5083e-02,\n",
      "         2.8295e-02,  4.4862e-02,  8.9437e-02, -7.6394e-02,  6.8254e-02,\n",
      "         5.6989e-02,  2.2313e-02,  9.9964e-02,  2.5360e-02, -7.2518e-02,\n",
      "        -7.5503e-02,  6.7372e-02,  6.0516e-02, -4.5259e-02,  6.6401e-02,\n",
      "        -3.9962e-02,  3.2600e-02,  4.6614e-02,  1.3397e-02,  1.0433e-01,\n",
      "         4.0621e-02,  1.8084e-02, -6.9629e-02, -1.0081e-02, -5.7232e-02,\n",
      "        -4.1416e-02,  5.6200e-02,  8.4101e-02, -6.0578e-02,  5.0365e-02,\n",
      "        -2.3489e-03,  5.7097e-02, -1.5025e-02, -3.6853e-02,  3.2031e-02,\n",
      "         8.8798e-02,  9.2323e-02, -4.6851e-02, -9.1248e-03,  8.6478e-03,\n",
      "        -1.7084e-02,  9.0759e-02,  3.4622e-02,  9.9048e-03, -7.2356e-02,\n",
      "         7.1612e-03, -1.7088e-02, -7.9506e-02, -3.7046e-03, -2.2148e-02,\n",
      "         9.3840e-03, -3.8868e-02, -5.3914e-02,  4.3160e-02, -5.8755e-02,\n",
      "        -2.1637e-02,  5.3461e-02, -6.8743e-02,  8.3516e-02,  8.0747e-02,\n",
      "         5.8328e-02,  4.1689e-02, -1.5321e-02,  3.5411e-02, -1.1372e-02,\n",
      "        -6.7991e-02, -2.8656e-02, -2.0417e-02,  5.8771e-02,  5.8120e-02,\n",
      "         2.4930e-02, -3.4889e-02, -1.7518e-02, -3.6275e-02, -8.8869e-02,\n",
      "        -6.5446e-02,  8.6023e-02, -8.2133e-03, -5.0717e-02, -7.2475e-02,\n",
      "         1.2849e-01, -1.0302e-02,  3.8877e-04, -6.1549e-03,  5.4870e-02,\n",
      "         8.0120e-02, -6.8958e-02, -8.1730e-03, -3.2263e-02, -7.7683e-02,\n",
      "        -8.3306e-03, -2.9144e-02,  6.7748e-02, -3.0191e-02,  4.0436e-02,\n",
      "         3.4562e-02, -5.2953e-02, -2.5658e-02, -7.2804e-02, -1.5232e-02,\n",
      "         1.7263e-02,  4.0390e-04, -2.8596e-02, -1.0917e-02, -7.2170e-02,\n",
      "        -5.2879e-02,  7.3675e-04, -8.3697e-03, -8.6449e-02, -2.9154e-02,\n",
      "        -3.8475e-02,  8.7868e-02,  7.5310e-02,  6.8517e-02, -8.5598e-02,\n",
      "         1.6051e-02,  9.1151e-02,  2.9764e-02,  5.9636e-02,  6.0453e-02,\n",
      "        -7.5493e-02, -6.3572e-02,  2.3421e-02,  7.5467e-02, -1.5276e-02,\n",
      "         8.4074e-02,  3.6878e-02, -4.5645e-02,  3.5000e-02,  5.1347e-02,\n",
      "        -7.4252e-02, -7.5452e-02, -1.9307e-02, -2.8873e-03,  3.0499e-02,\n",
      "         6.2062e-02, -9.9922e-02,  2.0526e-02, -3.7839e-02, -6.4873e-02,\n",
      "        -3.5090e-02,  7.0202e-02,  2.0623e-02, -3.8685e-02, -7.4416e-02,\n",
      "        -5.7511e-02, -2.9684e-02, -8.6705e-02,  3.5137e-02, -2.1679e-02,\n",
      "         4.3306e-02, -6.3470e-02, -6.7890e-02,  3.1689e-02, -9.3288e-02,\n",
      "        -9.9013e-02,  3.1881e-02, -4.9599e-02, -5.8514e-03,  8.4840e-02,\n",
      "        -4.2966e-02,  5.8964e-02,  6.7681e-02, -7.0681e-02, -5.5027e-02,\n",
      "        -6.8848e-02,  6.8571e-02, -9.4010e-02,  1.2380e-02, -3.6590e-02,\n",
      "         8.2635e-02, -6.0080e-02,  1.5988e-02,  2.5309e-02, -2.2773e-03,\n",
      "        -2.4252e-02, -2.9358e-02,  5.7174e-02, -3.7319e-02,  9.3910e-02,\n",
      "        -5.9437e-02,  7.3960e-02, -1.2846e-02,  5.5018e-03, -8.9606e-02,\n",
      "         4.0243e-02, -3.7407e-02, -1.1158e-02,  5.4720e-02, -1.7723e-02,\n",
      "         1.0068e-01,  8.6300e-03,  9.3093e-03, -6.4277e-02,  5.6810e-02,\n",
      "         7.7619e-02, -4.0196e-03, -1.5186e-02, -3.2287e-02,  2.5313e-02,\n",
      "        -3.2938e-02, -9.3995e-02, -9.4482e-03,  9.8663e-02, -9.0549e-02,\n",
      "         1.3280e-02, -5.0080e-02,  4.9762e-02, -8.4630e-02,  4.7654e-02,\n",
      "        -5.3934e-03,  7.8738e-02,  5.3276e-03, -7.1779e-02,  5.0332e-02,\n",
      "        -8.8859e-02,  5.8692e-02, -4.3235e-02, -8.2942e-02,  1.8596e-02,\n",
      "         8.9436e-02, -9.2718e-02, -3.8012e-02,  2.8138e-02,  9.8534e-02,\n",
      "        -5.5806e-02, -1.6102e-02,  1.2213e-01,  9.7450e-02, -4.7776e-02,\n",
      "         1.1900e-01,  1.3708e-01,  4.6140e-02,  7.7697e-02, -1.8770e-02,\n",
      "         9.7615e-02,  9.0286e-02,  7.8190e-02,  9.9549e-02,  3.4920e-02,\n",
      "         4.7269e-02,  6.4226e-02, -8.7373e-03, -3.7038e-02,  1.0314e-01,\n",
      "        -3.4848e-02,  2.1414e-02,  5.2369e-02,  5.7367e-02,  8.2775e-02,\n",
      "         5.0143e-02,  8.9021e-02, -3.3243e-02,  9.0932e-02, -3.3141e-02,\n",
      "         6.5280e-02,  1.4663e-01,  1.1441e-02,  5.1354e-02,  8.4318e-02,\n",
      "         6.7965e-02, -4.0670e-02,  4.6364e-03,  9.5755e-02,  1.0179e-01,\n",
      "        -3.8914e-02,  1.2670e-01,  3.0723e-02,  4.7850e-02,  5.4058e-03,\n",
      "         8.1640e-02, -2.2624e-03, -4.4129e-03,  5.7349e-02,  2.9782e-02,\n",
      "        -1.3208e-02,  2.0361e-02,  3.2763e-03,  1.4460e-01,  4.5604e-02,\n",
      "         4.3025e-02,  2.7492e-03, -2.3762e-02,  1.0176e-01,  2.4003e-02,\n",
      "        -7.0170e-02,  1.0533e-01,  3.3559e-02,  9.7627e-02, -1.9447e-02,\n",
      "         7.5986e-02,  5.3049e-02,  2.0462e-02,  8.9274e-02,  3.8745e-02,\n",
      "        -1.3363e-02, -5.2309e-02, -4.0128e-02,  7.2656e-02, -4.7689e-02,\n",
      "        -1.8389e-02, -1.8951e-02, -3.9148e-03, -7.6485e-02, -3.0932e-03,\n",
      "         9.6268e-03, -8.6744e-03,  9.0596e-02, -4.8832e-02,  4.2540e-02,\n",
      "         1.0629e-01,  9.8310e-02,  6.6546e-02,  6.9789e-02, -8.0223e-03,\n",
      "         2.3646e-02,  1.5319e-01,  1.1109e-01, -6.0627e-02, -1.8285e-02,\n",
      "         1.2708e-01, -4.9930e-05, -1.8908e-03, -8.6624e-05,  1.7629e-02,\n",
      "         6.1098e-03,  2.1537e-02,  1.0819e-02,  1.0092e-01, -8.4285e-03,\n",
      "         6.6102e-02,  8.3631e-02,  1.1136e-01, -2.1196e-02,  3.9707e-02,\n",
      "         5.8706e-02,  9.5099e-02,  7.8052e-02,  8.9341e-03, -1.8353e-02,\n",
      "        -4.2605e-02,  4.9224e-02,  1.2346e-01, -3.7643e-02, -2.8236e-02,\n",
      "        -7.4857e-02,  8.8870e-02, -6.4588e-02, -6.2235e-02, -1.3771e-02,\n",
      "         1.2386e-01,  9.6788e-02], requires_grad=True)), ('lstm.bias_hh_l1_reverse', Parameter containing:\n",
      "tensor([ 9.4767e-03, -3.9376e-02, -1.7093e-02, -5.1204e-02, -5.8110e-02,\n",
      "         6.8330e-02,  1.9738e-02, -3.8952e-03,  1.3379e-01,  7.9150e-02,\n",
      "         1.0496e-01,  1.3880e-02,  6.0167e-02,  2.0860e-03, -3.7021e-02,\n",
      "         8.7028e-02,  5.0522e-02,  4.8091e-02, -7.3048e-02, -3.3277e-02,\n",
      "         1.3041e-01,  1.0423e-01, -1.6399e-02, -4.6123e-03,  3.3913e-02,\n",
      "         7.3586e-02,  4.8276e-02,  8.9431e-02,  1.7543e-02, -8.6970e-04,\n",
      "         7.2347e-02, -7.8533e-03,  2.4677e-02, -5.1032e-02, -2.4400e-02,\n",
      "         1.5403e-02, -4.0567e-02,  3.8069e-02,  7.7084e-02,  7.9354e-02,\n",
      "        -2.7269e-02,  9.0171e-02,  9.6582e-02,  6.3090e-02,  9.4314e-02,\n",
      "         4.2892e-02,  7.8632e-02,  7.7522e-02, -2.9717e-03,  1.1328e-01,\n",
      "         6.9772e-02,  3.1452e-02,  9.0903e-02, -3.8223e-02,  1.0809e-01,\n",
      "         2.5781e-02,  7.6794e-02,  4.7378e-02,  4.4129e-02,  4.1605e-02,\n",
      "        -1.1765e-02,  7.5722e-02, -3.5063e-02,  5.6177e-02,  3.6622e-02,\n",
      "         3.1947e-02,  1.3808e-02,  1.5153e-01,  5.2876e-02,  1.1981e-01,\n",
      "         4.0424e-02,  6.7876e-02, -2.9551e-02, -2.6248e-02, -1.1577e-02,\n",
      "        -7.0663e-02,  5.8639e-02, -3.7804e-02, -6.0857e-02, -2.0226e-03,\n",
      "         9.2594e-02, -1.0048e-02,  8.8602e-03,  7.3196e-02,  1.2035e-01,\n",
      "        -2.1457e-02,  2.2796e-02,  6.4806e-02,  7.1339e-02,  5.8151e-02,\n",
      "        -4.1940e-03,  4.1161e-02,  6.6712e-02, -7.1472e-03,  4.5861e-03,\n",
      "        -5.7567e-02,  3.0329e-02,  6.9718e-02, -4.1938e-02,  6.8764e-02,\n",
      "         7.4314e-02,  5.8436e-02, -3.4002e-02,  9.0218e-02,  1.4717e-02,\n",
      "        -1.3680e-02, -3.0981e-02,  3.0943e-02, -2.8811e-02,  7.6460e-02,\n",
      "         5.4654e-02,  2.6593e-02,  9.9223e-02,  1.3498e-01, -3.6059e-02,\n",
      "         3.1780e-02,  1.0883e-01, -2.1805e-02,  3.9690e-02, -3.0530e-02,\n",
      "        -2.5112e-02, -1.6106e-02,  1.0431e-01,  4.0794e-02,  8.1939e-02,\n",
      "         3.1312e-02,  4.5626e-02,  1.5577e-02,  2.1822e-02, -4.8818e-02,\n",
      "         3.6835e-02, -2.0188e-02, -4.9397e-02, -2.7309e-02, -1.0451e-01,\n",
      "        -5.1998e-02, -7.6907e-02,  5.0253e-02, -2.5009e-02,  6.1572e-02,\n",
      "         2.2937e-02,  4.6187e-02, -3.9961e-02,  4.8570e-02,  5.2564e-02,\n",
      "         9.5382e-02,  4.8036e-02,  7.5170e-02,  9.7151e-02,  4.9757e-02,\n",
      "        -3.6747e-02,  3.7860e-02,  1.0510e-01, -3.3879e-02, -5.2640e-02,\n",
      "         1.9708e-02, -2.2487e-02,  9.2245e-02, -7.4927e-02, -7.7572e-02,\n",
      "        -2.0473e-03,  5.3398e-02,  4.7463e-07,  5.0564e-02,  4.3195e-02,\n",
      "        -5.2029e-02,  8.3390e-02,  7.3015e-02,  9.8139e-02,  3.8643e-02,\n",
      "        -6.7523e-02,  2.3842e-03, -8.8881e-02,  7.6310e-02, -3.7061e-02,\n",
      "        -6.3719e-02,  6.8274e-02,  3.2529e-02,  5.9982e-02, -3.9504e-02,\n",
      "        -1.1527e-02,  5.4428e-02,  3.2257e-02, -1.0134e-01, -5.1929e-02,\n",
      "        -3.4902e-02, -1.5436e-02,  2.6864e-02,  8.1954e-02,  7.7708e-02,\n",
      "        -2.4916e-02,  7.9446e-02, -5.5800e-02, -7.2757e-02, -6.2018e-02,\n",
      "        -1.2417e-02, -1.9008e-02,  6.6466e-02,  6.2095e-02,  5.8267e-02,\n",
      "         2.2328e-02, -3.6662e-02, -2.1215e-02, -7.5299e-02,  4.4280e-02,\n",
      "         1.5911e-02,  7.8805e-02,  6.9740e-02, -7.5334e-02,  7.1007e-02,\n",
      "         4.2262e-02, -9.6737e-03,  4.8846e-02,  6.1513e-02,  1.0263e-01,\n",
      "         9.0256e-02, -2.7158e-02,  4.3035e-02, -6.8145e-03,  5.8285e-02,\n",
      "        -8.1481e-02,  4.2957e-03,  6.5418e-02, -6.3447e-02,  6.2289e-02,\n",
      "        -8.1590e-02,  9.1597e-03,  8.1709e-03, -4.4095e-02, -3.3997e-02,\n",
      "        -5.4170e-02, -1.4361e-03, -1.1131e-01, -8.3890e-03, -5.7303e-02,\n",
      "         3.5546e-02,  8.1264e-02, -8.4258e-02,  1.0663e-01, -6.3326e-04,\n",
      "        -5.5518e-03, -2.2963e-02, -4.3172e-02, -8.1828e-02, -2.7686e-02,\n",
      "        -2.8115e-02,  7.0575e-02,  1.1161e-02, -2.8200e-03,  6.2372e-02,\n",
      "        -6.9839e-03,  7.1418e-02, -2.1579e-02, -2.0313e-02,  1.3755e-02,\n",
      "         8.8035e-02, -2.5839e-02, -3.3967e-02, -7.8681e-02, -2.7595e-02,\n",
      "         2.4440e-02, -9.1227e-02, -5.7648e-02,  1.1573e-01,  5.9091e-02,\n",
      "         5.5218e-03,  7.3769e-02, -6.1208e-03, -3.6565e-02, -7.9249e-02,\n",
      "         8.0802e-02, -6.6797e-02, -2.7095e-02,  4.9584e-02, -3.1556e-02,\n",
      "        -1.4916e-02, -3.3520e-02, -2.2191e-02, -2.1940e-02,  2.8341e-02,\n",
      "        -6.2206e-02,  3.0511e-02,  5.7514e-02, -1.1962e-02, -1.2229e-02,\n",
      "         2.8903e-02, -2.4671e-02,  3.8513e-02, -5.6006e-02,  3.4929e-02,\n",
      "         7.0036e-02, -6.6578e-02,  1.7253e-02,  5.0078e-02,  3.4146e-02,\n",
      "        -3.0493e-02,  3.4859e-03,  6.6294e-02,  7.2434e-02, -1.7294e-02,\n",
      "        -3.0530e-02, -5.5141e-02,  5.7106e-02, -3.4761e-02, -4.5398e-02,\n",
      "         1.0066e-01,  4.4660e-02,  1.2809e-02, -3.9633e-02, -5.5435e-03,\n",
      "         1.9849e-02,  9.3150e-02, -1.0267e-01, -8.7973e-02,  1.8256e-02,\n",
      "        -7.1652e-02,  4.0206e-02,  2.8365e-03, -3.7882e-02, -2.0384e-02,\n",
      "        -3.9058e-02, -7.8686e-02,  9.0474e-02, -4.0803e-02,  4.3872e-03,\n",
      "         4.6332e-02, -1.0063e-01, -5.3558e-02, -5.7871e-02, -8.1518e-02,\n",
      "         8.1129e-02,  5.5117e-02, -5.4476e-02,  1.8381e-03, -3.3540e-02,\n",
      "         1.9665e-02, -3.1630e-02,  4.8587e-02,  4.8829e-02,  2.2648e-02,\n",
      "        -9.5991e-03, -6.0252e-02,  5.4355e-02,  5.9089e-02, -4.5480e-02,\n",
      "        -2.5697e-02,  1.3254e-02, -3.6200e-02,  2.8441e-02,  8.4415e-02,\n",
      "         2.4296e-02,  5.6988e-02,  6.3160e-04, -2.5683e-02, -3.9865e-02,\n",
      "         9.0886e-02, -8.4669e-02,  2.5690e-02, -5.5732e-02, -4.8906e-02,\n",
      "         8.5303e-03, -1.9378e-02,  1.9925e-03,  2.1457e-02, -2.2175e-02,\n",
      "         3.5914e-02, -5.9011e-02,  1.0047e-02, -8.1043e-02,  7.1458e-02,\n",
      "         5.6177e-02, -1.7937e-02,  1.5874e-02,  4.2403e-02, -8.8968e-02,\n",
      "        -5.6934e-02, -1.6663e-02, -1.0930e-02, -8.8462e-02, -7.3492e-02,\n",
      "         7.1758e-02, -8.3502e-02, -8.8041e-02,  8.5506e-02,  8.3707e-02,\n",
      "         1.0054e-01,  1.5000e-02,  2.1312e-02, -2.2224e-02, -6.3245e-02,\n",
      "         4.2437e-02,  5.3310e-02,  8.3470e-02,  8.9642e-02,  1.0398e-01,\n",
      "         1.0369e-01, -3.0784e-02,  7.3321e-02,  2.2825e-02, -1.1656e-02,\n",
      "         1.0034e-01,  8.4232e-02,  2.8228e-02,  6.2936e-02,  1.1749e-02,\n",
      "        -3.5109e-02,  2.5756e-02,  7.8000e-02,  6.6970e-02, -5.2037e-03,\n",
      "         4.8265e-02,  1.0193e-01,  5.0718e-02,  5.1966e-02,  9.2932e-02,\n",
      "         2.2517e-02,  8.7424e-03,  7.9571e-02,  1.1422e-02,  8.6954e-02,\n",
      "         9.0472e-02, -7.3851e-02, -1.2664e-03, -3.5111e-02, -4.5397e-02,\n",
      "        -1.8427e-03,  6.9895e-02, -2.0104e-02,  1.0194e-01,  2.5893e-02,\n",
      "         4.1257e-02, -4.0542e-02,  1.3847e-02,  6.8911e-02,  2.7161e-02,\n",
      "        -1.7561e-02, -1.7542e-02,  5.9897e-02, -1.8631e-02,  3.2623e-02,\n",
      "        -4.9816e-03, -2.0198e-02,  3.6686e-03, -5.8976e-02,  1.0774e-01,\n",
      "        -2.5115e-02,  6.7051e-02,  1.2328e-02,  1.2928e-01,  5.8454e-02,\n",
      "        -1.5823e-02,  1.0264e-01, -4.5950e-02,  4.4241e-03,  2.6532e-02,\n",
      "        -3.8706e-02, -6.5427e-02,  3.2355e-03,  9.7295e-02, -3.3315e-02,\n",
      "         6.5620e-02, -4.4553e-03,  5.3687e-02, -5.2514e-02, -3.0655e-02,\n",
      "         6.8303e-02,  9.1619e-02,  7.7134e-02,  9.7615e-02,  7.9441e-02,\n",
      "         1.0299e-01,  5.9665e-02,  5.5214e-02, -4.3603e-02,  9.3997e-02,\n",
      "         1.3382e-01, -1.5410e-02,  1.1020e-01,  4.4757e-02, -1.5528e-03,\n",
      "         1.8984e-02,  1.1582e-01,  4.4648e-02, -4.9714e-02, -1.5383e-02,\n",
      "         6.0857e-02,  1.8934e-02,  4.5786e-02,  1.2560e-01,  8.5625e-02,\n",
      "         8.4659e-02,  1.1553e-02,  1.2826e-02, -8.7047e-03,  7.2820e-02,\n",
      "         2.5911e-02,  1.2923e-01,  1.0273e-01, -7.2107e-02,  1.0380e-01,\n",
      "         1.7774e-02,  4.8796e-02, -1.3916e-03,  1.0331e-01,  1.0473e-01,\n",
      "         2.2464e-02,  7.0198e-02,  4.4491e-02,  3.9070e-02,  6.0961e-02,\n",
      "         1.1655e-01, -2.7122e-02], requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[ 6.1785e-02,  8.5634e-03,  3.1633e-02,  ...,  4.5079e-02,\n",
      "          4.0395e-02,  5.3130e-03],\n",
      "        [-1.2464e-02, -6.3470e-02,  9.1802e-02,  ...,  4.6730e-02,\n",
      "          5.1205e-02, -6.4866e-02],\n",
      "        [ 4.3775e-02,  8.6707e-02,  6.4379e-02,  ..., -1.2397e-01,\n",
      "         -4.4382e-02,  1.1604e-02],\n",
      "        ...,\n",
      "        [ 7.8260e-02, -1.5236e-01, -3.7829e-02,  ...,  2.5807e-05,\n",
      "         -6.8828e-02,  2.1067e-02],\n",
      "        [-5.3135e-03, -8.0686e-02,  1.5477e-01,  ..., -5.4435e-02,\n",
      "         -8.0316e-02, -6.4509e-02],\n",
      "        [-2.7740e-02, -2.5735e-01, -1.5448e-02,  ..., -1.0369e-02,\n",
      "          1.3861e-01,  3.5126e-03]], requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([-0.0092, -0.0303,  0.0445, -0.0246, -0.0082,  0.0552, -0.0001,  0.0134,\n",
      "        -0.0025,  0.0262,  0.0666,  0.0055,  0.0165, -0.0012, -0.0009, -0.0654,\n",
      "         0.0480,  0.0096, -0.0347,  0.0148], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_afr, valid_data_afr, test_data_afr), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_afr.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_afr.vocab)\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "pad_index = train_afr.vocab.stoi[train_afr.pad_token]\n",
    "tag_pad_idx = tagged_train_afr.vocab.stoi[tagged_train_afr.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTMTagger_Pretrained(afr_weights_matrix, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate dutch params in dict\n",
    "transfer_param_dict = {}\n",
    "params = model.named_parameters()\n",
    "for name, param in params:\n",
    "    transfer_param_dict[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l1_reverse', 'lstm.weight_hh_l1_reverse', 'lstm.bias_ih_l1_reverse', 'lstm.bias_hh_l1_reverse', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(transfer_param_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "params2 = model2.named_parameters()\n",
    "for name, param in params2:\n",
    "    if(name == \"embedding.weight\" or name == \"fc.weight\" or name == \"fc.bias\"):\n",
    "        continue\n",
    "    else:\n",
    "        param.data = transfer_param_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 2.571 | Train Acc: 19.98%\n",
      "\t Val. Loss: 2.261 |  Val. Acc: 37.95%\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 1.915 | Train Acc: 49.32%\n",
      "\t Val. Loss: 1.726 |  Val. Acc: 55.73%\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 1.336 | Train Acc: 67.49%\n",
      "\t Val. Loss: 1.211 |  Val. Acc: 69.64%\n",
      "Epoch: 04 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.860 | Train Acc: 81.62%\n",
      "\t Val. Loss: 0.820 |  Val. Acc: 82.57%\n",
      "Epoch: 05 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.538 | Train Acc: 90.07%\n",
      "\t Val. Loss: 0.582 |  Val. Acc: 87.99%\n",
      "Epoch: 06 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.345 | Train Acc: 93.40%\n",
      "\t Val. Loss: 0.460 |  Val. Acc: 89.54%\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.237 | Train Acc: 95.28%\n",
      "\t Val. Loss: 0.394 |  Val. Acc: 90.71%\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.173 | Train Acc: 96.61%\n",
      "\t Val. Loss: 0.353 |  Val. Acc: 91.21%\n",
      "Epoch: 09 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.133 | Train Acc: 97.73%\n",
      "\t Val. Loss: 0.334 |  Val. Acc: 91.43%\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.104 | Train Acc: 98.43%\n",
      "\t Val. Loss: 0.313 |  Val. Acc: 91.75%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model2, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model2, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.301 |  Test Acc: 92.56%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
