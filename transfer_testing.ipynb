{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll #pip3 install this if you don't have it\n",
    "import torchtext.data as tt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torchtext\n",
    "from tools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRIKAANS_TRAIN = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "AFRIKAANS_DEV = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu'\n",
    "AFRIKAANS_TEST = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu'\n",
    "\n",
    "DUTCH_TRAIN = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\"\n",
    "DUTCH_DEV = \"UD_Dutch-Alpino/nl_alpino-ud-dev.conllu\"\n",
    "DUTCH_TEST = \"UD_Dutch-Alpino/nl_alpino-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb\n",
    "def make_sentences(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    sentences = []\n",
    "    tagged_sentences = []\n",
    "    for each in data:\n",
    "        tagged_sentence=[]\n",
    "        sentence = []\n",
    "        for token in each:\n",
    "            if token.upos and token.form:\n",
    "                tagged_sentence.append(token.upos)\n",
    "                sentence.append(token.form.lower())\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_afr_raw, tagged_train_afr_raw = make_sentences(AFRIKAANS_TRAIN)\n",
    "dev_afr_raw, tagged_dev_afr_raw = make_sentences(AFRIKAANS_DEV)\n",
    "test_afr_raw, tagged_test_afr_raw = make_sentences(AFRIKAANS_TEST)\n",
    "\n",
    "train_du_raw, tagged_train_du_raw = make_sentences(DUTCH_TRAIN)\n",
    "dev_du_raw, tagged_dev_du_raw = make_sentences(DUTCH_DEV)\n",
    "test_du_raw, tagged_test_du_raw = make_sentences(DUTCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFRIKAANS\n",
      "Tagged sentences in train set:  1315\n",
      "Tagged words in train set: 33894\n",
      "========================================\n",
      "Tagged sentences in dev set:  194\n",
      "Tagged words in dev set: 5317\n",
      "========================================\n",
      "Tagged sentences in test set:  425\n",
      "Tagged words in test set: 10065\n",
      "****************************************\n",
      "Total sentences in dataset: 1934\n"
     ]
    }
   ],
   "source": [
    "print(\"AFRIKAANS\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_afr_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_afr_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_afr_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_afr_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_afr_raw)+len(tagged_dev_afr_raw)+len(tagged_test_afr_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH\n",
      "Tagged sentences in train set:  12264\n",
      "Tagged words in train set: 185999\n",
      "========================================\n",
      "Tagged sentences in dev set:  718\n",
      "Tagged words in dev set: 11549\n",
      "========================================\n",
      "Tagged sentences in test set:  596\n",
      "Tagged words in test set: 11053\n",
      "****************************************\n",
      "Total sentences in dataset: 13578\n"
     ]
    }
   ],
   "source": [
    "print(\"DUTCH\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_du_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_du_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_du_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_du_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_du_raw)+len(tagged_dev_du_raw)+len(tagged_test_du_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tringm/POSTagger_Pytorch/blob/master/src/util/nlp.py\n",
    "def build_tag_field(sentences_tokens):\n",
    "    token_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('tokens', token_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_tokens]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return token_field\n",
    "    \n",
    "def build_text_field(sentences_words):\n",
    "    text_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_words]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, AFR\n",
    "train_afr = build_text_field(train_afr_raw)\n",
    "dev_afr = build_text_field(dev_afr_raw)\n",
    "test_afr = build_text_field(test_afr_raw)\n",
    "tagged_train_afr = build_tag_field(tagged_train_afr_raw)\n",
    "tagged_dev_afr = build_tag_field(tagged_dev_afr_raw)\n",
    "tagged_test_afr = build_tag_field(tagged_test_afr_raw)\n",
    "\n",
    "fields_train_afr = ((\"text\", train_afr), (\"udtags\", tagged_train_afr))\n",
    "examples_train_afr = [tt.Example.fromlist(item, fields_train_afr) for item in zip(train_afr_raw, tagged_train_afr_raw)]\n",
    "fields_dev_afr = ((\"text\", dev_afr), (\"udtags\", tagged_dev_afr))\n",
    "examples_dev_afr = [tt.Example.fromlist(item, fields_dev_afr) for item in zip(dev_afr_raw, tagged_dev_afr_raw)]\n",
    "fields_test_afr = ((\"text\", test_afr), (\"udtags\", tagged_test_afr))\n",
    "examples_test_afr = [tt.Example.fromlist(item, fields_test_afr) for item in zip(test_afr_raw, tagged_test_afr_raw)]\n",
    "\n",
    "train_data_afr = tt.Dataset(examples_train_afr, fields_train_afr)\n",
    "valid_data_afr = tt.Dataset(examples_dev_afr, fields_dev_afr)\n",
    "test_data_afr = tt.Dataset(examples_test_afr, fields_test_afr)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "#train_afr.vocab = af_vec\n",
    "dev_afr.vocab = train_afr.vocab\n",
    "test_afr.vocab = train_afr.vocab\n",
    "tagged_train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "tagged_dev_afr.vocab = tagged_train_afr.vocab\n",
    "tagged_test_afr.vocab = tagged_train_afr.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, DUT\n",
    "train_du = build_text_field(train_du_raw)\n",
    "dev_du = build_text_field(dev_du_raw)\n",
    "test_du = build_text_field(test_du_raw)\n",
    "tagged_train_du = build_tag_field(tagged_train_du_raw)\n",
    "tagged_dev_du = build_tag_field(tagged_dev_du_raw)\n",
    "tagged_test_du = build_tag_field(tagged_test_du_raw)\n",
    "\n",
    "fields_train_du = ((\"text\", train_du), (\"udtags\", tagged_train_du))\n",
    "examples_train_du = [tt.Example.fromlist(item, fields_train_du) for item in zip(train_du_raw, tagged_train_du_raw)]\n",
    "fields_dev_du = ((\"text\", dev_du), (\"udtags\", tagged_dev_du))\n",
    "examples_dev_du = [tt.Example.fromlist(item, fields_dev_du) for item in zip(dev_du_raw, tagged_dev_du_raw)]\n",
    "fields_test_du = ((\"text\", test_du), (\"udtags\", tagged_test_du))\n",
    "examples_test_du = [tt.Example.fromlist(item, fields_test_du) for item in zip(test_du_raw, tagged_test_du_raw)]\n",
    "\n",
    "train_data_du = tt.Dataset(examples_train_du, fields_train_du)\n",
    "valid_data_du = tt.Dataset(examples_dev_du, fields_dev_du)\n",
    "test_data_du = tt.Dataset(examples_test_du, fields_test_du)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "#train_du.vocab = nl_vec\n",
    "dev_du.vocab = train_du.vocab\n",
    "test_du.vocab = train_du.vocab\n",
    "tagged_train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "tagged_dev_du.vocab = tagged_train_du.vocab\n",
    "tagged_test_du.vocab = tagged_train_du.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "#model\n",
    "batch_size=128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#needs to be tuple of dataset objects\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_du, valid_data_du, test_data_du), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_du.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_du.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_du.vocab.stoi[train_du.pad_token]\n",
    "tag_pad_idx = tagged_train_du.vocab.stoi[tagged_train_du.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMTagger(in_dim, emb_dim, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(text)        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags) \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dutch POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 1.475 | Train Acc: 59.94%\n",
      "\t Val. Loss: 0.879 |  Val. Acc: 71.99%\n",
      "Epoch: 02 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.649 | Train Acc: 79.18%\n",
      "\t Val. Loss: 0.647 |  Val. Acc: 79.45%\n",
      "Epoch: 03 | Epoch Time: 0m 51s\n",
      "\tTrain Loss: 0.480 | Train Acc: 84.51%\n",
      "\t Val. Loss: 0.540 |  Val. Acc: 82.76%\n",
      "Epoch: 04 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.384 | Train Acc: 87.73%\n",
      "\t Val. Loss: 0.490 |  Val. Acc: 84.06%\n",
      "Epoch: 05 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.317 | Train Acc: 89.97%\n",
      "\t Val. Loss: 0.434 |  Val. Acc: 86.04%\n",
      "Epoch: 06 | Epoch Time: 1m 27s\n",
      "\tTrain Loss: 0.263 | Train Acc: 91.81%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 87.05%\n",
      "Epoch: 07 | Epoch Time: 1m 48s\n",
      "\tTrain Loss: 0.220 | Train Acc: 93.19%\n",
      "\t Val. Loss: 0.372 |  Val. Acc: 88.10%\n",
      "Epoch: 08 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.184 | Train Acc: 94.46%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 88.66%\n",
      "Epoch: 09 | Epoch Time: 1m 36s\n",
      "\tTrain Loss: 0.152 | Train Acc: 95.56%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 89.08%\n",
      "Epoch: 10 | Epoch Time: 1m 37s\n",
      "\tTrain Loss: 0.124 | Train Acc: 96.53%\n",
      "\t Val. Loss: 0.343 |  Val. Acc: 89.54%\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch: 11 | Epoch Time: 1m 37s\n",
      "\tTrain Loss: 0.101 | Train Acc: 97.28%\n",
      "\t Val. Loss: 0.344 |  Val. Acc: 89.65%\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch: 12 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.082 | Train Acc: 97.91%\n",
      "\t Val. Loss: 0.348 |  Val. Acc: 89.61%\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping, reloading checkpoint model\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "patience = 3\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=False,filename='checkpt.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping, reloading checkpoint model\")\n",
    "        model.load_state_dict(torch.load('checkpt.pt'))\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.353 |  Test Acc: 88.30%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding.weight', Parameter containing:\n",
      "tensor([[ 0.2436, -0.3784, -0.1444,  ..., -0.8988, -0.2998, -0.1949],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1849,  0.5520,  0.1829,  ...,  0.6962,  0.6675,  0.3490],\n",
      "        ...,\n",
      "        [-0.1972, -1.8140, -1.2404,  ..., -0.1259,  0.8170, -0.7388],\n",
      "        [ 0.3744, -1.1066,  1.5164,  ..., -1.7509, -0.1311,  0.3587],\n",
      "        [ 1.0007, -0.2663, -0.7185,  ...,  0.0577, -1.2294, -0.8283]],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.0998, -0.1207,  0.1599,  ...,  0.0201,  0.0673, -0.0819],\n",
      "        [-0.0680, -0.2432, -0.0775,  ...,  0.0259, -0.2502, -0.0508],\n",
      "        [ 0.0327,  0.0144,  0.1106,  ...,  0.1337,  0.0953,  0.0707],\n",
      "        ...,\n",
      "        [ 0.0067,  0.0017,  0.0221,  ..., -0.1124, -0.0532,  0.0960],\n",
      "        [-0.0859,  0.1256,  0.1068,  ...,  0.0086,  0.0474, -0.0104],\n",
      "        [-0.2324,  0.0411, -0.0957,  ..., -0.0643,  0.1035, -0.2875]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.0739, -0.0836, -0.1058,  ...,  0.1002, -0.0590,  0.1484],\n",
      "        [ 0.0904, -0.0169, -0.0391,  ...,  0.0832,  0.1553, -0.0386],\n",
      "        [ 0.1161, -0.0981, -0.1400,  ...,  0.2044, -0.0940,  0.0124],\n",
      "        ...,\n",
      "        [ 0.1791, -0.1346, -0.1685,  ...,  0.0907,  0.0627,  0.1554],\n",
      "        [ 0.1991, -0.0204, -0.1083,  ...,  0.1794,  0.0060,  0.1510],\n",
      "        [ 0.1243, -0.0823, -0.0180,  ...,  0.0919, -0.0415,  0.0573]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0', Parameter containing:\n",
      "tensor([-1.8680e-03,  1.2175e-01,  6.6708e-02, -3.4224e-03, -1.6721e-02,\n",
      "        -2.1758e-02,  4.3902e-02,  3.8545e-02, -1.1687e-02,  5.3786e-02,\n",
      "         1.5349e-01,  1.0925e-01,  1.1007e-01,  9.2069e-02,  5.5688e-02,\n",
      "         8.0946e-02,  1.2552e-01,  8.9293e-02,  5.3231e-03,  2.5812e-02,\n",
      "         6.6692e-02,  1.2285e-01,  4.1940e-02,  3.5129e-02,  6.8118e-02,\n",
      "         1.2724e-01,  1.1937e-01,  1.2823e-01, -1.5586e-05,  1.4000e-01,\n",
      "         1.1395e-01,  1.3584e-01,  1.6397e-01,  9.0069e-02, -4.5137e-03,\n",
      "         4.1591e-02,  1.0176e-01,  1.5135e-01,  9.8621e-02,  1.3515e-01,\n",
      "         1.8151e-01,  8.2889e-02,  6.6041e-02,  9.3811e-02,  1.4922e-01,\n",
      "         1.2583e-01,  1.6140e-01,  1.3101e-01,  1.1925e-01,  7.8531e-02,\n",
      "         1.0187e-01,  3.1287e-02,  1.4308e-01,  1.2852e-01,  5.8579e-02,\n",
      "         1.1907e-01,  1.0129e-01,  4.8265e-02,  6.7784e-02,  1.0818e-01,\n",
      "         2.8216e-02,  9.0785e-02,  2.3110e-02,  7.3873e-02,  6.9527e-02,\n",
      "         7.4738e-02,  8.2772e-02,  8.7381e-02,  1.5370e-01,  9.8294e-02,\n",
      "         1.1083e-01,  1.2020e-01,  1.7018e-02,  1.0990e-01,  3.4559e-02,\n",
      "         7.7488e-02, -6.2257e-03,  8.5327e-02,  1.4094e-01,  5.9454e-02,\n",
      "         6.5390e-03,  5.4039e-02,  8.6321e-02,  8.3270e-02,  1.4212e-01,\n",
      "         1.0989e-01, -1.6023e-02,  4.6835e-02,  3.7416e-02,  1.1374e-01,\n",
      "         4.5841e-02,  2.7150e-02, -8.2244e-03, -3.1457e-03,  6.5344e-03,\n",
      "         7.8935e-02,  4.0214e-02,  7.9310e-02,  1.2648e-01,  1.3424e-01,\n",
      "         1.1589e-01,  5.4726e-02,  9.2583e-02,  2.8073e-02,  1.5490e-01,\n",
      "         8.8732e-02,  3.4402e-02,  6.2873e-02,  1.0646e-01,  2.9059e-02,\n",
      "         6.7095e-02,  1.4090e-01,  2.5533e-03,  6.9444e-02,  1.3879e-01,\n",
      "        -6.8432e-03,  7.7433e-02,  1.1118e-01,  1.2242e-01,  4.8326e-02,\n",
      "         8.7107e-03,  8.1749e-02,  1.2673e-01,  8.6006e-03,  1.5092e-01,\n",
      "         9.1812e-02,  5.4141e-02,  5.3918e-02,  7.0881e-03, -9.2315e-03,\n",
      "        -2.4341e-02,  5.6131e-02, -5.1172e-02,  1.1629e-02,  5.4539e-02,\n",
      "         1.1879e-02,  6.8797e-02,  6.8131e-02, -5.2209e-03, -1.9828e-03,\n",
      "         8.8239e-02,  3.7885e-03, -8.2048e-02, -5.5264e-02,  6.4914e-02,\n",
      "         3.6703e-02, -4.4113e-02, -1.7529e-02,  1.3164e-02, -2.6000e-02,\n",
      "        -7.4726e-02,  1.9311e-02,  6.1505e-02,  4.2888e-02, -9.4089e-02,\n",
      "        -1.5036e-02, -6.0700e-02,  2.2632e-02, -6.9477e-02,  5.6315e-02,\n",
      "         2.2939e-03,  9.2125e-02,  4.3509e-02, -5.9661e-03, -6.1262e-02,\n",
      "         7.2231e-02,  9.6904e-02, -1.1870e-01,  7.0630e-02,  7.9306e-02,\n",
      "         1.6293e-02,  1.4151e-01, -5.9151e-02,  3.2868e-02, -4.6343e-02,\n",
      "         8.9615e-02,  5.3228e-02,  6.5695e-02,  1.0167e-01, -1.0671e-01,\n",
      "         2.2094e-02, -2.3811e-02, -2.5804e-02, -3.5304e-03,  3.9681e-02,\n",
      "         2.1858e-02,  8.1184e-03,  4.6678e-02,  2.8555e-02,  7.0929e-02,\n",
      "         6.1496e-02, -1.2708e-02, -6.4882e-02, -1.8967e-02,  1.3930e-01,\n",
      "         4.0083e-02,  7.2659e-02,  1.2556e-01, -1.0760e-02,  2.2406e-02,\n",
      "         2.1814e-02, -1.3710e-02,  5.0579e-02,  6.2127e-02,  3.3482e-02,\n",
      "         1.4068e-02, -1.0824e-01, -8.2734e-02,  1.0304e-02,  4.1579e-02,\n",
      "         1.3259e-02,  6.2581e-02, -1.9068e-02, -1.2460e-03, -3.6253e-02,\n",
      "         4.0681e-02, -9.1261e-02, -1.4232e-02,  5.7047e-02,  3.9563e-02,\n",
      "        -5.1603e-03,  3.8687e-02,  6.9138e-02, -5.9139e-02,  3.2854e-02,\n",
      "         2.1408e-02, -3.7649e-02, -5.6195e-02,  9.5156e-02, -5.4888e-02,\n",
      "         1.0051e-01, -8.8976e-02,  3.0870e-03,  6.1848e-02,  4.4744e-02,\n",
      "        -3.5497e-02,  1.1307e-03, -2.2554e-02, -3.2756e-02, -6.4835e-02,\n",
      "        -8.3060e-02,  1.9417e-02,  1.2326e-01,  7.9848e-02, -3.0913e-02,\n",
      "         1.1925e-01, -6.0171e-02,  5.1517e-03,  4.8494e-02,  4.1245e-02,\n",
      "        -5.8852e-02,  7.7125e-02,  3.1181e-02,  6.1863e-02, -1.3995e-01,\n",
      "         1.2463e-01,  7.4513e-02,  4.3759e-02, -9.0824e-02, -5.2240e-02,\n",
      "         4.0477e-02,  7.2727e-03,  1.0539e-01, -2.9957e-02,  3.5139e-02,\n",
      "        -1.1469e-01,  5.1128e-02,  6.6739e-02,  5.3301e-02, -1.0830e-01,\n",
      "        -4.3039e-02,  5.5127e-02, -7.8982e-02,  8.1043e-02, -5.8708e-02,\n",
      "        -4.9346e-02,  2.1516e-03, -1.3915e-02,  6.0685e-02,  5.0834e-02,\n",
      "        -4.6338e-02,  9.3142e-03,  4.5521e-02, -7.6941e-02,  7.7300e-02,\n",
      "         1.1397e-01, -4.3140e-02, -9.5562e-02,  7.2661e-02,  1.0977e-01,\n",
      "        -6.4506e-02,  5.1785e-02, -6.8430e-02,  7.8544e-03,  2.0328e-02,\n",
      "        -4.9126e-02, -6.5528e-02,  1.0320e-01, -8.0069e-02,  1.3373e-01,\n",
      "        -1.9639e-02, -7.7322e-03, -1.1297e-02, -1.1290e-02,  1.2359e-01,\n",
      "        -4.3995e-02, -3.0140e-02, -8.2925e-02,  5.4945e-02,  1.3972e-01,\n",
      "         3.8401e-03,  9.1757e-02, -5.5105e-03, -6.7603e-02,  2.2754e-02,\n",
      "         3.9285e-02, -1.0908e-01, -1.2052e-01, -2.5729e-02,  4.7369e-02,\n",
      "         9.5795e-02, -4.0903e-02,  4.2157e-02, -1.5839e-02,  2.2315e-02,\n",
      "        -1.1413e-01, -6.7982e-02, -7.2019e-02, -9.1921e-02, -4.6466e-02,\n",
      "        -3.2981e-02,  3.5106e-02, -4.5248e-02, -3.4375e-02,  2.8544e-02,\n",
      "        -1.1029e-01, -8.4797e-02, -7.9045e-02,  1.1297e-01,  3.6015e-02,\n",
      "         6.8125e-02, -9.0406e-02, -9.4332e-02,  7.4765e-02,  6.8192e-02,\n",
      "        -7.6990e-02,  1.1142e-01,  1.7976e-02,  9.1148e-02,  9.7777e-02,\n",
      "        -7.1161e-02, -9.4560e-02, -1.0115e-01, -1.6212e-02, -3.5546e-02,\n",
      "        -6.4141e-02,  4.5522e-02, -9.7773e-02, -1.3211e-01,  1.1333e-02,\n",
      "         1.5655e-01,  8.2270e-02,  1.0942e-01,  4.2015e-03, -9.9892e-02,\n",
      "        -1.1817e-01, -8.3041e-02,  5.6431e-02, -1.3964e-01, -7.1963e-02,\n",
      "        -1.0892e-01, -1.1549e-01, -1.1275e-01,  8.5669e-02, -5.8668e-02,\n",
      "         5.1483e-02,  3.0907e-02,  2.2017e-02, -7.7914e-02, -3.1143e-02,\n",
      "        -1.3091e-01,  9.4490e-02,  6.5837e-02,  3.1623e-02,  1.7964e-01,\n",
      "         2.6931e-02,  5.3711e-02,  7.6751e-02,  1.8077e-04,  1.8616e-02,\n",
      "        -9.7648e-03,  1.1029e-02,  2.7002e-02,  1.0847e-01,  3.7924e-02,\n",
      "         9.8451e-02,  1.7498e-01,  8.5282e-02,  2.4825e-02, -7.4048e-03,\n",
      "         1.5702e-02,  1.1004e-01,  1.0758e-01,  1.0955e-01,  1.4778e-01,\n",
      "         8.9811e-02,  2.1984e-03,  1.3636e-01,  1.0503e-01,  1.1263e-01,\n",
      "         1.3894e-01,  1.4805e-01,  3.2422e-02,  5.5896e-02,  5.1104e-02,\n",
      "         4.2955e-02,  2.8858e-02,  8.4626e-02,  1.2234e-01,  1.2613e-01,\n",
      "         9.9630e-02,  4.5593e-02,  4.1618e-02,  5.7905e-02,  2.4660e-02,\n",
      "         1.3392e-01,  8.4444e-02,  1.2154e-01,  9.3067e-02,  5.0372e-02,\n",
      "         5.3795e-02,  5.8626e-03,  2.4806e-02,  9.8058e-02,  1.1616e-01,\n",
      "         1.4543e-01,  2.3956e-02,  3.6375e-02,  5.8079e-02,  1.1427e-01,\n",
      "         1.3963e-01,  1.1050e-01,  3.7838e-02,  1.0112e-01,  1.7167e-01,\n",
      "         2.1118e-02,  1.6483e-01,  8.4221e-02,  1.7843e-01,  1.4468e-01,\n",
      "        -8.9199e-03,  1.3547e-01,  1.1203e-01,  9.3781e-02,  1.2403e-01,\n",
      "         1.1910e-01,  1.0168e-01,  1.4983e-02,  1.8688e-02,  1.8505e-02,\n",
      "         2.2207e-02, -8.2303e-03,  1.5142e-01,  1.5385e-01,  4.1028e-02,\n",
      "        -2.3664e-02,  5.5386e-02,  1.0649e-01,  2.6822e-02,  1.7800e-01,\n",
      "         8.5616e-03,  3.4043e-02, -3.8440e-03,  1.0360e-01,  1.1673e-01,\n",
      "         2.4101e-04,  1.2024e-01,  1.3479e-01,  1.5141e-01,  1.1754e-02,\n",
      "         5.2122e-02,  1.5639e-01,  8.0466e-02,  2.8213e-02,  1.4020e-01,\n",
      "         2.4725e-02,  5.9680e-02,  7.6594e-02,  1.2207e-01,  1.2918e-01,\n",
      "         1.1565e-01,  3.2115e-02,  8.5490e-02,  5.9835e-02,  8.9242e-02,\n",
      "         1.2170e-01,  8.4260e-02,  2.4268e-02,  6.8599e-02,  2.8462e-02,\n",
      "        -1.6856e-02,  1.3069e-01,  1.1742e-01,  6.5506e-02,  7.6988e-02,\n",
      "         1.6205e-01,  1.8655e-01,  1.3944e-01,  6.5543e-02,  7.1950e-02,\n",
      "         1.0404e-01,  6.3656e-02], requires_grad=True)), ('lstm.bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.0903,  0.1553,  0.1053, -0.0013,  0.0621,  0.0694,  0.1199,  0.0125,\n",
      "         0.1255,  0.1215,  0.0872,  0.1277,  0.0190,  0.0646,  0.1367,  0.0886,\n",
      "         0.1276,  0.0554,  0.1155, -0.0130, -0.0345,  0.0372,  0.0453,  0.0712,\n",
      "         0.0279,  0.1196,  0.0722, -0.0070,  0.1439,  0.0871, -0.0240,  0.0217,\n",
      "         0.1368,  0.0562,  0.1384,  0.0557,  0.0016, -0.0059,  0.0627, -0.0056,\n",
      "         0.1126,  0.0909,  0.1539,  0.1493,  0.0974,  0.1662,  0.0662, -0.0056,\n",
      "         0.1522,  0.0085,  0.0411,  0.1471, -0.0023,  0.0266,  0.0992,  0.0283,\n",
      "         0.1594,  0.0154,  0.1215,  0.0285,  0.0576, -0.0067,  0.0762,  0.0639,\n",
      "         0.0890,  0.1158,  0.0921, -0.0613,  0.1718, -0.0137,  0.0818,  0.0144,\n",
      "         0.1020,  0.1648, -0.0080,  0.0746,  0.0425,  0.0120,  0.0964,  0.0184,\n",
      "         0.0586,  0.0141,  0.0711,  0.0422,  0.1527,  0.1319,  0.1030,  0.0564,\n",
      "         0.1186,  0.1167,  0.1574,  0.0919,  0.0758, -0.0010,  0.0156,  0.0670,\n",
      "         0.0214,  0.1050,  0.1188,  0.0315,  0.0213,  0.0396,  0.0630,  0.0061,\n",
      "         0.1666,  0.1248,  0.0823,  0.0206,  0.1246,  0.0590,  0.0073,  0.1453,\n",
      "         0.1306,  0.0227,  0.0973,  0.1244,  0.0059,  0.0865,  0.0198,  0.1456,\n",
      "         0.1422, -0.0032,  0.1357,  0.0552,  0.1422,  0.0571,  0.1326,  0.0555,\n",
      "         0.1316, -0.0849, -0.0755, -0.0685,  0.0168,  0.0665,  0.0940, -0.0417,\n",
      "         0.0358,  0.0069, -0.0986, -0.0098, -0.0250,  0.0068,  0.0188, -0.0497,\n",
      "        -0.0314, -0.0275,  0.0150,  0.1317,  0.0055, -0.0184, -0.0054, -0.0967,\n",
      "         0.1059, -0.0317, -0.0680,  0.1154,  0.0578,  0.0213, -0.0591,  0.0576,\n",
      "        -0.0874, -0.0248,  0.0778,  0.0506,  0.0540, -0.0527,  0.0466,  0.0175,\n",
      "        -0.0585, -0.0264, -0.0946,  0.1261,  0.0109,  0.0291,  0.0552,  0.0241,\n",
      "         0.0036,  0.0937,  0.0152, -0.1220,  0.0631,  0.1138, -0.0980,  0.1079,\n",
      "        -0.0421,  0.0606, -0.0597, -0.0268,  0.1140,  0.0503, -0.0182, -0.0371,\n",
      "        -0.0421,  0.0585,  0.0019,  0.0392, -0.0779, -0.0442, -0.0217, -0.0300,\n",
      "         0.1254,  0.0065, -0.0072, -0.0604,  0.0355, -0.0103,  0.0147, -0.0493,\n",
      "         0.0290, -0.0137, -0.0223,  0.0236, -0.1069, -0.0196, -0.0016, -0.0159,\n",
      "         0.0441, -0.0213,  0.0083, -0.0866, -0.0019,  0.0192,  0.0727, -0.0566,\n",
      "        -0.0783,  0.0942, -0.0338,  0.0606,  0.0065, -0.0058,  0.0348, -0.1102,\n",
      "        -0.0132, -0.0264,  0.0416, -0.0068, -0.0478,  0.0304, -0.0608, -0.0841,\n",
      "         0.0590, -0.0028,  0.0163,  0.0236,  0.1208,  0.0390, -0.0665,  0.0415,\n",
      "         0.0500, -0.1032, -0.0562, -0.0598, -0.0702, -0.0068, -0.0288,  0.0327,\n",
      "         0.0462, -0.0933, -0.0231, -0.1353,  0.0089,  0.1072,  0.1126,  0.0954,\n",
      "        -0.0340, -0.0875,  0.1085, -0.0482,  0.0779, -0.0297,  0.0157,  0.0114,\n",
      "        -0.0529,  0.0599,  0.0306,  0.0343, -0.0096, -0.0917, -0.0597,  0.0091,\n",
      "        -0.0031,  0.0174,  0.1006, -0.0375,  0.0534, -0.0220, -0.0534, -0.0884,\n",
      "        -0.0070,  0.0304, -0.0088,  0.0860, -0.1138,  0.0757,  0.0357,  0.0056,\n",
      "        -0.0089,  0.0376, -0.1167,  0.1103, -0.0850, -0.1243,  0.1197, -0.1044,\n",
      "         0.0501,  0.0500, -0.1337,  0.0266,  0.1368,  0.0911,  0.0312,  0.1220,\n",
      "        -0.1064, -0.0832, -0.0949,  0.0671,  0.0146, -0.0355, -0.0354, -0.1054,\n",
      "         0.0899, -0.0391, -0.0356,  0.0274,  0.0416, -0.0873, -0.0711, -0.0350,\n",
      "        -0.0409, -0.1489,  0.0162,  0.0088,  0.0677,  0.0343, -0.0824, -0.0208,\n",
      "         0.0195, -0.0052,  0.0925,  0.0758, -0.0519,  0.0137, -0.0917,  0.1097,\n",
      "         0.0720, -0.0498,  0.0181,  0.0733, -0.0052,  0.0212,  0.0007,  0.0345,\n",
      "         0.0384,  0.0981,  0.0063, -0.1142, -0.0467,  0.0022, -0.0867, -0.0301,\n",
      "         0.0969,  0.0086,  0.0776, -0.0040, -0.0207, -0.0732, -0.0409, -0.1065,\n",
      "        -0.0941, -0.1294, -0.0766, -0.0363, -0.1139,  0.0843, -0.1177,  0.1199,\n",
      "         0.1380,  0.0406,  0.0055,  0.1197, -0.0844,  0.0450, -0.0869,  0.1261,\n",
      "         0.1191,  0.1458,  0.0667,  0.0974,  0.0917,  0.1248,  0.1307,  0.0658,\n",
      "         0.1190,  0.1390,  0.0690,  0.1112,  0.0920,  0.1064,  0.1283,  0.1325,\n",
      "         0.1116,  0.1585,  0.0571,  0.0094,  0.0081,  0.0958, -0.0262,  0.1214,\n",
      "         0.1187,  0.0824, -0.0048,  0.1421,  0.0500, -0.0081,  0.1104,  0.1370,\n",
      "         0.1433,  0.0203,  0.0863,  0.0823,  0.0180,  0.1221,  0.0709,  0.0779,\n",
      "         0.1375,  0.0673,  0.1242,  0.0525,  0.0828,  0.1716,  0.0467,  0.1167,\n",
      "         0.0524,  0.0777,  0.1328,  0.1494,  0.1522,  0.1409,  0.0746,  0.0563,\n",
      "         0.0438,  0.0192,  0.0479,  0.0555,  0.1386,  0.1527,  0.0494,  0.0176,\n",
      "         0.1052,  0.0954,  0.0058,  0.0599,  0.0459,  0.1264,  0.0790,  0.0323,\n",
      "         0.0701,  0.1312,  0.0819,  0.0183,  0.0570,  0.0991,  0.0048,  0.0537,\n",
      "         0.1391,  0.1301,  0.0617,  0.1082,  0.0829,  0.0728, -0.0052,  0.0381,\n",
      "         0.0325,  0.1086,  0.0126,  0.1373,  0.1305,  0.0926,  0.1437,  0.1077,\n",
      "         0.0745,  0.0754,  0.1118,  0.0508,  0.0734,  0.1522,  0.0094,  0.1115,\n",
      "         0.0348,  0.0965, -0.0051,  0.0928,  0.1109,  0.0466,  0.0907,  0.1408,\n",
      "         0.0088,  0.0044,  0.0096,  0.0799,  0.0760,  0.0648,  0.0652,  0.1114,\n",
      "         0.1129,  0.0421,  0.0437,  0.0227,  0.1273,  0.0894,  0.1743,  0.0833],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.0745,  0.0949, -0.0649,  ...,  0.1063, -0.0810, -0.0061],\n",
      "        [ 0.1085, -0.0509,  0.1012,  ...,  0.0250, -0.1337,  0.0267],\n",
      "        [ 0.0184, -0.0184,  0.0439,  ..., -0.0385, -0.1204, -0.0648],\n",
      "        ...,\n",
      "        [-0.0357,  0.0656, -0.0515,  ..., -0.1550,  0.0106, -0.0753],\n",
      "        [ 0.0872,  0.0895,  0.0852,  ..., -0.1155,  0.0997, -0.0462],\n",
      "        [ 0.0537,  0.0922, -0.0210,  ...,  0.0272,  0.0760, -0.1114]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[-0.0466,  0.0212,  0.0715,  ...,  0.1616, -0.0475, -0.0130],\n",
      "        [-0.0210,  0.0823,  0.1556,  ...,  0.1107, -0.0747,  0.1228],\n",
      "        [-0.1806,  0.0207,  0.1097,  ...,  0.1983, -0.1437,  0.1540],\n",
      "        ...,\n",
      "        [-0.0808,  0.1233,  0.1127,  ...,  0.1587, -0.1956,  0.0572],\n",
      "        [ 0.0495,  0.0148,  0.0600,  ..., -0.0240, -0.0281,  0.0961],\n",
      "        [ 0.0181,  0.1197,  0.0959,  ..., -0.0595, -0.1746, -0.0655]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([ 0.0878,  0.0129,  0.0854,  0.0293,  0.0387,  0.0772,  0.0803,  0.0093,\n",
      "         0.0687, -0.0019, -0.0088,  0.1291,  0.0655,  0.0078,  0.0342,  0.0868,\n",
      "         0.0923,  0.0899,  0.1208,  0.0624,  0.0999,  0.1431,  0.1273,  0.1706,\n",
      "         0.0041,  0.0748,  0.0157,  0.1057,  0.0182,  0.0425,  0.0615,  0.0389,\n",
      "        -0.0172,  0.1083,  0.0829,  0.0035,  0.0236, -0.0132,  0.0392,  0.0606,\n",
      "         0.0839,  0.0621,  0.0330, -0.0132,  0.0437,  0.0698,  0.0734,  0.0938,\n",
      "         0.0389,  0.0640,  0.0259,  0.1083,  0.0754,  0.0781,  0.0325,  0.0535,\n",
      "         0.0146,  0.1126,  0.0894,  0.0279,  0.0326,  0.1205, -0.0157, -0.0226,\n",
      "         0.1203,  0.0611,  0.0738,  0.1145,  0.1121,  0.0066, -0.0107, -0.0005,\n",
      "        -0.0182, -0.0098,  0.1251,  0.1602,  0.0449,  0.0436, -0.0260,  0.0401,\n",
      "         0.1021, -0.0040,  0.0995,  0.1262,  0.0720,  0.0811,  0.1287,  0.1541,\n",
      "        -0.0102,  0.1398,  0.0715,  0.0494,  0.1612,  0.0486,  0.1458,  0.0404,\n",
      "         0.0930, -0.0047,  0.0329,  0.0805, -0.0173, -0.0063,  0.1376,  0.1138,\n",
      "        -0.0059,  0.1289, -0.0283, -0.0025,  0.1287,  0.0410,  0.0259,  0.1214,\n",
      "         0.1273, -0.0119,  0.0594,  0.0116,  0.1612,  0.1649,  0.1207,  0.0499,\n",
      "         0.0238,  0.0452, -0.0018,  0.0487,  0.1631,  0.0331,  0.0408,  0.0580,\n",
      "        -0.0520,  0.0342,  0.0852,  0.0148, -0.0322,  0.0601, -0.0567, -0.0059,\n",
      "        -0.0503, -0.0966, -0.1035,  0.0335, -0.0384, -0.0619,  0.0849, -0.0101,\n",
      "        -0.0338,  0.0797,  0.0316,  0.0005, -0.1142,  0.0484,  0.0385, -0.0460,\n",
      "         0.0559,  0.0998, -0.0175, -0.0096,  0.0723,  0.0588, -0.0299, -0.0456,\n",
      "         0.0317,  0.0180,  0.0680,  0.0871, -0.0369,  0.0515,  0.0075, -0.0526,\n",
      "        -0.0856,  0.0219,  0.0047,  0.0487,  0.0011, -0.0085,  0.0289,  0.0864,\n",
      "         0.0600,  0.0918, -0.0021,  0.0081,  0.0617,  0.0411, -0.0904, -0.0593,\n",
      "         0.1133, -0.0689, -0.0958,  0.0197, -0.0512, -0.0433,  0.0287,  0.0341,\n",
      "        -0.0679,  0.0269, -0.0188,  0.0396,  0.0552,  0.0889, -0.0500,  0.0708,\n",
      "         0.0168, -0.0242, -0.0385, -0.0845,  0.0020, -0.0263,  0.0936,  0.0543,\n",
      "         0.0154,  0.0277,  0.0128, -0.0705, -0.0292, -0.0140, -0.0020, -0.0997,\n",
      "         0.0485,  0.1035,  0.0523, -0.0339,  0.0577,  0.0850, -0.0175,  0.0595,\n",
      "        -0.0300, -0.0008,  0.0265, -0.0714, -0.0505,  0.0749, -0.0052, -0.0931,\n",
      "         0.0709,  0.1322, -0.0870,  0.0174,  0.0034,  0.1293,  0.0824, -0.0061,\n",
      "        -0.0106, -0.0150,  0.0902,  0.0245, -0.0910,  0.0597, -0.0369,  0.0445,\n",
      "        -0.0262, -0.0222,  0.0249, -0.0132, -0.0144, -0.0409,  0.1206, -0.0173,\n",
      "        -0.0982,  0.0732, -0.0367, -0.0059, -0.0796,  0.0827, -0.0586,  0.0455,\n",
      "         0.1294,  0.0960,  0.0210, -0.0343, -0.1525,  0.0545, -0.1024,  0.0412,\n",
      "         0.0419,  0.0927,  0.0750, -0.0336,  0.0725, -0.0643,  0.0460,  0.0176,\n",
      "        -0.0586,  0.0358, -0.0795, -0.0278, -0.0834,  0.0152, -0.0419, -0.0806,\n",
      "         0.0505, -0.0028, -0.0210, -0.0415,  0.0986,  0.1202, -0.0097, -0.1085,\n",
      "        -0.0703,  0.0409, -0.1007,  0.1002, -0.1085, -0.1087, -0.1170,  0.0421,\n",
      "         0.0162,  0.0549, -0.1272,  0.0583,  0.0328,  0.0686, -0.0148, -0.1152,\n",
      "         0.0126, -0.0520,  0.0243, -0.0767, -0.0610,  0.0264, -0.0933,  0.0738,\n",
      "         0.0243,  0.0157,  0.0356,  0.0335, -0.0019,  0.1132,  0.0915, -0.0974,\n",
      "         0.0322, -0.0074, -0.1138,  0.0290,  0.1127, -0.1136,  0.1064,  0.0328,\n",
      "         0.0442, -0.1187, -0.0284, -0.0610,  0.0015,  0.0367,  0.1237, -0.1151,\n",
      "         0.0854, -0.0490,  0.0362,  0.0570, -0.0489, -0.0167,  0.0705,  0.0036,\n",
      "         0.0610,  0.0749, -0.1289,  0.1045,  0.1155,  0.0132,  0.1172,  0.0354,\n",
      "         0.0904, -0.0159,  0.0200,  0.0605, -0.0100,  0.0716,  0.0848, -0.0064,\n",
      "         0.0833,  0.0302,  0.0483, -0.0478, -0.0390,  0.0955,  0.0189, -0.0139,\n",
      "         0.1163, -0.1099,  0.0980, -0.0823, -0.0549,  0.0985, -0.1133, -0.0796,\n",
      "         0.0086,  0.0448,  0.0324,  0.1062,  0.0916, -0.0017,  0.1012,  0.0833,\n",
      "         0.0412,  0.0660,  0.0933,  0.0610,  0.0651, -0.0120,  0.0949,  0.1120,\n",
      "         0.0023,  0.0668,  0.0530,  0.0376, -0.0004,  0.0921,  0.1015,  0.1091,\n",
      "         0.0561,  0.0095,  0.0123,  0.1574,  0.1551, -0.0121,  0.0600,  0.0896,\n",
      "         0.0739,  0.1061,  0.0456,  0.0091,  0.0215,  0.0123,  0.1375,  0.0195,\n",
      "         0.0487,  0.1093,  0.0356,  0.1159,  0.1570,  0.0875,  0.1331,  0.0876,\n",
      "         0.0909,  0.0276, -0.0073,  0.1082,  0.0648,  0.1289, -0.0233,  0.0057,\n",
      "         0.0787,  0.0227, -0.0036,  0.1240,  0.1019,  0.1198,  0.0845, -0.0015,\n",
      "         0.0278,  0.1506,  0.1469,  0.1663,  0.0780,  0.1094,  0.1335,  0.0656,\n",
      "         0.0834,  0.0779,  0.1168,  0.1242,  0.1299,  0.1188,  0.0163,  0.0379,\n",
      "         0.1273,  0.0999, -0.0194,  0.1095,  0.0205,  0.1152,  0.0971,  0.0144,\n",
      "         0.1495,  0.0504,  0.0930,  0.0819,  0.1107,  0.0483,  0.0691,  0.0922,\n",
      "         0.0821,  0.0939,  0.0651,  0.0008,  0.1357,  0.0863,  0.1326,  0.1469,\n",
      "         0.0214,  0.1394,  0.0385,  0.0966,  0.0695,  0.1082,  0.1251,  0.0424,\n",
      "         0.0712,  0.0539,  0.0883,  0.1217, -0.0010,  0.0106, -0.0152,  0.0432,\n",
      "        -0.0203,  0.1312,  0.1313,  0.0808,  0.1294,  0.1445,  0.1278,  0.0992],\n",
      "       requires_grad=True)), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([-1.1565e-04,  1.0833e-01,  8.8366e-02,  1.2497e-01, -1.2670e-02,\n",
      "        -8.7999e-03,  2.2651e-02,  1.3753e-01,  4.2427e-02,  5.6860e-02,\n",
      "        -2.0497e-02, -3.5693e-02,  1.4146e-01,  7.5077e-02,  1.0697e-01,\n",
      "         6.6420e-02,  7.7597e-02,  1.1116e-01,  1.2331e-01,  1.3675e-01,\n",
      "         1.1904e-01,  1.7051e-02,  3.5636e-02,  1.0574e-01,  1.3339e-01,\n",
      "         2.6477e-02, -5.1641e-03,  1.0317e-01,  4.6062e-03,  1.7234e-02,\n",
      "         9.7176e-02,  8.4119e-02,  1.3509e-02,  1.3104e-01,  4.8438e-02,\n",
      "         1.2410e-01,  1.0836e-01, -2.5455e-02,  6.7404e-02,  3.9596e-02,\n",
      "         5.6397e-02,  9.9893e-02,  2.8497e-02,  1.3676e-01, -2.2925e-04,\n",
      "         6.0067e-02,  1.4325e-02,  9.4879e-02,  2.2664e-02,  9.3317e-02,\n",
      "         6.8244e-02,  1.3317e-01,  5.3404e-02,  1.2476e-01,  1.1944e-01,\n",
      "         7.1502e-02,  5.6444e-02, -6.6503e-03,  2.7901e-02,  8.0005e-02,\n",
      "         1.6508e-02,  6.0257e-02,  4.7074e-02, -1.4878e-02,  8.6704e-02,\n",
      "         2.9673e-02,  1.9295e-02,  1.3737e-01,  5.5004e-02, -1.6379e-02,\n",
      "         1.3868e-01,  2.4913e-02,  5.6228e-02,  4.2271e-02,  4.1798e-02,\n",
      "         6.9553e-02,  1.3944e-01,  6.2121e-02,  2.0130e-02, -1.7588e-02,\n",
      "         8.5891e-03,  1.3432e-01,  1.3147e-01,  1.4281e-01,  1.3374e-01,\n",
      "         5.7426e-02,  2.4115e-02,  4.5840e-02,  2.4433e-02,  1.0837e-01,\n",
      "         8.8037e-02,  1.1641e-01,  1.3853e-01, -1.9470e-02,  1.0503e-01,\n",
      "        -2.5028e-02,  1.0302e-01,  5.3300e-02,  8.9916e-02, -2.6847e-02,\n",
      "         8.5552e-02,  1.1136e-01,  9.6406e-03,  6.7156e-02, -6.7498e-04,\n",
      "         5.9077e-02,  6.2094e-02,  2.1492e-02,  6.7063e-02, -4.8688e-02,\n",
      "         3.8900e-03,  7.0350e-02,  1.0852e-01,  1.5429e-01,  1.0984e-01,\n",
      "         3.6327e-02,  5.4424e-03,  9.9438e-02,  9.0132e-02,  5.0045e-02,\n",
      "         1.4170e-01,  4.1671e-02,  6.1260e-02,  1.1790e-01,  1.1810e-01,\n",
      "         7.9616e-02,  7.1790e-02,  6.7675e-02, -8.6283e-02,  6.8516e-02,\n",
      "        -5.3273e-02,  6.9878e-02,  8.1853e-02, -6.4044e-02, -4.8218e-02,\n",
      "         1.5285e-02,  4.0488e-02,  7.8620e-03, -4.6757e-02,  2.9258e-02,\n",
      "         3.2429e-02,  2.4980e-02, -6.6878e-02,  7.0740e-02, -5.6709e-02,\n",
      "        -2.0884e-02, -2.4631e-02,  8.6864e-03, -1.2457e-01, -4.6145e-02,\n",
      "        -2.9062e-03,  5.8237e-02, -4.7370e-02, -2.4215e-03,  3.8341e-02,\n",
      "        -2.9282e-02,  9.6575e-02, -2.4217e-02, -1.4871e-02,  6.9228e-02,\n",
      "        -2.7418e-02, -8.0089e-04, -3.1973e-02,  9.2885e-02,  6.9129e-03,\n",
      "         8.2032e-02,  1.6984e-03, -5.0606e-02,  8.4749e-03, -1.8981e-02,\n",
      "         1.0977e-02,  3.3738e-03,  5.7281e-02, -2.0610e-02, -3.9551e-02,\n",
      "         2.8709e-02,  5.1663e-02, -5.9283e-02,  4.1992e-02,  9.8353e-02,\n",
      "        -3.9208e-03,  4.0096e-02, -1.7866e-02, -2.5014e-02,  8.4730e-03,\n",
      "         5.6774e-02,  1.1448e-03,  5.9905e-03, -3.9041e-02,  5.3975e-02,\n",
      "         8.3442e-02, -5.2822e-02, -5.7319e-02, -3.2014e-02,  2.2789e-02,\n",
      "        -1.3520e-02, -2.7158e-02, -8.1252e-03, -7.0572e-03, -5.6223e-02,\n",
      "        -5.6702e-02, -7.0661e-02, -7.6823e-02, -9.6083e-02,  6.2742e-02,\n",
      "        -1.7176e-02,  4.7508e-02,  9.2333e-02, -3.3275e-02, -5.7723e-02,\n",
      "        -5.4023e-02, -3.8226e-02,  1.9354e-02,  2.9819e-02,  1.0343e-01,\n",
      "        -3.3514e-02, -9.4338e-02,  2.5592e-02, -1.3888e-02,  8.6453e-02,\n",
      "         7.5890e-02,  2.0370e-02,  6.2062e-02, -2.2140e-02, -2.6429e-02,\n",
      "        -1.1421e-01,  6.5929e-03,  7.9998e-02, -4.2543e-02, -8.4572e-03,\n",
      "        -5.1104e-02, -8.1953e-02,  1.9084e-02, -7.5293e-04, -4.9796e-02,\n",
      "         2.2530e-02,  4.2752e-02, -1.5484e-02,  9.5678e-02,  4.0548e-02,\n",
      "        -4.9594e-02,  8.6999e-03,  8.3151e-02,  1.0756e-01,  4.0089e-02,\n",
      "        -2.9173e-02,  7.9387e-02, -1.8940e-02, -4.5467e-02, -2.7985e-02,\n",
      "         2.8259e-03,  9.6944e-02,  4.3649e-02,  4.1125e-02,  6.2440e-02,\n",
      "        -3.2509e-02, -1.0833e-01,  6.2893e-02,  1.1736e-01,  5.1026e-02,\n",
      "        -1.2048e-01,  1.3118e-01,  7.8674e-02,  3.9100e-02,  6.5150e-02,\n",
      "        -7.6700e-03,  7.7432e-02, -9.8808e-02, -1.7776e-02, -8.1184e-02,\n",
      "        -1.3438e-01,  6.1910e-02,  5.1710e-02,  6.7028e-02,  9.0829e-02,\n",
      "         4.7513e-03, -1.5990e-02, -6.5595e-02,  1.8914e-02,  1.5862e-02,\n",
      "        -6.1715e-02,  7.9070e-03, -3.2100e-02,  4.9410e-02, -8.4642e-02,\n",
      "         9.3459e-02,  4.6197e-02, -6.6956e-02, -7.9106e-02, -7.8356e-02,\n",
      "         7.5292e-02, -1.2156e-01,  6.6539e-02,  4.9896e-02,  1.5578e-02,\n",
      "        -2.5655e-03, -6.1318e-02,  4.3886e-02,  2.2834e-02,  1.1140e-01,\n",
      "         3.7922e-03, -5.6893e-02, -9.7217e-02, -8.4568e-02,  9.4874e-02,\n",
      "        -8.1927e-03, -1.2639e-02,  7.1680e-02,  1.2326e-01,  5.7699e-03,\n",
      "         2.4329e-02, -1.3864e-02,  1.2268e-01,  3.2545e-02, -1.1442e-02,\n",
      "         5.7975e-02,  2.4715e-02,  4.6913e-02, -9.8684e-02,  1.3931e-01,\n",
      "         1.1103e-01,  5.2242e-02,  3.3691e-02, -2.9096e-02,  7.6625e-04,\n",
      "         1.1237e-01,  1.1614e-01, -3.9813e-02, -3.0938e-02,  3.5111e-02,\n",
      "        -3.9295e-02, -5.5753e-02,  6.9359e-02, -4.4965e-02,  8.9290e-02,\n",
      "         1.8379e-02,  1.3680e-02, -3.4087e-02, -5.1178e-05, -4.6382e-02,\n",
      "        -1.1891e-01, -2.5987e-02,  5.0145e-02,  2.4711e-02,  2.6807e-02,\n",
      "        -7.8484e-02,  2.1268e-03,  9.8552e-02, -2.2615e-02, -6.1581e-02,\n",
      "         9.5721e-02,  8.3352e-02,  3.6132e-02,  4.5990e-02,  2.2527e-02,\n",
      "        -2.5420e-02,  5.2746e-02, -4.8346e-02,  5.0020e-02, -1.9668e-02,\n",
      "         1.0422e-01,  8.4619e-02, -8.7279e-02,  5.8536e-02,  1.4410e-01,\n",
      "         8.0547e-02,  1.0587e-01, -1.2348e-01, -3.8860e-02, -1.2356e-02,\n",
      "         6.9131e-02,  7.7699e-02,  4.7069e-02, -4.6125e-02, -8.2545e-02,\n",
      "         2.3829e-02,  1.9871e-02, -3.5686e-02,  1.0179e-01,  1.1346e-02,\n",
      "        -4.7005e-02,  3.4063e-03,  8.3116e-03, -1.3799e-02,  1.4874e-01,\n",
      "         1.4759e-01, -2.4698e-03,  2.8012e-02, -8.9450e-03,  6.9417e-02,\n",
      "         5.1322e-02,  2.1168e-02,  1.1990e-01, -1.2027e-03,  5.1307e-02,\n",
      "         1.2509e-01,  7.7278e-02,  2.9913e-02,  4.2448e-02,  3.8500e-02,\n",
      "         1.4927e-01,  9.0276e-02,  3.1133e-02,  2.5135e-03,  8.2393e-02,\n",
      "         1.4218e-01,  4.4809e-02,  1.2756e-01,  1.6853e-02,  5.0623e-02,\n",
      "         5.4940e-02,  1.5755e-02,  1.8109e-02,  8.7716e-03,  4.4272e-02,\n",
      "         1.1396e-01,  1.0155e-01,  1.7626e-02,  1.3546e-01,  5.4650e-02,\n",
      "         5.1175e-02,  6.0379e-02,  9.2938e-03,  2.5790e-02,  3.6038e-02,\n",
      "        -1.4128e-02, -1.7945e-03,  1.4089e-01,  1.2039e-01,  7.9887e-02,\n",
      "         1.0789e-01,  1.4344e-01,  2.1304e-02,  8.2454e-03,  1.5538e-02,\n",
      "         1.7838e-02,  1.2281e-01,  4.5318e-02,  9.4727e-02,  4.0923e-02,\n",
      "         4.1226e-02,  1.2874e-01,  2.4868e-02,  1.5937e-01,  8.8450e-03,\n",
      "         6.9839e-02, -2.9687e-02,  1.8448e-02,  5.2471e-03,  3.9939e-02,\n",
      "         6.4004e-02,  4.0136e-02, -2.4667e-02,  8.8143e-02,  8.3145e-02,\n",
      "         1.2437e-01,  5.7454e-02,  1.2459e-01,  1.0621e-01,  6.3560e-02,\n",
      "        -2.2804e-02,  1.1363e-01,  3.4282e-02,  1.2177e-01,  8.6972e-02,\n",
      "         6.5864e-02, -2.8015e-03,  1.4009e-02,  8.7381e-02,  1.2271e-01,\n",
      "        -1.4322e-02,  4.5210e-02,  1.5272e-01,  1.5975e-01, -1.0656e-02,\n",
      "         5.5051e-02,  1.1937e-01,  3.0430e-02,  1.1297e-01,  7.2534e-02,\n",
      "         1.2420e-01,  5.4465e-02,  1.0889e-01,  3.3567e-02,  9.1543e-02,\n",
      "         5.9164e-02,  6.3215e-02,  8.5549e-02,  6.5392e-02,  1.1345e-01,\n",
      "         1.2161e-01,  4.7174e-02,  7.8105e-02,  6.2689e-02, -2.8291e-02,\n",
      "         1.1810e-01,  1.1934e-01,  5.9399e-03,  8.2120e-02,  1.3984e-01,\n",
      "         3.8830e-02,  1.0339e-01,  1.5497e-01,  2.5257e-02, -2.9994e-02,\n",
      "        -1.6950e-04,  7.2165e-02, -2.0994e-04,  1.3435e-01,  7.9029e-02,\n",
      "        -4.1788e-03,  2.5763e-03], requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[-0.0822,  0.1012,  0.0621,  ..., -0.0973,  0.0801, -0.0605],\n",
      "        [-0.0080,  0.0367,  0.0292,  ..., -0.0548,  0.0719, -0.0307],\n",
      "        [-0.1254,  0.0739, -0.0926,  ..., -0.1260, -0.0714,  0.1163],\n",
      "        ...,\n",
      "        [ 0.1031,  0.0990,  0.0847,  ...,  0.0254,  0.0815, -0.0924],\n",
      "        [ 0.0238,  0.2295, -0.0713,  ..., -0.0272, -0.0855,  0.0168],\n",
      "        [-0.0134, -0.1585,  0.1709,  ...,  0.0922, -0.0757, -0.1593]],\n",
      "       requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([-0.0458,  0.0180,  0.0008, -0.0304,  0.0594,  0.0396,  0.0307,  0.0140,\n",
      "         0.0852,  0.0096,  0.0331,  0.0872,  0.0921, -0.0250, -0.0253, -0.0284,\n",
      "         0.0043,  0.0170,  0.0254, -0.0752], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_afr, valid_data_afr, test_data_afr), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_afr.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_afr.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_afr.vocab.stoi[train_afr.pad_token]\n",
    "tag_pad_idx = tagged_train_afr.vocab.stoi[tagged_train_afr.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTMTagger(in_dim, emb_dim, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate dutch params in dict\n",
    "transfer_param_dict = {}\n",
    "params = model.named_parameters()\n",
    "for name, param in params:\n",
    "    transfer_param_dict[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(transfer_param_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with and without fully connecected weights. No embedding weights however\n",
    "params2 = model2.named_parameters()\n",
    "for name, param in params2:\n",
    "    if(name == \"embedding.weight\" or name == \"fc.weight\" or name == \"fc.bias\"):\n",
    "    #if(name == \"embedding.weight\"):\n",
    "        continue\n",
    "    else:\n",
    "        param.data = transfer_param_dict[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Afrikaans POS Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 2.550 | Train Acc: 32.01%\n",
      "\t Val. Loss: 2.157 |  Val. Acc: 41.18%\n",
      "Epoch: 02 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 1.815 | Train Acc: 53.80%\n",
      "\t Val. Loss: 1.682 |  Val. Acc: 61.08%\n",
      "Epoch: 03 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 1.395 | Train Acc: 66.96%\n",
      "\t Val. Loss: 1.369 |  Val. Acc: 64.96%\n",
      "Epoch: 04 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 1.121 | Train Acc: 71.61%\n",
      "\t Val. Loss: 1.155 |  Val. Acc: 70.21%\n",
      "Epoch: 05 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.937 | Train Acc: 75.41%\n",
      "\t Val. Loss: 1.002 |  Val. Acc: 73.31%\n",
      "Epoch: 06 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.805 | Train Acc: 78.20%\n",
      "\t Val. Loss: 0.890 |  Val. Acc: 75.65%\n",
      "Epoch: 07 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.698 | Train Acc: 80.95%\n",
      "\t Val. Loss: 0.805 |  Val. Acc: 78.12%\n",
      "Epoch: 08 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.617 | Train Acc: 83.35%\n",
      "\t Val. Loss: 0.741 |  Val. Acc: 79.56%\n",
      "Epoch: 09 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.558 | Train Acc: 85.15%\n",
      "\t Val. Loss: 0.688 |  Val. Acc: 81.10%\n",
      "Epoch: 10 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.499 | Train Acc: 86.93%\n",
      "\t Val. Loss: 0.641 |  Val. Acc: 82.45%\n",
      "Epoch: 11 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.453 | Train Acc: 88.15%\n",
      "\t Val. Loss: 0.605 |  Val. Acc: 83.44%\n",
      "Epoch: 12 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.413 | Train Acc: 89.22%\n",
      "\t Val. Loss: 0.574 |  Val. Acc: 84.39%\n",
      "Epoch: 13 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.372 | Train Acc: 90.31%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 85.14%\n",
      "Epoch: 14 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.343 | Train Acc: 91.19%\n",
      "\t Val. Loss: 0.524 |  Val. Acc: 85.73%\n",
      "Epoch: 15 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.307 | Train Acc: 92.03%\n",
      "\t Val. Loss: 0.506 |  Val. Acc: 86.27%\n",
      "Epoch: 16 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.283 | Train Acc: 92.66%\n",
      "\t Val. Loss: 0.484 |  Val. Acc: 86.80%\n",
      "Epoch: 17 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.259 | Train Acc: 93.24%\n",
      "\t Val. Loss: 0.469 |  Val. Acc: 87.21%\n",
      "Epoch: 18 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.234 | Train Acc: 94.11%\n",
      "\t Val. Loss: 0.457 |  Val. Acc: 87.31%\n",
      "Epoch: 19 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.215 | Train Acc: 94.62%\n",
      "\t Val. Loss: 0.443 |  Val. Acc: 87.72%\n",
      "Epoch: 20 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.196 | Train Acc: 95.27%\n",
      "\t Val. Loss: 0.431 |  Val. Acc: 87.98%\n",
      "Epoch: 21 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.179 | Train Acc: 95.78%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 88.25%\n",
      "Epoch: 22 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.164 | Train Acc: 96.23%\n",
      "\t Val. Loss: 0.412 |  Val. Acc: 88.49%\n",
      "Epoch: 23 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.148 | Train Acc: 96.68%\n",
      "\t Val. Loss: 0.404 |  Val. Acc: 88.75%\n",
      "Epoch: 24 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.133 | Train Acc: 97.10%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 88.93%\n",
      "Epoch: 25 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.122 | Train Acc: 97.41%\n",
      "\t Val. Loss: 0.389 |  Val. Acc: 88.98%\n",
      "Epoch: 26 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.110 | Train Acc: 97.76%\n",
      "\t Val. Loss: 0.386 |  Val. Acc: 89.14%\n",
      "Epoch: 27 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.099 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.381 |  Val. Acc: 89.09%\n",
      "Epoch: 28 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.090 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.378 |  Val. Acc: 89.19%\n",
      "Epoch: 29 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.083 | Train Acc: 98.45%\n",
      "\t Val. Loss: 0.376 |  Val. Acc: 89.28%\n",
      "Epoch: 30 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.074 | Train Acc: 98.74%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 89.52%\n",
      "Epoch: 31 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.069 | Train Acc: 98.90%\n",
      "\t Val. Loss: 0.368 |  Val. Acc: 89.55%\n",
      "Epoch: 32 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.061 | Train Acc: 99.07%\n",
      "\t Val. Loss: 0.365 |  Val. Acc: 89.75%\n",
      "Epoch: 33 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.057 | Train Acc: 99.24%\n",
      "\t Val. Loss: 0.365 |  Val. Acc: 89.66%\n",
      "Epoch: 34 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.051 | Train Acc: 99.35%\n",
      "\t Val. Loss: 0.362 |  Val. Acc: 89.77%\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch: 35 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.046 | Train Acc: 99.48%\n",
      "\t Val. Loss: 0.363 |  Val. Acc: 89.79%\n",
      "Epoch: 36 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.042 | Train Acc: 99.58%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 89.73%\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch: 37 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.038 | Train Acc: 99.65%\n",
      "\t Val. Loss: 0.361 |  Val. Acc: 89.90%\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch: 38 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.035 | Train Acc: 99.71%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 89.91%\n",
      "Epoch: 39 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.033 | Train Acc: 99.77%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 89.91%\n",
      "Epoch: 40 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.030 | Train Acc: 99.81%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 90.03%\n",
      "Epoch: 41 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.84%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 89.98%\n",
      "Epoch: 42 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.025 | Train Acc: 99.87%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 90.06%\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch: 43 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.023 | Train Acc: 99.91%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 90.05%\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch: 44 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.91%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 89.98%\n",
      "Epoch: 45 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.020 | Train Acc: 99.91%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 90.10%\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch: 46 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.94%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 90.10%\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch: 47 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.96%\n",
      "\t Val. Loss: 0.359 |  Val. Acc: 90.19%\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping, reloading checkpoint model\n"
     ]
    }
   ],
   "source": [
    "from tools import EarlyStopping\n",
    "N_EPOCHS = 50\n",
    "patience = 3\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=False,filename='checkpt_af.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model2, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model2, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "    early_stopping(valid_loss, model2)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping, reloading checkpoint model\")\n",
    "        model2.load_state_dict(torch.load('checkpt_af.pt'))\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.295 |  Test Acc: 91.21%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(unk, 89.58), (88.55,90.57), (87.39, 90.37), (87.36, 90.77,), (88.30,91.21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
