{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll #pip3 install this if you don't have it\n",
    "import torchtext.data as tt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFRIKAANS_TRAIN = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "AFRIKAANS_DEV = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-dev.conllu'\n",
    "AFRIKAANS_TEST = 'UD_Afrikaans-AfriBooms/af_afribooms-ud-test.conllu'\n",
    "\n",
    "DUTCH_TRAIN = \"UD_Dutch-Alpino/nl_alpino-ud-train.conllu\"\n",
    "DUTCH_DEV = \"UD_Dutch-Alpino/nl_alpino-ud-dev.conllu\"\n",
    "DUTCH_TEST = \"UD_Dutch-Alpino/nl_alpino-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb\n",
    "def make_sentences(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    sentences = []\n",
    "    tagged_sentences = []\n",
    "    for each in data:\n",
    "        tagged_sentence=[]\n",
    "        sentence = []\n",
    "        for token in each:\n",
    "            if token.upos and token.form:\n",
    "                tagged_sentence.append(token.upos)\n",
    "                sentence.append(token.form.lower())\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        sentences.append(sentence)\n",
    "    return sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_afr_raw, tagged_train_afr_raw = make_sentences(AFRIKAANS_TRAIN)\n",
    "dev_afr_raw, tagged_dev_afr_raw = make_sentences(AFRIKAANS_DEV)\n",
    "test_afr_raw, tagged_test_afr_raw = make_sentences(AFRIKAANS_TEST)\n",
    "\n",
    "train_du_raw, tagged_train_du_raw = make_sentences(DUTCH_TRAIN)\n",
    "dev_du_raw, tagged_dev_du_raw = make_sentences(DUTCH_DEV)\n",
    "test_du_raw, tagged_test_du_raw = make_sentences(DUTCH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFRIKAANS\n",
      "Tagged sentences in train set:  1315\n",
      "Tagged words in train set: 33894\n",
      "========================================\n",
      "Tagged sentences in dev set:  194\n",
      "Tagged words in dev set: 5317\n",
      "========================================\n",
      "Tagged sentences in test set:  425\n",
      "Tagged words in test set: 10065\n",
      "****************************************\n",
      "Total sentences in dataset: 1934\n"
     ]
    }
   ],
   "source": [
    "print(\"AFRIKAANS\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_afr_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_afr_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_afr_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_afr_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_afr_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_afr_raw)+len(tagged_dev_afr_raw)+len(tagged_test_afr_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUTCH\n",
      "Tagged sentences in train set:  12264\n",
      "Tagged words in train set: 185999\n",
      "========================================\n",
      "Tagged sentences in dev set:  718\n",
      "Tagged words in dev set: 11549\n",
      "========================================\n",
      "Tagged sentences in test set:  596\n",
      "Tagged words in test set: 11053\n",
      "****************************************\n",
      "Total sentences in dataset: 13578\n"
     ]
    }
   ],
   "source": [
    "print(\"DUTCH\")\n",
    "print(\"Tagged sentences in train set: \", len(tagged_train_du_raw))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in tagged_train_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(tagged_dev_du_raw))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in tagged_dev_du_raw for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(tagged_test_du_raw))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in tagged_test_du_raw for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(tagged_train_du_raw)+len(tagged_dev_du_raw)+len(tagged_test_du_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/tringm/POSTagger_Pytorch/blob/master/src/util/nlp.py\n",
    "def build_tag_field(sentences_tokens):\n",
    "    token_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('tokens', token_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_tokens]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return token_field\n",
    "    \n",
    "def build_text_field(sentences_words):\n",
    "    text_field = tt.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "    fields = [('text', text_field)]\n",
    "    examples = [tt.Example.fromlist([t], fields) for t in sentences_words]\n",
    "    torch_dataset = tt.Dataset(examples, fields)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, AFR\n",
    "train_afr = build_text_field(train_afr_raw)\n",
    "dev_afr = build_text_field(dev_afr_raw)\n",
    "test_afr = build_text_field(test_afr_raw)\n",
    "tagged_train_afr = build_tag_field(tagged_train_afr_raw)\n",
    "tagged_dev_afr = build_tag_field(tagged_dev_afr_raw)\n",
    "tagged_test_afr = build_tag_field(tagged_test_afr_raw)\n",
    "\n",
    "fields_train_afr = ((\"text\", train_afr), (\"udtags\", tagged_train_afr))\n",
    "examples_train_afr = [tt.Example.fromlist(item, fields_train_afr) for item in zip(train_afr_raw, tagged_train_afr_raw)]\n",
    "fields_dev_afr = ((\"text\", dev_afr), (\"udtags\", tagged_dev_afr))\n",
    "examples_dev_afr = [tt.Example.fromlist(item, fields_dev_afr) for item in zip(dev_afr_raw, tagged_dev_afr_raw)]\n",
    "fields_test_afr = ((\"text\", test_afr), (\"udtags\", tagged_test_afr))\n",
    "examples_test_afr = [tt.Example.fromlist(item, fields_test_afr) for item in zip(test_afr_raw, tagged_test_afr_raw)]\n",
    "\n",
    "train_data_afr = tt.Dataset(examples_train_afr, fields_train_afr)\n",
    "valid_data_afr = tt.Dataset(examples_dev_afr, fields_dev_afr)\n",
    "test_data_afr = tt.Dataset(examples_test_afr, fields_test_afr)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "#train_afr.vocab = af_vec\n",
    "dev_afr.vocab = train_afr.vocab\n",
    "test_afr.vocab = train_afr.vocab\n",
    "tagged_train_afr.build_vocab(train_data_afr, valid_data_afr, test_data_afr)\n",
    "tagged_dev_afr.vocab = tagged_train_afr.vocab\n",
    "tagged_test_afr.vocab = tagged_train_afr.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fields, DUT\n",
    "train_du = build_text_field(train_du_raw)\n",
    "dev_du = build_text_field(dev_du_raw)\n",
    "test_du = build_text_field(test_du_raw)\n",
    "tagged_train_du = build_tag_field(tagged_train_du_raw)\n",
    "tagged_dev_du = build_tag_field(tagged_dev_du_raw)\n",
    "tagged_test_du = build_tag_field(tagged_test_du_raw)\n",
    "\n",
    "fields_train_du = ((\"text\", train_du), (\"udtags\", tagged_train_du))\n",
    "examples_train_du = [tt.Example.fromlist(item, fields_train_du) for item in zip(train_du_raw, tagged_train_du_raw)]\n",
    "fields_dev_du = ((\"text\", dev_du), (\"udtags\", tagged_dev_du))\n",
    "examples_dev_du = [tt.Example.fromlist(item, fields_dev_du) for item in zip(dev_du_raw, tagged_dev_du_raw)]\n",
    "fields_test_du = ((\"text\", test_du), (\"udtags\", tagged_test_du))\n",
    "examples_test_du = [tt.Example.fromlist(item, fields_test_du) for item in zip(test_du_raw, tagged_test_du_raw)]\n",
    "\n",
    "train_data_du = tt.Dataset(examples_train_du, fields_train_du)\n",
    "valid_data_du = tt.Dataset(examples_dev_du, fields_dev_du)\n",
    "test_data_du = tt.Dataset(examples_test_du, fields_test_du)\n",
    "\n",
    "#build vocabs so that they are shared between splits\n",
    "train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "#train_du.vocab = nl_vec\n",
    "dev_du.vocab = train_du.vocab\n",
    "test_du.vocab = train_du.vocab\n",
    "tagged_train_du.build_vocab(train_data_du, valid_data_du, test_data_du)\n",
    "tagged_dev_du.vocab = tagged_train_du.vocab\n",
    "tagged_test_du.vocab = tagged_train_du.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "#model\n",
    "batch_size=128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#needs to be tuple of dataset objects\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_du, valid_data_du, test_data_du), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without dropout first\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    #https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
    "        #fully connected layer\n",
    "        self.fc = nn.Linear((hidden_dim * 2 if bidirectional else hidden_dim), output_dim)\n",
    "     \n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_du.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_du.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_du.vocab.stoi[train_du.pad_token]\n",
    "tag_pad_idx = tagged_train_du.vocab.stoi[tagged_train_du.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMTagger(in_dim, emb_dim, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "        \n",
    "        optimizer.zero_grad()       \n",
    "        predictions = model(text)        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags) \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 34s\n",
      "\tTrain Loss: 1.442 | Train Acc: 60.19%\n",
      "\t Val. Loss: 0.874 |  Val. Acc: 71.62%\n",
      "Epoch: 02 | Epoch Time: 0m 34s\n",
      "\tTrain Loss: 0.639 | Train Acc: 79.46%\n",
      "\t Val. Loss: 0.666 |  Val. Acc: 78.68%\n",
      "Epoch: 03 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.479 | Train Acc: 84.53%\n",
      "\t Val. Loss: 0.572 |  Val. Acc: 81.58%\n",
      "Epoch: 04 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.385 | Train Acc: 87.71%\n",
      "\t Val. Loss: 0.529 |  Val. Acc: 82.94%\n",
      "Epoch: 05 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.317 | Train Acc: 90.02%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 83.99%\n",
      "Epoch: 06 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.264 | Train Acc: 91.78%\n",
      "\t Val. Loss: 0.468 |  Val. Acc: 85.02%\n",
      "Epoch: 07 | Epoch Time: 0m 50s\n",
      "\tTrain Loss: 0.221 | Train Acc: 93.26%\n",
      "\t Val. Loss: 0.468 |  Val. Acc: 85.50%\n",
      "Epoch: 08 | Epoch Time: 0m 43s\n",
      "\tTrain Loss: 0.185 | Train Acc: 94.52%\n",
      "\t Val. Loss: 0.455 |  Val. Acc: 86.09%\n",
      "Epoch: 09 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.153 | Train Acc: 95.58%\n",
      "\t Val. Loss: 0.454 |  Val. Acc: 86.54%\n",
      "Epoch: 10 | Epoch Time: 0m 42s\n",
      "\tTrain Loss: 0.128 | Train Acc: 96.44%\n",
      "\t Val. Loss: 0.466 |  Val. Acc: 86.66%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.560 |  Test Acc: 85.03%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embedding.weight', Parameter containing:\n",
      "tensor([[-1.5510,  0.2902,  1.4410,  ...,  1.4271,  1.8291, -1.3389],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.7725,  0.3579,  0.5922,  ...,  1.1290, -0.7719,  0.6018],\n",
      "        ...,\n",
      "        [-2.2577, -0.9497, -1.0379,  ..., -1.0123,  2.0057,  0.1253],\n",
      "        [ 0.5489,  0.5219,  1.7291,  ...,  0.6690,  0.8639,  0.4860],\n",
      "        [ 0.5249, -0.1805,  0.4028,  ...,  0.1350, -0.1444, -0.7871]],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.0849, -0.2312,  0.0095,  ..., -0.1276,  0.0494,  0.1115],\n",
      "        [-0.0313, -0.1294,  0.1299,  ..., -0.0491, -0.0870,  0.1488],\n",
      "        [ 0.0720, -0.1329,  0.1901,  ..., -0.0552,  0.0060,  0.0280],\n",
      "        ...,\n",
      "        [ 0.0497,  0.2230,  0.1945,  ...,  0.0044, -0.1269, -0.1645],\n",
      "        [ 0.1972,  0.0509, -0.1077,  ...,  0.2021, -0.0643,  0.1017],\n",
      "        [ 0.0501,  0.0225,  0.0436,  ...,  0.0241, -0.0547, -0.0349]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.0305, -0.0882, -0.0570,  ..., -0.0050,  0.0847, -0.0807],\n",
      "        [-0.0112,  0.0349, -0.0421,  ...,  0.0836,  0.0204,  0.0007],\n",
      "        [-0.0360, -0.0068, -0.1614,  ..., -0.0044,  0.1179, -0.0317],\n",
      "        ...,\n",
      "        [ 0.0958, -0.0206, -0.1302,  ...,  0.0143,  0.1197,  0.0056],\n",
      "        [ 0.1310, -0.1159, -0.1115,  ..., -0.0040,  0.1211,  0.1120],\n",
      "        [-0.0214, -0.0738, -0.0345,  ..., -0.1128,  0.0596,  0.0118]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0', Parameter containing:\n",
      "tensor([ 9.1079e-02,  1.3698e-01,  1.1821e-01, -1.3283e-02,  1.0278e-01,\n",
      "         3.0388e-03,  8.4111e-02,  2.3500e-02, -1.6160e-02,  9.3515e-02,\n",
      "         1.2021e-01,  4.5264e-02,  1.0982e-01,  8.3876e-02,  1.0428e-01,\n",
      "         8.5415e-02,  1.1364e-01,  1.5842e-01,  1.2065e-01,  3.1353e-02,\n",
      "        -3.3850e-02,  1.4917e-01,  9.4593e-02,  1.7544e-01,  8.4715e-02,\n",
      "         1.2395e-01,  8.2598e-02,  1.3421e-01,  1.7707e-02,  1.0589e-01,\n",
      "         5.3283e-02,  4.1504e-02,  1.4323e-01,  1.7805e-02,  4.7433e-02,\n",
      "         2.0863e-02,  1.0834e-01,  5.6761e-02,  1.1904e-01,  1.2421e-01,\n",
      "         1.1313e-01,  2.0446e-02,  1.4014e-01, -1.4307e-02,  1.4838e-01,\n",
      "         7.8455e-02,  1.3053e-02,  7.6890e-02,  1.3891e-01, -3.8434e-04,\n",
      "         2.6544e-02,  1.5758e-01,  6.0466e-02,  1.2717e-02,  8.4876e-03,\n",
      "         9.8500e-03,  1.5562e-03,  1.2729e-01,  3.2143e-02,  1.2489e-01,\n",
      "         4.1419e-02,  1.1271e-01,  6.7753e-02,  1.0201e-01,  6.5066e-02,\n",
      "         2.8869e-02,  1.3307e-01,  5.6159e-03,  9.3474e-02,  1.4789e-02,\n",
      "         1.1605e-01,  1.0796e-01,  1.4516e-01,  7.6328e-02,  7.3857e-02,\n",
      "         2.2761e-02,  1.1571e-01,  2.9557e-02,  3.0840e-02,  4.5642e-02,\n",
      "         9.2317e-02,  2.9897e-02, -1.7159e-02,  1.2089e-01, -1.4056e-02,\n",
      "        -1.9530e-02,  1.1352e-01,  2.9146e-02,  9.9435e-02,  7.0800e-02,\n",
      "         9.3202e-02,  4.5141e-02,  1.2294e-01,  7.5866e-02,  1.1703e-01,\n",
      "         2.4214e-02,  3.8849e-02,  1.6196e-01, -3.8605e-02,  8.4068e-02,\n",
      "         1.3799e-01,  2.7871e-02,  1.0501e-01,  5.9906e-02,  1.1237e-01,\n",
      "         4.1906e-02,  3.0715e-02,  7.3450e-02,  4.4724e-02, -1.0754e-02,\n",
      "         5.4400e-02,  5.6453e-03,  1.1912e-01,  3.9924e-02,  8.4523e-02,\n",
      "         1.3017e-02,  4.0264e-02,  6.7516e-03,  1.2994e-01,  1.4799e-01,\n",
      "         3.4168e-02,  8.3813e-02,  7.1193e-02,  7.5789e-02,  1.5558e-01,\n",
      "         1.2645e-01,  1.2295e-01,  1.3573e-01,  8.8628e-02,  7.6699e-03,\n",
      "         2.3354e-02, -7.7949e-02,  1.6250e-02,  3.0969e-02,  5.7637e-03,\n",
      "         3.6568e-02,  6.5241e-02,  7.3656e-02, -6.5216e-02, -6.7401e-02,\n",
      "        -2.9046e-02, -3.4020e-02, -2.4532e-02,  1.0483e-01, -1.7884e-02,\n",
      "        -4.0452e-02, -4.4198e-02,  3.8122e-02,  2.3535e-02, -1.2088e-02,\n",
      "         1.9268e-02,  7.2729e-02, -7.4760e-02,  2.4159e-02,  3.4234e-02,\n",
      "         1.8325e-02,  1.5818e-02,  1.1075e-01,  3.8945e-03,  6.2376e-02,\n",
      "         7.8668e-02, -6.6387e-02, -2.0050e-02, -2.7951e-02,  1.2788e-02,\n",
      "         8.1684e-02,  2.9925e-02, -2.5796e-02,  1.0934e-01, -4.5141e-02,\n",
      "         3.7278e-02,  6.2033e-02,  6.2470e-02, -4.2535e-02, -4.8692e-02,\n",
      "         3.7749e-03,  2.5794e-02,  9.6299e-02,  1.2552e-01,  6.5644e-02,\n",
      "        -4.5219e-02,  6.6852e-02,  9.4470e-02,  8.0273e-02, -3.8039e-02,\n",
      "         2.1003e-02,  5.1076e-02, -7.8196e-02, -1.6582e-03, -9.1239e-02,\n",
      "        -1.5249e-02,  6.9677e-02,  5.3562e-02,  7.0020e-02,  2.7503e-02,\n",
      "        -4.5752e-02, -6.0072e-02, -6.9313e-02,  6.1329e-02, -2.6668e-02,\n",
      "        -1.5373e-05,  6.0785e-02, -1.0459e-01,  9.9977e-02,  7.2213e-02,\n",
      "        -4.2799e-02, -5.1665e-02, -5.5061e-02, -3.3844e-02,  1.0809e-01,\n",
      "         5.3657e-02, -8.1632e-04, -5.7671e-02, -6.5700e-02,  5.8097e-02,\n",
      "         4.8410e-02,  8.6782e-02, -4.4678e-02,  5.8223e-02,  1.8989e-02,\n",
      "         2.5201e-02,  6.8281e-02,  1.8358e-02, -4.6242e-03, -1.0269e-01,\n",
      "        -5.1738e-02, -1.5979e-02,  9.7381e-02, -2.2476e-02,  4.7897e-02,\n",
      "        -6.9996e-03, -1.1149e-02, -3.1615e-02,  7.0098e-02,  1.2470e-01,\n",
      "         3.7274e-02, -3.7721e-02,  2.2854e-02,  3.5724e-02, -9.5814e-02,\n",
      "         4.4676e-02,  1.6481e-03,  3.7870e-02,  6.9001e-02,  1.9586e-02,\n",
      "         2.8701e-02, -4.8206e-02,  1.6888e-03, -1.0218e-01,  6.7099e-02,\n",
      "         6.6967e-02,  6.1916e-02, -1.2190e-02,  1.0705e-02,  2.7386e-02,\n",
      "         3.6277e-02,  5.6708e-02, -3.7662e-02,  3.4042e-02,  1.3223e-02,\n",
      "        -8.2011e-02, -4.9301e-03, -8.4511e-02,  9.3800e-02,  4.3176e-02,\n",
      "        -3.4912e-02, -8.1688e-02,  4.7817e-02,  6.6373e-04,  7.3792e-02,\n",
      "        -4.0241e-02,  6.8156e-02, -4.6903e-02, -8.4516e-02, -3.4312e-02,\n",
      "         1.1063e-01, -3.4257e-02, -7.0790e-02,  4.6443e-02, -6.0062e-03,\n",
      "         6.2764e-03, -9.3719e-03,  7.3929e-02,  6.7508e-02, -5.0565e-02,\n",
      "        -2.0216e-02,  6.5267e-02,  4.3704e-02, -7.4026e-02,  1.1887e-01,\n",
      "         1.2269e-02,  6.6745e-02, -6.4904e-03,  3.6850e-02, -1.5314e-02,\n",
      "        -3.3066e-02,  1.0465e-01,  9.7800e-02,  9.7070e-03,  4.9580e-02,\n",
      "        -1.1578e-01,  9.2859e-02,  7.7479e-02, -5.7270e-02, -6.3623e-02,\n",
      "        -1.2503e-02, -1.3469e-01,  7.4295e-02, -5.0419e-02, -4.9908e-02,\n",
      "         2.5503e-02,  1.3853e-02, -4.4205e-02, -5.2409e-02, -7.6717e-02,\n",
      "         9.6242e-02,  6.0541e-02,  5.7173e-02, -7.6410e-02, -7.3293e-02,\n",
      "         1.0540e-01, -1.1272e-01, -6.4624e-02, -1.4047e-01,  9.0588e-02,\n",
      "        -5.0857e-02,  4.1740e-02, -2.9241e-02, -1.2409e-01, -1.1656e-01,\n",
      "         3.9991e-02, -6.6696e-02, -8.3259e-02,  9.0223e-03, -9.2394e-02,\n",
      "        -5.9197e-02,  3.4479e-03,  8.7494e-02, -9.0278e-02,  4.4683e-02,\n",
      "        -3.3514e-02,  2.9738e-02,  7.2332e-02,  1.5004e-02, -3.3102e-02,\n",
      "        -1.1363e-01,  8.5582e-02, -7.2384e-02, -7.7974e-02,  2.3029e-02,\n",
      "        -6.1457e-02, -1.3297e-01,  1.2153e-02, -5.0784e-02, -4.8052e-02,\n",
      "         7.6824e-02,  2.6585e-03, -3.4442e-02,  7.8738e-02, -2.4047e-02,\n",
      "         1.4217e-02,  8.0622e-02, -6.9644e-02,  4.4186e-02,  8.2588e-02,\n",
      "        -3.7268e-02,  4.3877e-02, -7.3623e-03,  1.2294e-01,  3.7235e-02,\n",
      "         1.2496e-02, -9.1394e-02,  4.1262e-02, -1.0350e-02, -1.0279e-01,\n",
      "        -9.2879e-02,  1.0306e-01,  4.5390e-02, -1.5202e-02, -1.6300e-02,\n",
      "         2.2049e-02, -7.8441e-02,  1.0072e-01, -2.4265e-02,  4.8552e-02,\n",
      "         3.3133e-02,  5.9080e-02,  1.3995e-01,  1.1464e-01,  1.0523e-01,\n",
      "         9.9728e-02,  4.5635e-03,  7.3820e-02,  8.4771e-02,  2.8083e-02,\n",
      "         1.4026e-01,  1.1432e-01,  1.9402e-01,  6.2884e-02,  4.3665e-02,\n",
      "         1.1637e-02,  4.7033e-02,  1.7410e-01,  1.4494e-01,  1.2635e-01,\n",
      "        -8.3452e-03,  7.7801e-02,  7.6805e-02,  1.1937e-02,  2.3821e-02,\n",
      "         2.3044e-02,  9.7444e-02,  2.0470e-03,  3.3361e-02,  1.4972e-02,\n",
      "         1.4600e-01,  1.2400e-01,  4.3121e-02,  1.2362e-01,  7.2018e-02,\n",
      "         1.8056e-01,  2.0282e-02,  4.8465e-02,  1.8017e-01,  1.1316e-01,\n",
      "         1.2493e-01,  1.3888e-01,  9.0220e-02,  2.2659e-02,  1.0743e-01,\n",
      "         1.8645e-01,  1.5046e-01,  3.0506e-02,  5.2914e-02,  8.2590e-02,\n",
      "         1.4692e-01,  1.1696e-01,  4.1457e-03,  3.9702e-02, -1.4930e-02,\n",
      "         1.0809e-01,  3.1765e-02,  1.4180e-01,  1.0417e-01,  1.0984e-01,\n",
      "         1.4790e-01,  5.4815e-02,  1.4923e-01,  1.0015e-01,  1.2654e-01,\n",
      "         6.8880e-02,  6.1889e-02,  1.3768e-01,  5.4480e-02,  8.2444e-02,\n",
      "         1.4753e-01,  3.0570e-02, -6.1253e-03,  1.3257e-01,  1.5852e-01,\n",
      "         1.2122e-01,  7.7889e-02,  4.8801e-02,  8.9128e-03,  5.7013e-02,\n",
      "        -4.4454e-03,  1.0059e-01,  1.1473e-01,  7.5022e-02,  1.3028e-01,\n",
      "         9.1183e-02,  7.3043e-02, -2.0755e-03,  9.2384e-02,  1.4331e-01,\n",
      "         3.5990e-02,  1.0134e-01,  4.6916e-02,  1.1695e-01,  5.9511e-04,\n",
      "         1.3774e-01,  4.2815e-02,  9.2067e-02,  7.4598e-02,  4.8123e-02,\n",
      "         1.0812e-01,  1.6959e-01,  4.9576e-02,  7.4788e-02,  7.9392e-03,\n",
      "         1.5700e-01,  8.7397e-02, -8.3805e-03, -7.4775e-03,  1.5520e-01,\n",
      "         2.3148e-02,  6.1881e-02,  6.4537e-02,  8.5144e-02, -1.5563e-02,\n",
      "         8.7140e-03,  1.3413e-01,  3.2093e-02,  2.2691e-02,  4.6374e-02,\n",
      "         3.8223e-02,  2.4085e-02,  8.8936e-02,  7.0478e-02,  6.8582e-02,\n",
      "         1.4204e-02,  9.1206e-02], requires_grad=True)), ('lstm.bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.0353,  0.1083,  0.0676,  0.0663,  0.0037,  0.0779,  0.0886, -0.0131,\n",
      "         0.1220,  0.0279,  0.0385,  0.0845,  0.1441,  0.0631,  0.0816,  0.1412,\n",
      "         0.1128,  0.0922,  0.0148, -0.0007, -0.0330,  0.0092,  0.1010,  0.1011,\n",
      "         0.0835,  0.0446,  0.0670,  0.0686,  0.0388, -0.0477,  0.1177,  0.1221,\n",
      "         0.0901, -0.0081,  0.1378,  0.0982,  0.1637, -0.0028,  0.0464,  0.1476,\n",
      "         0.0762,  0.0649,  0.0479,  0.0899,  0.0296,  0.0863,  0.0452,  0.0315,\n",
      "         0.1145,  0.0123,  0.0734,  0.1119,  0.0168,  0.0744,  0.1281,  0.1088,\n",
      "         0.0070,  0.1359,  0.1120,  0.1661,  0.1135,  0.1476,  0.0764,  0.1665,\n",
      "         0.0229,  0.0400,  0.0320,  0.1016,  0.0670, -0.0034, -0.0078,  0.1114,\n",
      "         0.0786,  0.0990,  0.0193,  0.1018,  0.0593,  0.0879,  0.0376,  0.0899,\n",
      "         0.0034,  0.0109,  0.0495,  0.1472,  0.0487, -0.0085,  0.1569,  0.1393,\n",
      "         0.0774, -0.0151,  0.1011,  0.1050,  0.0894,  0.1409,  0.0958,  0.1286,\n",
      "         0.0196,  0.0443,  0.0692,  0.1040,  0.0339,  0.1439,  0.0961,  0.0736,\n",
      "         0.1474,  0.1348,  0.1566,  0.0482,  0.1077,  0.0756,  0.1099,  0.0100,\n",
      "        -0.0094, -0.0062,  0.1008,  0.0493,  0.0950,  0.0541,  0.1364,  0.0660,\n",
      "         0.1242,  0.0394,  0.0825,  0.1115,  0.0089,  0.1415,  0.0645,  0.1039,\n",
      "         0.0613, -0.1314, -0.0579,  0.0316, -0.1032, -0.0977, -0.0882, -0.0312,\n",
      "         0.0753,  0.0764, -0.0172, -0.0010,  0.0405,  0.0780, -0.0410,  0.0898,\n",
      "        -0.0863, -0.0882, -0.0324, -0.0522,  0.0358,  0.0417, -0.0501, -0.0287,\n",
      "        -0.1128,  0.0045, -0.0595, -0.0176,  0.0738,  0.0962, -0.0498,  0.0487,\n",
      "        -0.0259, -0.0116,  0.0123, -0.0070, -0.0871, -0.0554, -0.0513,  0.0824,\n",
      "         0.0626, -0.0413,  0.0358, -0.0248,  0.0653, -0.0487,  0.0975, -0.0076,\n",
      "        -0.0806,  0.0862, -0.0041,  0.0022,  0.0743, -0.0197,  0.0589,  0.0051,\n",
      "        -0.0046,  0.0632,  0.1019, -0.0873,  0.0982, -0.0417, -0.0576,  0.0016,\n",
      "         0.0595, -0.0040, -0.0722, -0.0345,  0.1055,  0.0356,  0.1270, -0.0423,\n",
      "        -0.0284, -0.0789,  0.0070,  0.0539,  0.0080, -0.0101,  0.0699,  0.0143,\n",
      "         0.0028,  0.0155, -0.1062, -0.0890, -0.1005, -0.0776,  0.0256, -0.0383,\n",
      "         0.1125, -0.0269,  0.0102,  0.0451,  0.0275,  0.0761,  0.0807,  0.0308,\n",
      "        -0.0374, -0.0670,  0.0378,  0.0706,  0.0834, -0.0179, -0.0304,  0.0488,\n",
      "         0.0280, -0.0206,  0.0173,  0.0849, -0.1296,  0.0580, -0.0143, -0.0671,\n",
      "         0.0338, -0.0731,  0.0898, -0.0357,  0.0110, -0.0094,  0.1197,  0.1132,\n",
      "        -0.0991,  0.1149,  0.0163,  0.0132,  0.0182, -0.1082,  0.0518, -0.0614,\n",
      "         0.0307, -0.0788, -0.0150, -0.0236,  0.0526, -0.0366, -0.0337,  0.0569,\n",
      "         0.1006, -0.0290, -0.0627,  0.0449,  0.0182,  0.0210, -0.1048,  0.0909,\n",
      "        -0.0448, -0.0200,  0.0507,  0.0480, -0.0946, -0.0223,  0.0045, -0.1179,\n",
      "        -0.0613, -0.0557,  0.0715,  0.0652, -0.0975,  0.0824,  0.0045,  0.0754,\n",
      "        -0.0454, -0.0109,  0.0075, -0.0186,  0.0319,  0.0116, -0.0423, -0.0133,\n",
      "         0.1072,  0.0816,  0.0225,  0.0339, -0.0742,  0.0931,  0.1091, -0.1078,\n",
      "        -0.0469, -0.0978, -0.0071,  0.0828, -0.0737, -0.0528,  0.0134, -0.1226,\n",
      "        -0.1025, -0.0881, -0.1318, -0.0142,  0.0874,  0.0938, -0.1112, -0.0497,\n",
      "         0.0962, -0.0092, -0.0514, -0.1072, -0.0101,  0.0513,  0.0555,  0.0762,\n",
      "        -0.1322,  0.0096,  0.0386, -0.1656, -0.0975,  0.0199, -0.0775, -0.0987,\n",
      "        -0.1162,  0.0376, -0.0950,  0.0804,  0.0066, -0.0287,  0.0803,  0.0208,\n",
      "         0.0688, -0.1186,  0.0821, -0.0199,  0.0500,  0.0751, -0.0594, -0.0808,\n",
      "        -0.1141,  0.0400, -0.0560,  0.1193,  0.0559, -0.0974, -0.0054, -0.0061,\n",
      "         0.0822,  0.0969, -0.0917, -0.0561,  0.0788,  0.1054,  0.0906, -0.0277,\n",
      "         0.0135, -0.0647, -0.0305, -0.0550,  0.0951, -0.0712, -0.0891, -0.1121,\n",
      "         0.0322, -0.0202, -0.1281, -0.1060, -0.0181, -0.0784,  0.0930,  0.0281,\n",
      "         0.0965,  0.1553,  0.0355,  0.0051,  0.0708,  0.0591,  0.0202,  0.0187,\n",
      "         0.1240,  0.0336,  0.1475,  0.1298,  0.0179,  0.1065,  0.1089,  0.1655,\n",
      "         0.0777,  0.1524,  0.1254,  0.1338,  0.0972,  0.1256,  0.0403,  0.0437,\n",
      "         0.1438,  0.0883,  0.0951,  0.0672,  0.0107,  0.0018, -0.0078,  0.1328,\n",
      "         0.0172,  0.0833,  0.0254,  0.1304,  0.0163,  0.1367,  0.0600,  0.0299,\n",
      "         0.0548,  0.1543,  0.0199,  0.1654,  0.0269,  0.1082,  0.1147,  0.1446,\n",
      "         0.0475,  0.1283,  0.0543,  0.0564,  0.0906,  0.1209,  0.0148,  0.1119,\n",
      "         0.1337,  0.0289,  0.0417,  0.0123, -0.0442,  0.0532, -0.0170,  0.0336,\n",
      "         0.0963,  0.0997,  0.0240,  0.1414,  0.0268,  0.1201,  0.0008,  0.0097,\n",
      "        -0.0147,  0.0037,  0.0521,  0.1649, -0.0015,  0.0487,  0.1222,  0.0473,\n",
      "        -0.0064,  0.0763,  0.0077,  0.0397,  0.1265,  0.1538,  0.0042,  0.1685,\n",
      "         0.0914,  0.0008,  0.0282,  0.1511,  0.1447, -0.0102,  0.1078,  0.0162,\n",
      "         0.1480,  0.0227,  0.0916,  0.0507,  0.1472,  0.0826,  0.1244,  0.0436,\n",
      "         0.1784,  0.0926,  0.0699,  0.1020,  0.0633,  0.0248,  0.0447, -0.0099,\n",
      "         0.1073, -0.0338,  0.0371,  0.0962,  0.0951,  0.0086,  0.0758,  0.0913,\n",
      "         0.0082,  0.0703,  0.1559,  0.1462,  0.0513,  0.1545,  0.0091,  0.1238],\n",
      "       requires_grad=True)), ('lstm.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.1064,  0.0452, -0.1203,  ..., -0.0126, -0.0316, -0.0482],\n",
      "        [-0.1140, -0.0126,  0.0108,  ..., -0.0597, -0.0573,  0.0870],\n",
      "        [ 0.0867,  0.0716, -0.0993,  ...,  0.0134,  0.0868, -0.0201],\n",
      "        ...,\n",
      "        [-0.0795, -0.0757,  0.0132,  ..., -0.0118,  0.0173,  0.0454],\n",
      "        [ 0.1009, -0.1163,  0.0005,  ...,  0.0262, -0.1045, -0.0847],\n",
      "        [-0.0381, -0.0645, -0.0027,  ..., -0.0391, -0.0789,  0.0942]],\n",
      "       requires_grad=True)), ('lstm.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[-0.0474, -0.0784, -0.0622,  ..., -0.0100, -0.0600, -0.0413],\n",
      "        [-0.0596, -0.0752, -0.1728,  ..., -0.0636, -0.1876, -0.0538],\n",
      "        [-0.0818, -0.1139, -0.1020,  ..., -0.0402, -0.0712, -0.0626],\n",
      "        ...,\n",
      "        [-0.0777,  0.1296, -0.0092,  ..., -0.0594, -0.0580,  0.0136],\n",
      "        [-0.0326,  0.0105, -0.0996,  ..., -0.1628, -0.0028, -0.1084],\n",
      "        [-0.1639, -0.1777, -0.0056,  ..., -0.0547, -0.1475,  0.0666]],\n",
      "       requires_grad=True)), ('lstm.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([ 1.0674e-01,  1.3498e-01, -2.5826e-03, -3.7190e-03,  5.0157e-02,\n",
      "         1.0996e-01, -3.8569e-03,  2.2223e-02, -6.4964e-03,  2.5369e-02,\n",
      "         1.2282e-01,  8.8695e-02,  9.4917e-02,  3.1530e-02,  1.4053e-01,\n",
      "         1.1534e-01,  1.3766e-02,  1.0598e-01,  3.8023e-02,  8.6301e-02,\n",
      "         5.7812e-02,  4.8338e-04,  5.3692e-02,  1.4794e-02,  8.8766e-02,\n",
      "         1.2976e-01,  2.2548e-02,  1.4439e-01,  2.7408e-02,  4.8276e-02,\n",
      "         1.3410e-02,  1.3930e-01,  3.6191e-02,  8.9971e-02,  2.0783e-02,\n",
      "         1.4423e-01,  1.3386e-02,  7.2039e-02,  1.1971e-01,  6.3223e-02,\n",
      "         1.0306e-01,  1.3719e-01,  9.6843e-02,  2.6856e-02,  1.1217e-01,\n",
      "        -1.7047e-02,  8.2899e-02,  1.2732e-01,  8.9865e-02,  8.5463e-02,\n",
      "         7.8493e-02,  8.5872e-02,  1.1659e-01,  1.0844e-01,  6.5652e-02,\n",
      "         1.3656e-01,  8.5610e-02,  1.4411e-01,  1.1624e-01,  1.2206e-01,\n",
      "        -6.1644e-03,  1.2271e-01,  3.4544e-02,  1.1499e-01,  1.3621e-02,\n",
      "         1.2201e-01,  5.4344e-03,  1.0612e-01,  8.6188e-02,  3.5797e-02,\n",
      "         1.2781e-01, -1.6437e-02,  8.0761e-02,  1.2118e-01,  2.0540e-02,\n",
      "         7.8524e-02,  3.7753e-02,  1.4645e-01,  1.4282e-01,  1.1686e-01,\n",
      "         7.7046e-03, -6.8004e-03,  1.2903e-01,  1.1250e-01,  1.5347e-01,\n",
      "         1.0713e-01,  3.7866e-03,  9.7910e-02,  9.4112e-02, -1.5245e-02,\n",
      "         1.0620e-01,  8.7539e-02,  1.6039e-02,  1.4423e-01,  6.4919e-02,\n",
      "         1.1718e-01,  8.8001e-02,  7.1441e-02,  7.3605e-02, -1.7731e-02,\n",
      "         1.1418e-01,  6.2896e-02,  1.4111e-01,  7.3633e-02,  1.6013e-02,\n",
      "         8.8573e-02, -3.0812e-02,  1.1500e-02,  4.1899e-02,  1.0040e-01,\n",
      "         1.4444e-01,  3.2310e-02,  1.8510e-02, -3.4496e-02,  5.1311e-02,\n",
      "         5.1221e-02,  6.4381e-02,  1.5204e-01,  6.5638e-02,  4.1055e-02,\n",
      "         1.3313e-02,  1.0765e-01,  9.0341e-02,  5.4517e-02,  1.4553e-01,\n",
      "         7.1732e-02, -2.6707e-03,  1.4648e-01,  4.1341e-02, -8.3422e-02,\n",
      "        -1.8583e-02, -1.9546e-02, -1.2763e-01, -9.2860e-02, -7.1206e-03,\n",
      "         4.7450e-02,  1.0009e-01, -7.1536e-03, -7.7556e-02,  4.6639e-02,\n",
      "        -8.4477e-05,  6.7570e-02, -1.5368e-03,  7.8353e-02, -3.0446e-02,\n",
      "         6.8300e-02,  1.9684e-02,  2.2370e-02,  1.0511e-01,  3.0517e-02,\n",
      "        -3.3634e-02,  4.0036e-02,  4.0286e-05,  6.4281e-02,  4.2721e-02,\n",
      "         4.9889e-02, -5.7832e-02,  3.2447e-02, -7.8105e-02, -8.3363e-03,\n",
      "         1.8428e-02,  3.9755e-02, -3.1441e-02, -3.8352e-03,  9.7838e-02,\n",
      "         4.8541e-02, -1.0176e-01, -7.8421e-03,  1.1354e-01, -1.0220e-02,\n",
      "        -4.4288e-02, -3.0168e-02,  9.9733e-02, -2.1276e-02,  8.1267e-02,\n",
      "        -1.5110e-03, -3.8359e-02,  9.8586e-02, -1.0521e-01, -7.5896e-02,\n",
      "         3.2047e-02,  7.1684e-02,  8.0040e-02, -3.9213e-02,  2.0531e-03,\n",
      "        -1.3232e-02,  7.9033e-02, -6.4809e-02,  3.7357e-02, -6.8900e-02,\n",
      "        -3.7507e-02, -9.3972e-02, -5.9358e-02,  9.4362e-02, -3.8605e-02,\n",
      "         1.0499e-01, -5.0688e-02, -9.2134e-02,  1.2668e-02, -2.5128e-02,\n",
      "        -3.9656e-02,  1.9799e-02,  5.3658e-02, -9.5195e-02,  4.9689e-02,\n",
      "         9.8667e-02, -8.3778e-02,  3.7435e-02,  5.3736e-02,  1.0989e-01,\n",
      "         3.4148e-02,  2.5654e-02, -9.7011e-02, -6.7816e-03,  1.6456e-02,\n",
      "         5.8261e-02,  2.6395e-02, -2.3123e-02, -5.4080e-02,  8.3824e-02,\n",
      "         6.2818e-02,  5.1714e-02,  6.1042e-02, -4.1676e-04,  5.6101e-02,\n",
      "         8.4082e-02, -2.7137e-02, -1.2604e-01,  6.2109e-03, -4.2042e-02,\n",
      "        -1.0009e-01, -6.7436e-03,  9.5418e-02, -8.3003e-02,  3.6440e-02,\n",
      "         1.8489e-02, -6.0950e-02, -7.9998e-02, -3.5407e-02,  1.0719e-01,\n",
      "         1.1287e-01, -2.6303e-02,  7.1583e-02,  9.5476e-02,  4.8664e-02,\n",
      "        -3.9295e-02,  1.2213e-02,  1.0850e-01,  4.7509e-03, -7.6626e-02,\n",
      "        -9.7437e-02, -2.8179e-02,  5.6245e-02, -3.5331e-02,  5.1032e-02,\n",
      "        -7.3770e-02, -7.3200e-02, -1.8033e-02, -1.1316e-01,  6.0555e-02,\n",
      "        -1.0356e-01, -1.3134e-02, -2.4669e-02,  1.0819e-01, -7.8115e-02,\n",
      "         1.0186e-01,  8.2744e-02, -1.0797e-01,  6.6440e-02, -6.7188e-02,\n",
      "         5.7882e-02,  9.5874e-02,  9.6544e-02,  6.1348e-02, -1.3226e-02,\n",
      "         7.1684e-02,  1.1586e-01,  6.6316e-02,  5.6773e-02, -2.9510e-02,\n",
      "        -8.3849e-03,  1.6248e-02, -1.0954e-02, -4.2397e-02,  6.3016e-02,\n",
      "         8.8199e-02, -1.0177e-02,  2.9469e-02,  9.7106e-02,  8.1075e-02,\n",
      "         3.4720e-02,  1.1232e-01,  1.1248e-01,  3.6885e-02,  1.1002e-01,\n",
      "         1.2202e-01,  7.4543e-02,  6.1736e-02,  3.8948e-02,  8.0567e-02,\n",
      "         6.6414e-03,  7.5944e-02,  8.8417e-02,  1.2856e-01,  4.8294e-02,\n",
      "         9.8004e-02,  2.0603e-02, -2.3347e-03, -6.1742e-03, -7.2070e-02,\n",
      "         1.5576e-02,  4.9535e-02,  6.4319e-02, -1.0456e-01,  3.8931e-02,\n",
      "         3.2348e-02,  4.5458e-02,  7.4266e-02, -6.1975e-02, -6.9335e-02,\n",
      "         9.3610e-02, -9.1871e-02,  8.4932e-02, -1.0923e-01, -1.2518e-01,\n",
      "         3.4731e-02, -3.6776e-03,  1.8690e-02, -6.4048e-02,  3.4194e-02,\n",
      "        -4.2357e-02, -4.8305e-03, -7.0598e-02, -4.3048e-02,  1.0335e-02,\n",
      "         1.0935e-01,  6.6810e-02, -3.3670e-03, -6.5499e-02, -8.4175e-02,\n",
      "         4.7076e-02,  4.5232e-02, -7.6125e-02,  6.6207e-02,  4.5666e-02,\n",
      "        -3.9815e-02,  4.5220e-02,  5.9302e-02, -6.4727e-02, -5.1181e-02,\n",
      "        -8.3249e-02,  1.2052e-01, -5.1466e-02, -1.0689e-01,  7.2470e-02,\n",
      "        -7.6943e-03, -1.0115e-01, -6.2328e-02,  7.7260e-02,  4.8015e-02,\n",
      "        -7.0184e-02,  1.1240e-01, -6.6138e-02, -5.5463e-02,  2.8180e-02,\n",
      "         4.7670e-02, -9.2054e-02,  7.3005e-02, -6.6946e-02,  4.7236e-02,\n",
      "         4.0484e-03, -3.4610e-02, -8.6345e-02,  4.1719e-02,  8.2085e-02,\n",
      "         2.8012e-02,  8.1352e-03,  9.8904e-04,  6.7729e-02, -2.5930e-03,\n",
      "         5.9426e-02, -1.0803e-01,  1.6988e-02, -9.1059e-02, -4.0433e-02,\n",
      "         4.7886e-02,  7.6635e-02, -1.5753e-02,  9.4232e-02,  3.2488e-02,\n",
      "         2.4125e-02, -6.9854e-03,  7.7725e-02,  8.5761e-02,  4.0997e-02,\n",
      "         9.7733e-02, -3.1523e-03,  7.9550e-04,  5.8425e-02,  1.4118e-01,\n",
      "        -5.6646e-03,  8.1928e-02, -4.8331e-03,  6.5500e-02,  2.2122e-02,\n",
      "         9.3662e-02,  1.2415e-01,  3.2020e-02,  9.1817e-02,  6.7466e-03,\n",
      "         1.3070e-01,  1.0155e-01,  1.0604e-01,  7.6839e-02,  8.4144e-02,\n",
      "         4.1448e-02,  4.7135e-03,  2.6704e-03,  7.2859e-02, -2.6403e-02,\n",
      "         1.4609e-01,  3.6275e-02,  1.5261e-01,  8.4442e-02,  1.0518e-01,\n",
      "         3.0872e-03,  2.4711e-02,  9.5255e-02,  7.0032e-02,  3.0592e-02,\n",
      "         1.3943e-01,  1.1540e-01,  1.1092e-01,  8.0770e-03,  6.7984e-02,\n",
      "         9.3689e-02, -2.4000e-02,  8.1607e-02,  1.7274e-02,  9.0672e-02,\n",
      "         1.0042e-01,  1.2706e-01, -1.0954e-02,  9.2826e-03,  1.3835e-02,\n",
      "         7.1995e-02,  1.0754e-01,  6.0382e-02,  2.7247e-02,  1.1060e-01,\n",
      "         1.2716e-01,  1.8098e-02,  1.5708e-01,  2.1999e-02,  9.7066e-02,\n",
      "         2.4402e-02, -1.1054e-02, -2.7838e-02,  7.4276e-02,  3.2248e-02,\n",
      "         4.0199e-02,  1.2206e-01,  5.1188e-02,  9.5915e-02,  4.3079e-02,\n",
      "         5.1115e-02,  7.5914e-02,  9.4957e-02,  1.4006e-01,  4.7900e-02,\n",
      "         4.4720e-02,  7.6108e-02, -7.0982e-03,  4.4033e-02,  1.0941e-01,\n",
      "         5.2910e-03,  5.8902e-02,  1.2785e-01,  1.0080e-01,  8.2010e-02,\n",
      "         7.7002e-02, -1.2638e-03,  1.1186e-01,  1.2587e-01,  5.6086e-02,\n",
      "         9.7056e-02,  1.4321e-01,  1.2030e-01,  8.3311e-02,  1.1753e-01,\n",
      "        -1.5304e-02,  9.6010e-02,  3.1802e-02,  6.2701e-02,  1.0143e-01,\n",
      "         9.3168e-02, -1.6750e-02,  9.0798e-02,  1.3199e-01,  6.6923e-02,\n",
      "        -9.9954e-03,  8.6812e-02,  1.2548e-01,  3.6930e-02, -2.3898e-02,\n",
      "         7.5709e-02,  1.5927e-01, -1.9773e-02,  1.3724e-01,  7.8646e-02,\n",
      "         1.3201e-01,  1.5269e-01], requires_grad=True)), ('lstm.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([ 1.0567e-01,  5.8571e-02, -1.2516e-02,  8.9943e-02,  1.3906e-01,\n",
      "         9.7560e-02,  3.3470e-02,  5.3542e-02,  1.1644e-01,  7.8913e-02,\n",
      "         8.4924e-02, -4.3364e-04,  1.0584e-01, -1.4855e-02,  5.3395e-02,\n",
      "         1.3767e-01,  3.2963e-02,  7.5778e-02,  5.5475e-02, -2.5808e-02,\n",
      "         1.3843e-02,  1.1251e-01,  9.8245e-02,  7.5841e-02,  4.9002e-02,\n",
      "         1.8061e-02, -3.5976e-02,  1.1635e-01, -3.7046e-02,  5.9902e-02,\n",
      "         9.8350e-02,  1.2931e-01,  3.9879e-02,  8.9745e-02,  3.6998e-02,\n",
      "         3.1786e-02,  7.0384e-04,  1.4996e-01, -2.0603e-02,  8.1749e-02,\n",
      "         6.7018e-02,  1.4214e-01, -1.1319e-02, -1.1243e-02, -3.5197e-02,\n",
      "        -1.1595e-02,  4.7932e-02,  6.5103e-02,  3.3506e-02,  9.5964e-02,\n",
      "         1.6006e-01,  2.0355e-02,  6.6758e-02,  1.5100e-01,  3.3310e-02,\n",
      "         5.3266e-02,  3.7654e-02,  1.0990e-02,  8.2752e-02,  4.6072e-02,\n",
      "         2.2776e-02,  6.7209e-02,  1.1277e-01,  9.6585e-02,  5.2615e-04,\n",
      "         5.7299e-02,  5.5775e-02,  1.1416e-01,  7.2516e-02,  1.3592e-01,\n",
      "         3.0831e-03,  5.8526e-02,  1.1664e-01, -2.8434e-02,  1.2191e-01,\n",
      "         1.0616e-01, -1.1584e-03, -1.8236e-03,  7.9100e-02,  6.5558e-02,\n",
      "         7.0462e-02, -8.5229e-03, -8.0663e-04,  1.2003e-01,  3.2184e-02,\n",
      "         1.0172e-01, -4.1971e-02,  1.3578e-02, -3.1945e-02,  3.2179e-02,\n",
      "         7.1256e-02, -1.1103e-02,  3.5453e-02,  1.9352e-02, -2.4865e-02,\n",
      "         9.6366e-02,  5.9248e-02,  1.1545e-01,  1.3442e-02, -2.3438e-02,\n",
      "         2.5322e-02,  2.8595e-02,  3.8461e-02,  1.1179e-01, -1.9859e-02,\n",
      "         2.3502e-02,  4.2379e-02,  2.4914e-02,  5.2267e-02,  6.0892e-02,\n",
      "         1.3900e-01, -4.8679e-02,  4.8188e-02, -2.0669e-02,  1.1834e-01,\n",
      "         1.1084e-01,  6.4488e-02,  8.8412e-02,  1.1164e-01,  1.0702e-01,\n",
      "         1.1614e-01,  1.1254e-01, -3.6191e-02,  9.9227e-02,  1.0676e-01,\n",
      "        -1.5517e-02,  7.2316e-02,  5.4860e-02, -2.7547e-02,  2.8008e-02,\n",
      "         6.6401e-02,  5.9680e-02, -5.9741e-02, -9.6002e-02, -4.8012e-02,\n",
      "         6.2246e-02,  6.6529e-02, -7.0974e-02, -9.2393e-02, -5.7515e-02,\n",
      "         5.8843e-02,  1.1516e-01,  6.4175e-02,  6.5241e-02, -9.9937e-02,\n",
      "         5.0187e-02, -5.5797e-03,  6.3627e-03,  4.5904e-02, -4.2969e-03,\n",
      "        -1.0412e-02, -9.2860e-02, -9.6558e-02,  3.8132e-03, -2.6092e-02,\n",
      "        -6.6099e-02, -7.6515e-02,  8.0022e-02, -5.9206e-02, -4.3597e-02,\n",
      "         8.2440e-02, -6.9185e-02, -3.6318e-02, -4.1533e-02,  8.9484e-02,\n",
      "        -9.2938e-03, -4.9113e-02,  1.0377e-01,  1.0039e-01,  3.0655e-03,\n",
      "        -2.5249e-04,  6.2680e-02, -4.0392e-02,  5.9365e-02, -5.9286e-02,\n",
      "        -6.2433e-02,  3.7577e-02,  8.8933e-02,  5.1782e-02,  2.2069e-03,\n",
      "        -6.4657e-03,  6.9780e-02,  6.8080e-02, -3.5781e-02,  7.2096e-03,\n",
      "        -1.1955e-02, -1.2552e-03,  1.5370e-02, -7.4176e-02, -1.2331e-02,\n",
      "        -9.1192e-02, -9.2925e-02,  2.8734e-02, -3.8095e-03,  7.9155e-02,\n",
      "         2.2625e-02,  3.3303e-02, -6.0773e-02, -1.7858e-02,  3.9408e-02,\n",
      "         1.0451e-02, -1.7233e-02,  3.7181e-02, -3.1054e-02,  1.0459e-01,\n",
      "         1.3004e-04, -5.9098e-02,  8.3399e-02,  5.3731e-02,  9.7931e-02,\n",
      "         4.8213e-02,  1.5878e-02, -2.2217e-02, -4.9063e-02,  8.4779e-02,\n",
      "         1.3103e-02,  3.1130e-02,  7.0149e-03,  1.7688e-02,  9.1477e-02,\n",
      "        -2.2116e-02,  7.5242e-02,  1.1572e-01,  6.2526e-02,  3.1747e-02,\n",
      "         7.1932e-02,  4.2150e-02, -4.8181e-02, -2.4960e-02,  2.4250e-02,\n",
      "        -2.4195e-02,  7.8547e-02, -6.2197e-02,  6.5173e-02,  5.5188e-02,\n",
      "        -5.9931e-02,  4.5061e-02, -7.4295e-02,  8.2294e-02, -1.1691e-02,\n",
      "         4.8159e-02, -6.9211e-02,  7.0851e-02,  7.4153e-02,  9.4965e-02,\n",
      "         2.9783e-02,  3.5701e-02,  6.8800e-02,  3.4569e-02, -5.9611e-02,\n",
      "        -9.7471e-02,  5.7507e-02, -5.0931e-02,  1.2808e-01,  1.1860e-01,\n",
      "         6.7998e-02, -1.0194e-01, -6.3016e-02, -1.1357e-01,  2.3375e-02,\n",
      "        -5.5535e-02, -1.0265e-02, -4.3822e-02,  8.5367e-02, -1.0793e-01,\n",
      "         1.1859e-02,  7.7842e-02, -1.0828e-01,  7.6514e-02, -2.5502e-02,\n",
      "         7.9867e-02, -1.3185e-02,  5.5215e-03,  3.3041e-02, -2.5519e-02,\n",
      "         7.3034e-02,  5.8961e-02,  1.1601e-01,  4.9278e-02, -7.9119e-02,\n",
      "         2.5900e-02, -1.2747e-02,  5.7707e-02, -2.6136e-02, -9.3929e-03,\n",
      "         2.8122e-02,  1.0240e-02, -3.5013e-02, -1.3976e-02, -4.9826e-02,\n",
      "         3.0759e-02,  1.0295e-01,  5.1095e-02, -7.9100e-03, -1.4485e-03,\n",
      "        -2.5328e-02, -1.8059e-02,  1.1861e-02,  1.9025e-02,  1.6300e-02,\n",
      "        -1.0233e-01,  5.3269e-02,  8.8443e-02,  1.5303e-02,  1.1604e-01,\n",
      "        -3.7203e-03,  6.0121e-02,  4.8850e-03,  8.2110e-02,  1.7924e-02,\n",
      "         1.0228e-01,  1.0700e-01, -4.8295e-02, -6.1394e-02, -4.3793e-02,\n",
      "         7.8200e-02,  8.7985e-02,  4.5108e-02,  6.9046e-02,  4.3119e-02,\n",
      "        -2.8515e-02, -9.2784e-02,  7.4609e-02, -6.0460e-02,  2.0441e-02,\n",
      "         4.0305e-02,  1.0965e-01,  7.6613e-02,  6.8620e-02,  9.7740e-02,\n",
      "        -6.9998e-02,  3.3438e-02, -8.2239e-02, -7.0819e-02, -2.8209e-02,\n",
      "         1.0741e-01,  7.8959e-02, -1.0805e-01,  1.0404e-01,  1.7013e-02,\n",
      "        -1.2406e-02,  3.8270e-02, -5.5748e-02,  8.2620e-02, -8.8730e-02,\n",
      "        -4.9027e-02,  6.6139e-02,  3.7250e-02,  1.9093e-02,  1.4483e-02,\n",
      "        -3.9756e-02,  4.9918e-02, -9.3980e-02, -6.1882e-02,  2.7130e-02,\n",
      "        -5.6872e-02, -2.2764e-02,  4.2567e-03,  4.6881e-02,  1.0444e-01,\n",
      "        -1.3315e-01,  5.9711e-02, -1.1190e-01, -8.8305e-02, -3.1176e-03,\n",
      "         3.5462e-02, -7.7581e-02,  1.0305e-01, -1.1421e-01, -1.4325e-02,\n",
      "        -8.3305e-02, -1.0535e-01, -2.2676e-02,  8.8026e-03,  6.2652e-02,\n",
      "        -1.1119e-01,  4.9258e-02,  8.2999e-02,  5.1909e-02,  5.5719e-02,\n",
      "        -4.4206e-02, -1.0576e-01, -9.9403e-02, -5.3507e-02, -3.4971e-02,\n",
      "         2.4498e-02,  4.3760e-02,  3.4613e-02,  5.2136e-02,  6.2292e-02,\n",
      "         1.8970e-02, -5.5523e-02,  2.8821e-02,  4.1407e-02,  9.3853e-02,\n",
      "         6.0609e-02, -6.6136e-03,  1.2872e-01,  6.2717e-02,  2.7619e-02,\n",
      "         1.3452e-01,  4.3059e-02,  9.6405e-03,  5.0681e-02,  8.4863e-02,\n",
      "         2.2789e-02,  3.4092e-02,  9.3426e-02,  8.5520e-02,  6.8345e-02,\n",
      "        -7.3493e-03,  1.3199e-01,  8.6942e-02,  1.0554e-01,  2.4211e-02,\n",
      "         7.9253e-02,  7.0947e-02,  1.2765e-01,  1.0793e-01,  1.2788e-01,\n",
      "         1.1733e-02, -3.9768e-03,  2.8594e-02,  2.5080e-02,  3.7432e-02,\n",
      "         2.6324e-02,  5.9428e-02, -3.3958e-03,  1.2177e-01,  9.1980e-02,\n",
      "         7.0110e-02,  1.4585e-01,  8.8461e-02,  8.7987e-02,  1.2275e-02,\n",
      "         8.0470e-02,  8.6954e-02,  6.6204e-02,  2.1637e-02, -6.4674e-03,\n",
      "         5.1998e-02, -2.8939e-02,  1.2114e-01,  9.8887e-02, -4.2293e-03,\n",
      "         8.8159e-02,  1.1006e-01,  9.7453e-03,  1.4785e-01,  1.1652e-01,\n",
      "         2.9192e-02,  4.4132e-02,  5.3617e-02,  1.0260e-01,  9.1968e-02,\n",
      "         5.9427e-03,  1.3896e-01,  4.0762e-02,  3.6191e-02,  3.4444e-02,\n",
      "         2.6786e-02,  1.4421e-01,  1.3615e-01,  1.2129e-01, -1.0893e-02,\n",
      "         2.5614e-02,  1.4380e-01,  8.2921e-02, -1.2431e-02,  1.0334e-02,\n",
      "         9.6942e-02,  5.1327e-04,  1.2403e-01,  5.6996e-02,  3.1719e-02,\n",
      "        -2.0136e-02,  8.6576e-02,  6.4440e-02,  1.2638e-01,  1.3084e-01,\n",
      "         1.5358e-01,  1.2204e-01,  6.8014e-02,  7.2475e-02,  4.1671e-02,\n",
      "         9.4074e-02,  9.4625e-02,  9.8041e-02,  1.1148e-01, -9.6231e-03,\n",
      "         1.0209e-02, -2.9921e-02,  1.2152e-01,  5.4248e-03,  5.8470e-02,\n",
      "         5.6416e-02,  9.9944e-02,  3.2648e-02, -9.5706e-03,  1.2957e-01,\n",
      "         9.4438e-02,  2.1110e-02,  6.0105e-02,  9.8844e-02,  1.2873e-01,\n",
      "         8.1956e-03,  1.6779e-02,  6.4365e-02,  3.0876e-02,  1.1052e-01,\n",
      "        -3.0276e-02,  1.2028e-01], requires_grad=True)), ('fc.weight', Parameter containing:\n",
      "tensor([[-0.0999,  0.0261,  0.1018,  ...,  0.0883,  0.0282, -0.0227],\n",
      "        [-0.0111,  0.0506,  0.0932,  ...,  0.0424,  0.0922, -0.0305],\n",
      "        [-0.0217, -0.1171,  0.1426,  ..., -0.0056,  0.1605,  0.1437],\n",
      "        ...,\n",
      "        [ 0.0707,  0.2201, -0.1197,  ...,  0.0106,  0.0432, -0.0845],\n",
      "        [-0.0987,  0.1361, -0.1523,  ...,  0.0006,  0.0281,  0.0443],\n",
      "        [-0.1557,  0.1090,  0.0811,  ..., -0.0741,  0.1929,  0.0401]],\n",
      "       requires_grad=True)), ('fc.bias', Parameter containing:\n",
      "tensor([ 0.0061, -0.0562, -0.0365,  0.0222, -0.0388,  0.0696,  0.0202,  0.0667,\n",
      "         0.0890,  0.0478,  0.0206, -0.0177, -0.0228,  0.0076,  0.0435,  0.0319,\n",
      "        -0.0184,  0.0614,  0.0049,  0.0319], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFER\n",
    "train_iterator, valid_iterator, test_iterator = tt.BucketIterator.splits(\n",
    "    (train_data_afr, valid_data_afr, test_data_afr), \n",
    "    batch_size = batch_size,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = len(train_afr.vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 128\n",
    "out_dim = len(tagged_train_afr.vocab)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "pad_index = train_afr.vocab.stoi[train_afr.pad_token]\n",
    "tag_pad_idx = tagged_train_afr.vocab.stoi[tagged_train_afr.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTMTagger(in_dim, emb_dim, hid_dim, out_dim, n_layers, bidirectional, pad_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
    "optimizer = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate dutch params in dict\n",
    "transfer_param_dict = {}\n",
    "params = model.named_parameters()\n",
    "for name, param in params:\n",
    "    transfer_param_dict[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(transfer_param_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with and without fully connecected weights. No embedding weights however\n",
    "params2 = model2.named_parameters()\n",
    "for name, param in params2:\n",
    "    if(name == \"embedding.weight\" or name == \"fc.weight\" or name == \"fc.bias\"):\n",
    "    #if(name == \"embedding.weight\"):\n",
    "        continue\n",
    "    else:\n",
    "        param.data = transfer_param_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 2.449 | Train Acc: 30.22%\n",
      "\t Val. Loss: 2.111 |  Val. Acc: 42.41%\n",
      "Epoch: 02 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.771 | Train Acc: 56.95%\n",
      "\t Val. Loss: 1.661 |  Val. Acc: 60.65%\n",
      "Epoch: 03 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.372 | Train Acc: 66.81%\n",
      "\t Val. Loss: 1.365 |  Val. Acc: 63.73%\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.110 | Train Acc: 71.11%\n",
      "\t Val. Loss: 1.154 |  Val. Acc: 68.46%\n",
      "Epoch: 05 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.929 | Train Acc: 75.19%\n",
      "\t Val. Loss: 0.997 |  Val. Acc: 72.12%\n",
      "Epoch: 06 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.795 | Train Acc: 78.47%\n",
      "\t Val. Loss: 0.885 |  Val. Acc: 75.30%\n",
      "Epoch: 07 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.685 | Train Acc: 81.43%\n",
      "\t Val. Loss: 0.801 |  Val. Acc: 77.42%\n",
      "Epoch: 08 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.607 | Train Acc: 83.71%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 78.73%\n",
      "Epoch: 09 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.536 | Train Acc: 85.73%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 79.94%\n",
      "Epoch: 10 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.484 | Train Acc: 87.22%\n",
      "\t Val. Loss: 0.645 |  Val. Acc: 81.10%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model2, train_iterator, optimizer, criterion, tag_pad_idx)\n",
    "    valid_loss, valid_acc = evaluate(model2, valid_iterator, criterion, tag_pad_idx)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.600 |  Test Acc: 82.53%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model2, test_iterator, criterion, tag_pad_idx)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
